/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-21 10:09:59,495 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-21 10:09:59,624 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=True)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 8
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_4'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [4, 5, 6, 8, 9, 13, 17, 19, 20, 21, 22, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12, 24, 11, 16, 13, 19, 25, 4]
trainable_blocks = [3]
skip_connections = False
repa_flag = False
repa_depth = 8
reserve_memory = True
image_list_json = ['data_info.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-21 10:09:59,625 - PixArt - INFO - World_size: 1, seed: 43
2025-08-21 10:09:59,625 - PixArt - INFO - Initializing: DDP for training
2025-08-21 10:09:59,625 - PixArt - INFO - vae scale factor: 0.13025
2025-08-21 10:09:59,628 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-21 10:09:59,628 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:05<01:05, 65.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:06<00:00, 62.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:06<00:00, 63.35s/it]
2025-08-21 10:12:56,470 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-21 10:12:56,471 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-21 10:13:11,330 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-21 10:13:11,330 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-21 10:13:17,854 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-08-21 10:13:25,726 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth. Load ema: False.
2025-08-21 10:13:38,376 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-21 10:13:38,378 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.11.scale_shift_table', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.cross_attn.q_linear.weight', 'blocks.11.cross_attn.q_linear.bias', 'blocks.11.cross_attn.kv_linear.weight', 'blocks.11.cross_attn.kv_linear.bias', 'blocks.11.cross_attn.proj.weight', 'blocks.11.cross_attn.proj.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.12.scale_shift_table', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias', 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.12.cross_attn.q_linear.weight', 'blocks.12.cross_attn.q_linear.bias', 'blocks.12.cross_attn.kv_linear.weight', 'blocks.12.cross_attn.kv_linear.bias', 'blocks.12.cross_attn.proj.weight', 'blocks.12.cross_attn.proj.bias', 'blocks.12.mlp.fc1.weight', 'blocks.12.mlp.fc1.bias', 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias', 'blocks.13.scale_shift_table', 'blocks.13.attn.qkv.weight', 'blocks.13.attn.qkv.bias', 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias', 'blocks.13.cross_attn.q_linear.weight', 'blocks.13.cross_attn.q_linear.bias', 'blocks.13.cross_attn.kv_linear.weight', 'blocks.13.cross_attn.kv_linear.bias', 'blocks.13.cross_attn.proj.weight', 'blocks.13.cross_attn.proj.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.mlp.fc1.bias', 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.scale_shift_table', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.cross_attn.q_linear.weight', 'blocks.16.cross_attn.q_linear.bias', 'blocks.16.cross_attn.kv_linear.weight', 'blocks.16.cross_attn.kv_linear.bias', 'blocks.16.cross_attn.proj.weight', 'blocks.16.cross_attn.proj.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.19.scale_shift_table', 'blocks.19.attn.qkv.weight', 'blocks.19.attn.qkv.bias', 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias', 'blocks.19.cross_attn.q_linear.weight', 'blocks.19.cross_attn.q_linear.bias', 'blocks.19.cross_attn.kv_linear.weight', 'blocks.19.cross_attn.kv_linear.bias', 'blocks.19.cross_attn.proj.weight', 'blocks.19.cross_attn.proj.bias', 'blocks.19.mlp.fc1.weight', 'blocks.19.mlp.fc1.bias', 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias', 'blocks.20.scale_shift_table', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.cross_attn.q_linear.weight', 'blocks.20.cross_attn.q_linear.bias', 'blocks.20.cross_attn.kv_linear.weight', 'blocks.20.cross_attn.kv_linear.bias', 'blocks.20.cross_attn.proj.weight', 'blocks.20.cross_attn.proj.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias', 'blocks.24.scale_shift_table', 'blocks.24.attn.qkv.weight', 'blocks.24.attn.qkv.bias', 'blocks.24.attn.proj.weight', 'blocks.24.attn.proj.bias', 'blocks.24.cross_attn.q_linear.weight', 'blocks.24.cross_attn.q_linear.bias', 'blocks.24.cross_attn.kv_linear.weight', 'blocks.24.cross_attn.kv_linear.bias', 'blocks.24.cross_attn.proj.weight', 'blocks.24.cross_attn.proj.bias', 'blocks.24.mlp.fc1.weight', 'blocks.24.mlp.fc1.bias', 'blocks.24.mlp.fc2.weight', 'blocks.24.mlp.fc2.bias', 'blocks.25.scale_shift_table', 'blocks.25.attn.qkv.weight', 'blocks.25.attn.qkv.bias', 'blocks.25.attn.proj.weight', 'blocks.25.attn.proj.bias', 'blocks.25.cross_attn.q_linear.weight', 'blocks.25.cross_attn.q_linear.bias', 'blocks.25.cross_attn.kv_linear.weight', 'blocks.25.cross_attn.kv_linear.bias', 'blocks.25.cross_attn.proj.weight', 'blocks.25.cross_attn.proj.bias', 'blocks.25.mlp.fc1.weight', 'blocks.25.mlp.fc1.bias', 'blocks.25.mlp.fc2.weight', 'blocks.25.mlp.fc2.bias']
2025-08-21 10:13:38,378 - PixArt - WARNING - Unexpected keys: ['repa_mlp.proj.0.weight', 'repa_mlp.proj.0.bias', 'repa_mlp.proj.2.weight', 'repa_mlp.proj.2.bias', 'repa_mlp.proj.4.weight', 'repa_mlp.proj.4.bias']
2025-08-21 10:13:38,382 - PixArt - INFO - PixArtMS Model Parameters: 355,789,472
2025-08-21 10:13:38,383 - PixArt - INFO - PixArtMS Trainable Model Parameters: 21,255,552
2025-08-21 10:13:38,383 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-21 10:13:38,384 - PixArt - INFO - T5 max token length: 300
2025-08-21 10:13:38,384 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-21 10:13:40,940 - PixArt - INFO - data_info.json data volume: 608000
2025-08-21 10:13:48,047 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 9.66 s, length (use/ori): 607997/608000
2025-08-21 10:13:48,048 - PixArt - INFO - Automatically adapt lr to 0.00000 (using sqrt scaling rule).
2025-08-21 10:13:48,066 - PixArt - INFO - CAMEWrapper Optimizer: total 255 param groups, 15 are learnable, 240 are fix. Lr group: 15 params with lr 0.00000; Weight decay group: 15 params with weight decay 0.03.
2025-08-21 10:13:48,066 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/76000 [00:00<?, ?it/s]2025-08-21 10:14:00,512 - PixArt - INFO - Step/Epoch [1/1][1/76000]:total_eta: 2 days, 22:25:55, epoch_eta:2 days, 22:25:59, time_all:0.334, time_data:0.184, lr:3.536e-09, s:(32, 32), loss:0.5086, grad_norm:0.5645
2025-08-21 10:14:00,566 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 18.95it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 18.70it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 19.11it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:00<00:00, 19.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:00<00:00, 19.69it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 21.06it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:00, 20.39it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 20.44it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:00<00:00, 20.24it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 20.22it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 21.81it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:00, 20.78it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 20.37it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:00<00:00, 20.17it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 20.29it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 21.87it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:00, 20.49it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 20.16it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:00<00:00, 19.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:00<00:00, 19.04it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 20.66it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 17.38it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:00<00:00, 19.31it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 19.82it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 19.74it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 22.73it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 21.14it/s]
  0%|          | 1/76000 [00:18<381:12:20, 18.06s/it]  0%|          | 2/76000 [00:18<163:06:27,  7.73s/it]  0%|          | 3/76000 [00:18<92:17:14,  4.37s/it]   0%|          | 4/76000 [00:19<58:55:52,  2.79s/it]  0%|          | 5/76000 [00:19<40:32:36,  1.92s/it]  0%|          | 6/76000 [00:20<29:17:04,  1.39s/it]  0%|          | 7/76000 [00:20<22:11:44,  1.05s/it]  0%|          | 8/76000 [00:20<17:31:13,  1.20it/s]  0%|          | 9/76000 [00:21<14:27:17,  1.46it/s]  0%|          | 10/76000 [00:21<12:23:44,  1.70it/s]  0%|          | 11/76000 [00:21<11:01:43,  1.91it/s]  0%|          | 12/76000 [00:22<9:59:38,  2.11it/s]   0%|          | 13/76000 [00:22<9:11:59,  2.29it/s]  0%|          | 14/76000 [00:22<8:48:39,  2.40it/s]  0%|          | 15/76000 [00:23<8:27:04,  2.50it/s]  0%|          | 16/76000 [00:23<8:19:04,  2.54it/s]  0%|          | 17/76000 [00:24<8:14:21,  2.56it/s]  0%|          | 18/76000 [00:24<8:06:30,  2.60it/s]  0%|          | 19/76000 [00:24<7:58:40,  2.65it/s]2025-08-21 10:14:18,999 - PixArt - INFO - Step/Epoch [20/1][20/76000]:total_eta: 1 day, 1:17:10, epoch_eta:1 day, 1:17:11, time_all:0.924, time_data:0.573, lr:7.071e-08, s:(32, 32), loss:0.3748, grad_norm:0.5669
  0%|          | 20/76000 [00:25<7:57:40,  2.65it/s]  0%|          | 21/76000 [00:25<7:53:32,  2.67it/s]  0%|          | 22/76000 [00:25<7:52:14,  2.68it/s]  0%|          | 23/76000 [00:26<7:48:26,  2.70it/s]  0%|          | 24/76000 [00:26<7:45:52,  2.72it/s]  0%|          | 25/76000 [00:26<7:41:19,  2.74it/s]  0%|          | 26/76000 [00:27<7:41:31,  2.74it/s]  0%|          | 27/76000 [00:27<7:38:28,  2.76it/s]  0%|          | 28/76000 [00:28<7:45:07,  2.72it/s]  0%|          | 29/76000 [00:28<7:41:07,  2.75it/s]  0%|          | 30/76000 [00:28<7:42:07,  2.74it/s]  0%|          | 31/76000 [00:29<7:40:11,  2.75it/s]  0%|          | 32/76000 [00:29<7:42:11,  2.74it/s]  0%|          | 33/76000 [00:29<7:39:39,  2.75it/s]  0%|          | 34/76000 [00:30<7:45:16,  2.72it/s]  0%|          | 35/76000 [00:30<7:45:27,  2.72it/s]  0%|          | 36/76000 [00:31<7:44:04,  2.73it/s]  0%|          | 37/76000 [00:31<7:47:53,  2.71it/s]  0%|          | 38/76000 [00:31<7:45:07,  2.72it/s]  0%|          | 39/76000 [00:32<7:45:17,  2.72it/s]2025-08-21 10:14:26,318 - PixArt - INFO - Step/Epoch [40/1][40/76000]:total_eta: 16:42:53, epoch_eta:16:42:53, time_all:0.366, time_data:0.005, lr:1.414e-07, s:(32, 32), loss:0.4263, grad_norm:0.6033
  0%|          | 40/76000 [00:32<7:47:25,  2.71it/s]  0%|          | 41/76000 [00:32<7:42:50,  2.74it/s]  0%|          | 42/76000 [00:33<7:39:34,  2.75it/s]  0%|          | 43/76000 [00:33<7:42:20,  2.74it/s]  0%|          | 44/76000 [00:33<7:37:10,  2.77it/s]