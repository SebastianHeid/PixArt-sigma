/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-18 15:00:34,162 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-18 15:00:34,269 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=True)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 8
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 11, 13, 17, 19, 21, 22, 25]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12, 24, 11, 16, 13, 19, 25]
trainable_blocks = [24]
skip_connections = False
repa_flag = False
repa_depth = 8
reserve_memory = True
image_list_json = ['data_info.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-18 15:00:34,270 - PixArt - INFO - World_size: 2, seed: 43
2025-08-18 15:00:34,270 - PixArt - INFO - Initializing: DDP for training
2025-08-18 15:00:34,270 - PixArt - INFO - vae scale factor: 0.13025
2025-08-18 15:00:34,271 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-18 15:00:34,271 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.24s/it]
  0%|          | 0/38000 [00:00<?, ?it/s]2025-08-18 15:01:51,777 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:01:51,778 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:02:04,940 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:02:04,940 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:02:11,324 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-08-18 15:02:14,008 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth. Load ema: False.
2025-08-18 15:02:15,344 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-18 15:02:15,348 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.11.scale_shift_table', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.cross_attn.q_linear.weight', 'blocks.11.cross_attn.q_linear.bias', 'blocks.11.cross_attn.kv_linear.weight', 'blocks.11.cross_attn.kv_linear.bias', 'blocks.11.cross_attn.proj.weight', 'blocks.11.cross_attn.proj.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.12.scale_shift_table', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias', 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.12.cross_attn.q_linear.weight', 'blocks.12.cross_attn.q_linear.bias', 'blocks.12.cross_attn.kv_linear.weight', 'blocks.12.cross_attn.kv_linear.bias', 'blocks.12.cross_attn.proj.weight', 'blocks.12.cross_attn.proj.bias', 'blocks.12.mlp.fc1.weight', 'blocks.12.mlp.fc1.bias', 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias', 'blocks.13.scale_shift_table', 'blocks.13.attn.qkv.weight', 'blocks.13.attn.qkv.bias', 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias', 'blocks.13.cross_attn.q_linear.weight', 'blocks.13.cross_attn.q_linear.bias', 'blocks.13.cross_attn.kv_linear.weight', 'blocks.13.cross_attn.kv_linear.bias', 'blocks.13.cross_attn.proj.weight', 'blocks.13.cross_attn.proj.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.mlp.fc1.bias', 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.scale_shift_table', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.cross_attn.q_linear.weight', 'blocks.16.cross_attn.q_linear.bias', 'blocks.16.cross_attn.kv_linear.weight', 'blocks.16.cross_attn.kv_linear.bias', 'blocks.16.cross_attn.proj.weight', 'blocks.16.cross_attn.proj.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.19.scale_shift_table', 'blocks.19.attn.qkv.weight', 'blocks.19.attn.qkv.bias', 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias', 'blocks.19.cross_attn.q_linear.weight', 'blocks.19.cross_attn.q_linear.bias', 'blocks.19.cross_attn.kv_linear.weight', 'blocks.19.cross_attn.kv_linear.bias', 'blocks.19.cross_attn.proj.weight', 'blocks.19.cross_attn.proj.bias', 'blocks.19.mlp.fc1.weight', 'blocks.19.mlp.fc1.bias', 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias', 'blocks.20.scale_shift_table', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.cross_attn.q_linear.weight', 'blocks.20.cross_attn.q_linear.bias', 'blocks.20.cross_attn.kv_linear.weight', 'blocks.20.cross_attn.kv_linear.bias', 'blocks.20.cross_attn.proj.weight', 'blocks.20.cross_attn.proj.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias', 'blocks.24.scale_shift_table', 'blocks.24.attn.qkv.weight', 'blocks.24.attn.qkv.bias', 'blocks.24.attn.proj.weight', 'blocks.24.attn.proj.bias', 'blocks.24.cross_attn.q_linear.weight', 'blocks.24.cross_attn.q_linear.bias', 'blocks.24.cross_attn.kv_linear.weight', 'blocks.24.cross_attn.kv_linear.bias', 'blocks.24.cross_attn.proj.weight', 'blocks.24.cross_attn.proj.bias', 'blocks.24.mlp.fc1.weight', 'blocks.24.mlp.fc1.bias', 'blocks.24.mlp.fc2.weight', 'blocks.24.mlp.fc2.bias']
2025-08-18 15:02:15,348 - PixArt - WARNING - Unexpected keys: ['repa_mlp.proj.0.weight', 'repa_mlp.proj.0.bias', 'repa_mlp.proj.2.weight', 'repa_mlp.proj.2.bias', 'repa_mlp.proj.4.weight', 'repa_mlp.proj.4.bias']
2025-08-18 15:02:15,352 - PixArt - INFO - PixArtMS Model Parameters: 377,045,024
2025-08-18 15:02:15,353 - PixArt - INFO - PixArtMS Trainable Model Parameters: 0
2025-08-18 15:02:15,354 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-18 15:02:15,354 - PixArt - INFO - T5 max token length: 300
2025-08-18 15:02:15,354 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-18 15:02:17,227 - PixArt - INFO - data_info.json data volume: 608000
2025-08-18 15:02:24,356 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 9.00 s, length (use/ori): 607997/608000
2025-08-18 15:02:24,358 - PixArt - INFO - Automatically adapt lr to 0.00001 (using sqrt scaling rule).
2025-08-18 15:02:24,377 - PixArt - INFO - CAMEWrapper Optimizer: total 270 param groups, 0 are learnable, 270 are fix. Lr group: ; Weight decay group: .
2025-08-18 15:02:24,377 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/38000 [00:00<?, ?it/s]  0%|          | 0/38000 [00:05<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 795, in <module>
[rank0]:     train()
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 308, in train
[rank0]:     logs.update(grad_norm=accelerator.gather(grad_norm).mean().item())
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/accelerator.py", line 2794, in gather
[rank0]:     return gather(tensor)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 371, in wrapper
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 432, in gather
[rank0]:     return _gpu_gather(tensor)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 351, in _gpu_gather
[rank0]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
[rank0]:     return func(data, *args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 341, in _gpu_gather_one
[rank0]:     gather_op(output_tensors, tensor)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3836, in all_gather_into_tensor
[rank0]:     work = group._allgather_base(output_tensor, input_tensor, opts)
[rank0]: ValueError: Tensors must be CUDA and dense
  0%|          | 0/38000 [01:05<?, ?it/s]
[rank1]: Traceback (most recent call last):
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 795, in <module>
[rank1]:     train()
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 308, in train
[rank1]:     logs.update(grad_norm=accelerator.gather(grad_norm).mean().item())
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/accelerator.py", line 2794, in gather
[rank1]:     return gather(tensor)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 371, in wrapper
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 432, in gather
[rank1]:     return _gpu_gather(tensor)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 351, in _gpu_gather
[rank1]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
[rank1]:     return func(data, *args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 341, in _gpu_gather_one
[rank1]:     gather_op(output_tensors, tensor)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3836, in all_gather_into_tensor
[rank1]:     work = group._allgather_base(output_tensor, input_tensor, opts)
[rank1]: ValueError: Tensors must be CUDA and dense
[rank0]:[W818 15:02:34.825625214 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0818 15:02:37.325000 48296 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48354 closing signal SIGTERM
E0818 15:02:37.742000 48296 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 48355) of binary: /export/scratch/sheid/miniconda3/envs/mmcv_env/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 207, in <module>
    main()
  File "/export/home/sheid/.local/lib/python3.10/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 203, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 188, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_15:02:37
  host      : compgpu10
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 48355)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-18 15:02:56,055 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-18 15:02:56,160 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=True)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 13, 17, 19, 20, 21, 22, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12, 24, 11, 16, 13, 19, 25]
trainable_blocks = []
skip_connections = False
repa_flag = True
repa_depth = 8
reserve_memory = True
image_list_json = ['data_info.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-18 15:02:56,161 - PixArt - INFO - World_size: 2, seed: 43
2025-08-18 15:02:56,161 - PixArt - INFO - Initializing: DDP for training
2025-08-18 15:02:56,161 - PixArt - INFO - vae scale factor: 0.13025
2025-08-18 15:02:56,162 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-18 15:02:56,162 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
2025-08-18 15:03:36,388 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:03:36,388 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:03:48,562 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:03:48,563 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:03:54,828 - PixArt - INFO - PixArtMS Model Parameters: 620,561,056
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 669, in <module>
[rank0]:     missing, unexpected = load_checkpoint(
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
[rank0]:     checkpoint = torch.load(ckpt_file, map_location="cpu")
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 1479, in load
[rank0]:     with _open_file_like(f, "rb") as opened_file:
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
[rank0]:     return _open_file(name_or_buffer, mode)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
[rank0]:     super().__init__(open(name, mode))
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25/checkpoints/epoch_1_step_38000.pth'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 669, in <module>
[rank1]:     missing, unexpected = load_checkpoint(
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
[rank1]:     checkpoint = torch.load(ckpt_file, map_location="cpu")
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 1479, in load
[rank1]:     with _open_file_like(f, "rb") as opened_file:
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
[rank1]:     return _open_file(name_or_buffer, mode)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
[rank1]:     super().__init__(open(name, mode))
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25/checkpoints/epoch_1_step_38000.pth'
[rank0]:[W818 15:03:55.183659433 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0818 15:03:57.458000 50063 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 50089 closing signal SIGTERM
E0818 15:03:57.879000 50063 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 50088) of binary: /export/scratch/sheid/miniconda3/envs/mmcv_env/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 207, in <module>
    main()
  File "/export/home/sheid/.local/lib/python3.10/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 203, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 188, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_15:03:57
  host      : compgpu10
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 50088)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-18 15:04:11,041 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-18 15:04:11,129 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/json',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 13, 17, 19, 20, 21, 22, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12, 24, 11, 16, 13, 19, 25]
trainable_blocks = []
skip_connections = False
repa_flag = True
repa_depth = 8
reserve_memory = False
image_list_json = ['data_info.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-18 15:04:11,129 - PixArt - INFO - World_size: 2, seed: 43
2025-08-18 15:04:11,129 - PixArt - INFO - Initializing: DDP for training
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.31s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]
2025-08-18 15:04:21,279 - PixArt - INFO - vae scale factor: 0.13025
2025-08-18 15:04:21,280 - PixArt - INFO - Preparing Visualization prompt embeddings...
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
2025-08-18 15:04:54,265 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:04:54,265 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:05:06,345 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:05:06,345 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:05:12,865 - PixArt - INFO - PixArtMS Model Parameters: 620,561,056
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 669, in <module>
[rank0]:     missing, unexpected = load_checkpoint(
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
[rank0]:     checkpoint = torch.load(ckpt_file, map_location="cpu")
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 1479, in load
[rank0]:     with _open_file_like(f, "rb") as opened_file:
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
[rank0]:     return _open_file(name_or_buffer, mode)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
[rank0]:     super().__init__(open(name, mode))
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning/checkpoints/epoch_1_step_38000.pth'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 669, in <module>
[rank1]:     missing, unexpected = load_checkpoint(
[rank1]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
[rank1]:     checkpoint = torch.load(ckpt_file, map_location="cpu")
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 1479, in load
[rank1]:     with _open_file_like(f, "rb") as opened_file:
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 759, in _open_file_like
[rank1]:     return _open_file(name_or_buffer, mode)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/serialization.py", line 740, in __init__
[rank1]:     super().__init__(open(name, mode))
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_24_11_16_13_19_25_finetuning/checkpoints/epoch_1_step_38000.pth'
[rank0]:[W818 15:05:13.983305887 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0818 15:05:15.580000 50885 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 50904 closing signal SIGTERM
E0818 15:05:15.745000 50885 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 50903) of binary: /export/scratch/sheid/miniconda3/envs/mmcv_env/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 207, in <module>
    main()
  File "/export/home/sheid/.local/lib/python3.10/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 203, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 188, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_15:05:15
  host      : compgpu10
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 50903)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-18 15:05:29,655 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-18 15:05:29,765 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root='/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M',
    image_list_json=['data_info_fixed.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 6
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17]
trainable_blocks = []
skip_connections = False
repa_flag = True
repa_depth = 8
reserve_memory = False
image_list_json = ['data_info_fixed.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-18 15:05:29,765 - PixArt - INFO - World_size: 2, seed: 43
2025-08-18 15:05:29,765 - PixArt - INFO - Initializing: DDP for training
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
2025-08-18 15:05:39,315 - PixArt - INFO - vae scale factor: 0.13025
2025-08-18 15:05:39,317 - PixArt - INFO - Preparing Visualization prompt embeddings...
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
2025-08-18 15:06:10,379 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:06:10,379 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:06:22,748 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:06:22,749 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:06:29,208 - PixArt - INFO - PixArtMS Model Parameters: 620,561,056
2025-08-18 15:06:34,668 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8/checkpoints/epoch_1_step_38000.pth. Load ema: False.
2025-08-18 15:06:39,229 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-18 15:06:39,231 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias']
2025-08-18 15:06:39,232 - PixArt - WARNING - Unexpected keys: []
2025-08-18 15:06:39,233 - PixArt - INFO - PixArtMS Model Parameters: 556,794,400
2025-08-18 15:06:39,234 - PixArt - INFO - PixArtMS Trainable Model Parameters: 556,794,400
2025-08-18 15:06:39,235 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-18 15:06:39,236 - PixArt - INFO - T5 max token length: 300
2025-08-18 15:06:39,236 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-18 15:06:50,116 - PixArt - INFO - data_info_fixed.json data volume: 2035947
2025-08-18 15:07:14,427 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 35.19 s, length (use/ori): 2035937/2035947
2025-08-18 15:07:14,428 - PixArt - INFO - Automatically adapt lr to 0.00001 (using sqrt scaling rule).
2025-08-18 15:07:14,465 - PixArt - INFO - CAMEWrapper Optimizer: total 396 param groups, 396 are learnable, 0 are fix. Lr group: 396 params with lr 0.00001; Weight decay group: 396 params with weight decay 0.03.
2025-08-18 15:07:14,465 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/127247 [00:00<?, ?it/s]  0%|          | 0/127247 [00:00<?, ?it/s]2025-08-18 15:07:23,158 - PixArt - INFO - Step/Epoch [1/1][1/127247]:total_eta: 9 days, 18:53:48, epoch_eta:4 days, 21:26:56, time_all:0.332, time_data:0.226, lr:1.000e-08, s:(32, 32), loss:12.5751, grad_norm:2.7826, repa_loss:-0.2871
  0%|          | 1/127247 [00:06<238:00:05,  6.73s/it]2025-08-18 15:07:23,179 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.50it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.45it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.59it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.74it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.37it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.50it/s][A100%|██████████| 13/13 [00:00<00:00, 15.66it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.50it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.44it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.12it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.05it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.18it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.57it/s][A100%|██████████| 13/13 [00:00<00:00, 15.51it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.16it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.57it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.82it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.96it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.93it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.83it/s][A100%|██████████| 13/13 [00:00<00:00, 16.01it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.68it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.10it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.30it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.40it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.50it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.40it/s][A100%|██████████| 13/13 [00:00<00:00, 16.54it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.66it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.29it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.38it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.26it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.26it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.13it/s][A100%|██████████| 13/13 [00:00<00:00, 16.42it/s]
  0%|          | 1/127247 [00:13<467:57:37, 13.24s/it]  0%|          | 2/127247 [00:17<328:20:10,  9.29s/it]  0%|          | 2/127247 [00:17<286:17:19,  8.10s/it]  0%|          | 3/127247 [00:23<267:09:58,  7.56s/it]  0%|          | 3/127247 [00:23<244:17:37,  6.91s/it]  0%|          | 4/127247 [00:28<235:16:23,  6.66s/it]  0%|          | 4/127247 [00:28<221:27:28,  6.27s/it]  0%|          | 5/127247 [00:32<205:25:47,  5.81s/it]  0%|          | 5/127247 [00:32<196:34:02,  5.56s/it]  0%|          | 6/127247 [00:34<154:41:27,  4.38s/it]  0%|          | 6/127247 [00:34<148:50:30,  4.21s/it]  0%|          | 7/127247 [00:36<122:25:43,  3.46s/it]  0%|          | 7/127247 [00:36<118:29:12,  3.35s/it]  0%|          | 8/127247 [00:37<101:28:24,  2.87s/it]  0%|          | 8/127247 [00:37<98:47:42,  2.80s/it]   0%|          | 9/127247 [00:39<87:34:09,  2.48s/it]   0%|          | 9/127247 [00:39<85:48:45,  2.43s/it]  0%|          | 10/127247 [00:40<78:03:23,  2.21s/it]  0%|          | 10/127247 [00:40<76:44:04,  2.17s/it]  0%|          | 11/127247 [00:42<71:32:43,  2.02s/it]  0%|          | 11/127247 [00:42<70:38:05,  2.00s/it]  0%|          | 12/127247 [00:44<66:54:29,  1.89s/it]  0%|          | 12/127247 [00:44<66:25:35,  1.88s/it]  0%|          | 13/127247 [00:45<66:48:38,  1.89s/it]  0%|          | 13/127247 [00:45<66:20:49,  1.88s/it]  0%|          | 14/127247 [00:51<104:09:36,  2.95s/it]  0%|          | 14/127247 [00:51<103:50:24,  2.94s/it]  0%|          | 15/127247 [00:57<140:39:19,  3.98s/it]  0%|          | 15/127247 [00:57<140:25:34,  3.97s/it]  0%|          | 16/127247 [01:02<151:57:06,  4.30s/it]  0%|          | 16/127247 [01:02<151:50:07,  4.30s/it]  0%|          | 17/127247 [01:08<165:04:57,  4.67s/it]  0%|          | 17/127247 [01:08<164:56:42,  4.67s/it]  0%|          | 18/127247 [01:14<176:45:59,  5.00s/it]  0%|          | 18/127247 [01:14<176:40:28,  5.00s/it]  0%|          | 19/127247 [01:15<140:42:02,  3.98s/it]  0%|          | 19/127247 [01:15<140:38:47,  3.98s/it]  0%|          | 20/127247 [01:17<115:41:40,  3.27s/it]2025-08-18 15:08:33,750 - PixArt - INFO - Step/Epoch [20/1][20/127247]:total_eta: 10 days, 19:59:02, epoch_eta:5 days, 9:58:58, time_all:3.530, time_data:0.491, lr:2.000e-07, s:(32, 32), loss:13.7125, grad_norm:3.4243, repa_loss:-0.3038
  0%|          | 20/127247 [01:17<115:37:48,  3.27s/it]  0%|          | 21/127247 [01:18<97:52:56,  2.77s/it]   0%|          | 21/127247 [01:18<97:52:58,  2.77s/it]   0%|          | 22/127247 [01:20<85:27:52,  2.42s/it]  0%|          | 22/127247 [01:20<85:26:14,  2.42s/it]  0%|          | 23/127247 [01:22<77:01:23,  2.18s/it]  0%|          | 23/127247 [01:22<77:00:22,  2.18s/it]  0%|          | 24/127247 [01:23<70:57:08,  2.01s/it]  0%|          | 24/127247 [01:23<71:01:40,  2.01s/it]  0%|          | 25/127247 [01:28<105:10:53,  2.98s/it]  0%|          | 25/127247 [01:28<105:09:31,  2.98s/it]  0%|          | 26/127247 [01:34<135:22:05,  3.83s/it]  0%|          | 26/127247 [01:34<135:20:57,  3.83s/it]  0%|          | 27/127247 [01:41<161:59:42,  4.58s/it]  0%|          | 27/127247 [01:41<161:58:16,  4.58s/it]  0%|          | 28/127247 [01:47<176:54:36,  5.01s/it]  0%|          | 28/127247 [01:47<176:52:58,  5.01s/it]  0%|          | 29/127247 [01:52<182:16:53,  5.16s/it]  0%|          | 29/127247 [01:52<182:16:28,  5.16s/it]  0%|          | 30/127247 [01:54<144:39:59,  4.09s/it]  0%|          | 30/127247 [01:54<144:40:04,  4.09s/it]  0%|          | 31/127247 [01:55<118:18:16,  3.35s/it]  0%|          | 31/127247 [01:55<118:17:42,  3.35s/it]  0%|          | 32/127247 [01:57<99:42:17,  2.82s/it]   0%|          | 32/127247 [01:57<99:42:02,  2.82s/it]   0%|          | 33/127247 [01:59<86:50:01,  2.46s/it]  0%|          | 33/127247 [01:58<86:49:24,  2.46s/it]  0%|          | 34/127247 [02:00<77:47:42,  2.20s/it]  0%|          | 34/127247 [02:00<77:50:03,  2.20s/it]  0%|          | 35/127247 [02:02<74:25:41,  2.11s/it]  0%|          | 35/127247 [02:02<74:25:36,  2.11s/it]  0%|          | 36/127247 [02:07<102:00:25,  2.89s/it]  0%|          | 36/127247 [02:07<102:00:44,  2.89s/it]  0%|          | 37/127247 [02:12<129:50:23,  3.67s/it]  0%|          | 37/127247 [02:12<129:50:25,  3.67s/it]  0%|          | 38/127247 [02:19<157:48:26,  4.47s/it]  0%|          | 38/127247 [02:19<157:48:33,  4.47s/it]  0%|          | 39/127247 [02:25<174:25:52,  4.94s/it]  0%|          | 39/127247 [02:25<174:25:21,  4.94s/it]  0%|          | 40/127247 [02:28<156:19:12,  4.42s/it]2025-08-18 15:09:44,781 - PixArt - INFO - Step/Epoch [40/1][40/127247]:total_eta: 10 days, 15:36:16, epoch_eta:5 days, 7:46:59, time_all:3.552, time_data:0.170, lr:4.000e-07, s:(32, 32), loss:11.8129, grad_norm:9.6936, repa_loss:-0.3412
  0%|          | 40/127247 [02:28<156:17:57,  4.42s/it]  0%|          | 41/127247 [02:29<126:27:51,  3.58s/it]  0%|          | 41/127247 [02:29<126:27:43,  3.58s/it]  0%|          | 42/127247 [02:31<105:28:19,  2.98s/it]  0%|          | 42/127247 [02:31<105:27:16,  2.98s/it]  0%|          | 43/127247 [02:33<90:53:52,  2.57s/it]   0%|          | 43/127247 [02:33<90:54:21,  2.57s/it]   0%|          | 44/127247 [02:36<102:12:31,  2.89s/it]  0%|          | 44/127247 [02:36<102:20:26,  2.90s/it]  0%|          | 45/127247 [02:43<137:21:42,  3.89s/it]  0%|          | 45/127247 [02:42<137:20:36,  3.89s/it]  0%|          | 46/127247 [02:48<158:08:36,  4.48s/it]  0%|          | 46/127247 [02:48<158:05:46,  4.47s/it]  0%|          | 47/127247 [02:50<127:47:49,  3.62s/it]  0%|          | 47/127247 [02:50<127:45:34,  3.62s/it]  0%|          | 48/127247 [02:52<106:30:23,  3.01s/it]  0%|          | 48/127247 [02:52<106:31:16,  3.01s/it]  0%|          | 49/127247 [02:53<91:28:46,  2.59s/it]   0%|          | 49/127247 [02:53<91:29:15,  2.59s/it]   0%|          | 50/127247 [02:58<113:33:08,  3.21s/it]  0%|          | 50/127247 [02:58<113:32:41,  3.21s/it]  0%|          | 51/127247 [03:03<134:52:23,  3.82s/it]  0%|          | 51/127247 [03:03<134:52:29,  3.82s/it]  0%|          | 52/127247 [03:09<156:34:21,  4.43s/it]  0%|          | 52/127247 [03:09<156:34:04,  4.43s/it]  0%|          | 53/127247 [03:11<134:43:58,  3.81s/it]  0%|          | 53/127247 [03:11<134:42:41,  3.81s/it]  0%|          | 54/127247 [03:13<111:17:35,  3.15s/it]  0%|          | 54/127247 [03:13<111:16:35,  3.15s/it]  0%|          | 55/127247 [03:15<94:48:12,  2.68s/it]   0%|          | 55/127247 [03:14<94:48:58,  2.68s/it]   0%|          | 56/127247 [03:16<83:38:54,  2.37s/it]  0%|          | 56/127247 [03:16<83:40:29,  2.37s/it]  0%|          | 57/127247 [03:21<108:52:02,  3.08s/it]  0%|          | 57/127247 [03:21<108:51:02,  3.08s/it]  0%|          | 58/127247 [03:27<137:08:06,  3.88s/it]  0%|          | 58/127247 [03:27<137:08:14,  3.88s/it]  0%|          | 59/127247 [03:33<161:36:04,  4.57s/it]  0%|          | 59/127247 [03:33<161:36:07,  4.57s/it]2025-08-18 15:10:55,054 - PixArt - INFO - Step/Epoch [60/1][60/127247]:total_eta: 10 days, 13:12:21, epoch_eta:5 days, 6:34:26, time_all:3.514, time_data:0.170, lr:6.000e-07, s:(32, 32), loss:8.4044, grad_norm:8.6245, repa_loss:-0.3426
  0%|          | 60/127247 [03:38<169:15:25,  4.79s/it]  0%|          | 60/127247 [03:38<169:14:27,  4.79s/it]  0%|          | 61/127247 [03:40<135:24:55,  3.83s/it]  0%|          | 61/127247 [03:40<135:25:28,  3.83s/it]  0%|          | 62/127247 [03:41<111:45:11,  3.16s/it]  0%|          | 62/127247 [03:41<111:45:19,  3.16s/it]  0%|          | 63/127247 [03:43<95:22:42,  2.70s/it]   0%|          | 63/127247 [03:43<95:23:26,  2.70s/it]   0%|          | 64/127247 [03:45<83:51:00,  2.37s/it]  0%|          | 64/127247 [03:44<84:00:24,  2.38s/it]  0%|          | 65/127247 [03:49<105:52:11,  3.00s/it]  0%|          | 65/127247 [03:49<105:51:45,  3.00s/it]  0%|          | 66/127247 [03:55<137:54:04,  3.90s/it]  0%|          | 66/127247 [03:55<137:50:49,  3.90s/it]  0%|          | 67/127247 [04:00<152:52:21,  4.33s/it]  0%|          | 67/127247 [04:00<152:49:15,  4.33s/it]  0%|          | 68/127247 [04:02<123:51:21,  3.51s/it]  0%|          | 68/127247 [04:02<123:50:42,  3.51s/it]  0%|          | 69/127247 [04:04<103:40:36,  2.93s/it]  0%|          | 69/127247 [04:03<103:39:03,  2.93s/it]  0%|          | 70/127247 [04:05<89:30:13,  2.53s/it]   0%|          | 70/127247 [04:05<89:29:57,  2.53s/it]   0%|          | 71/127247 [04:07<79:37:40,  2.25s/it]  0%|          | 71/127247 [04:07<79:37:18,  2.25s/it]  0%|          | 72/127247 [04:08<72:51:33,  2.06s/it]  0%|          | 72/127247 [04:08<72:50:27,  2.06s/it]  0%|          | 73/127247 [04:10<67:54:57,  1.92s/it]  0%|          | 73/127247 [04:10<68:01:05,  1.93s/it]  0%|          | 74/127247 [04:12<64:47:44,  1.83s/it]  0%|          | 74/127247 [04:12<64:53:24,  1.84s/it]  0%|          | 75/127247 [04:17<101:39:07,  2.88s/it]  0%|          | 75/127247 [04:17<101:38:25,  2.88s/it]  0%|          | 76/127247 [04:24<142:56:03,  4.05s/it]  0%|          | 76/127247 [04:24<142:52:44,  4.04s/it]  0%|          | 77/127247 [04:30<165:44:43,  4.69s/it]  0%|          | 77/127247 [04:30<165:41:32,  4.69s/it]  0%|          | 78/127247 [04:34<160:58:01,  4.56s/it]  0%|          | 78/127247 [04:34<160:55:39,  4.56s/it]  0%|          | 79/127247 [04:36<129:36:53,  3.67s/it]  0%|          | 79/127247 [04:36<129:35:33,  3.67s/it]  0%|          | 80/127247 [04:37<107:46:31,  3.05s/it]2025-08-18 15:11:54,232 - PixArt - INFO - Step/Epoch [80/1][80/127247]:total_eta: 10 days, 2:18:06, epoch_eta:5 days, 1:06:49, time_all:2.959, time_data:0.170, lr:8.000e-07, s:(32, 32), loss:5.4139, grad_norm:6.3655, repa_loss:-0.3433
  0%|          | 80/127247 [04:37<107:53:59,  3.05s/it]  0%|          | 81/127247 [04:40<100:09:20,  2.84s/it]  0%|          | 81/127247 [04:40<100:07:55,  2.83s/it]  0%|          | 82/127247 [04:45<121:50:08,  3.45s/it]  0%|          | 82/127247 [04:44<121:52:53,  3.45s/it]  0%|          | 83/127247 [04:50<142:16:36,  4.03s/it]  0%|          | 83/127247 [04:50<142:14:14,  4.03s/it]  0%|          | 84/127247 [04:56<162:03:11,  4.59s/it]  0%|          | 84/127247 [04:56<162:01:03,  4.59s/it]  0%|          | 85/127247 [04:59<151:05:15,  4.28s/it]  0%|          | 85/127247 [04:59<151:03:49,  4.28s/it]  0%|          | 86/127247 [05:01<122:42:55,  3.47s/it]  0%|          | 86/127247 [05:01<122:41:41,  3.47s/it]  0%|          | 87/127247 [05:03<102:55:49,  2.91s/it]  0%|          | 87/127247 [05:02<102:55:09,  2.91s/it]  0%|          | 88/127247 [05:04<89:05:18,  2.52s/it]   0%|          | 88/127247 [05:04<89:08:58,  2.52s/it]   0%|          | 89/127247 [05:09<114:13:22,  3.23s/it]  0%|          | 89/127247 [05:09<114:13:32,  3.23s/it]  0%|          | 90/127247 [05:15<139:01:27,  3.94s/it]  0%|          | 90/127247 [05:15<138:59:44,  3.94s/it]  0%|          | 91/127247 [05:20<156:33:29,  4.43s/it]  0%|          | 91/127247 [05:20<156:31:32,  4.43s/it]  0%|          | 92/127247 [05:24<148:50:41,  4.21s/it]  0%|          | 92/127247 [05:24<148:49:45,  4.21s/it]  0%|          | 93/127247 [05:26<121:05:14,  3.43s/it]  0%|          | 93/127247 [05:25<121:04:09,  3.43s/it]  0%|          | 94/127247 [05:27<101:45:21,  2.88s/it]  0%|          | 94/127247 [05:27<101:44:03,  2.88s/it]  0%|          | 95/127247 [05:29<88:09:18,  2.50s/it]   0%|          | 95/127247 [05:29<88:09:24,  2.50s/it]   0%|          | 96/127247 [05:30<78:40:34,  2.23s/it]  0%|          | 96/127247 [05:30<78:40:06,  2.23s/it]  0%|          | 97/127247 [05:32<71:57:38,  2.04s/it]  0%|          | 97/127247 [05:32<71:57:02,  2.04s/it]  0%|          | 98/127247 [05:34<67:24:35,  1.91s/it]  0%|          | 98/127247 [05:33<67:29:14,  1.91s/it]  0%|          | 99/127247 [05:38<96:37:40,  2.74s/it]  0%|          | 99/127247 [05:38<96:37:29,  2.74s/it]  0%|          | 100/127247 [05:43<120:19:17,  3.41s/it]2025-08-18 15:13:00,087 - PixArt - INFO - Step/Epoch [100/1][100/127247]:total_eta: 10 days, 0:22:53, epoch_eta:5 days, 0:08:40, time_all:3.293, time_data:0.171, lr:1.000e-06, s:(32, 32), loss:3.6277, grad_norm:1.9302, repa_loss:-0.3841
  0%|          | 100/127247 [05:43<120:18:08,  3.41s/it]  0%|          | 101/127247 [05:50<153:42:41,  4.35s/it]  0%|          | 101/127247 [05:50<153:42:21,  4.35s/it]  0%|          | 102/127247 [05:55<167:02:30,  4.73s/it]  0%|          | 102/127247 [05:55<167:01:14,  4.73s/it]  0%|          | 103/127247 [05:59<151:13:37,  4.28s/it]  0%|          | 103/127247 [05:58<151:13:33,  4.28s/it]  0%|          | 104/127247 [06:00<122:50:57,  3.48s/it]  0%|          | 104/127247 [06:00<122:51:18,  3.48s/it]  0%|          | 105/127247 [06:02<103:00:03,  2.92s/it]  0%|          | 105/127247 [06:02<102:59:36,  2.92s/it]  0%|          | 106/127247 [06:03<89:01:39,  2.52s/it]   0%|          | 106/127247 [06:03<89:02:14,  2.52s/it]   0%|          | 107/127247 [06:08<112:22:53,  3.18s/it]  0%|          | 107/127247 [06:08<112:21:57,  3.18s/it]  0%|          | 108/127247 [06:13<130:19:56,  3.69s/it]  0%|          | 108/127247 [06:13<130:18:49,  3.69s/it]  0%|          | 109/127247 [06:19<154:54:56,  4.39s/it]  0%|          | 109/127247 [06:19<154:55:50,  4.39s/it]  0%|          | 110/127247 [06:25<171:41:32,  4.86s/it]  0%|          | 110/127247 [06:25<171:40:42,  4.86s/it]  0%|          | 111/127247 [06:27<137:08:22,  3.88s/it]  0%|          | 111/127247 [06:26<137:08:36,  3.88s/it]  0%|          | 112/127247 [06:28<113:03:26,  3.20s/it]  0%|          | 112/127247 [06:28<113:03:15,  3.20s/it]  0%|          | 113/127247 [06:30<96:12:14,  2.72s/it]   0%|          | 113/127247 [06:30<96:12:46,  2.72s/it]   0%|          | 114/127247 [06:31<84:15:25,  2.39s/it]  0%|          | 114/127247 [06:31<84:18:20,  2.39s/it]  0%|          | 115/127247 [06:33<75:54:10,  2.15s/it]  0%|          | 115/127247 [06:33<75:55:41,  2.15s/it]  0%|          | 116/127247 [06:37<97:30:13,  2.76s/it]  0%|          | 116/127247 [06:37<97:36:14,  2.76s/it]  0%|          | 117/127247 [06:43<128:57:01,  3.65s/it]  0%|          | 117/127247 [06:43<128:54:10,  3.65s/it]  0%|          | 118/127247 [06:46<124:11:15,  3.52s/it]  0%|          | 118/127247 [06:46<124:11:46,  3.52s/it]  0%|          | 119/127247 [06:51<137:28:59,  3.89s/it]  0%|          | 119/127247 [06:51<137:26:36,  3.89s/it]  0%|          | 120/127247 [06:57<162:58:02,  4.61s/it]2025-08-18 15:14:14,088 - PixArt - INFO - Step/Epoch [120/1][120/127247]:total_eta: 10 days, 3:50:47, epoch_eta:5 days, 1:52:00, time_all:3.700, time_data:0.170, lr:1.200e-06, s:(32, 32), loss:2.9836, grad_norm:1.3291, repa_loss:-0.4136
  0%|          | 120/127247 [06:57<162:56:01,  4.61s/it]  0%|          | 121/127247 [07:01<151:35:45,  4.29s/it]  0%|          | 121/127247 [07:01<151:35:00,  4.29s/it]  0%|          | 122/127247 [07:02<123:06:26,  3.49s/it]  0%|          | 122/127247 [07:02<123:05:58,  3.49s/it]  0%|          | 123/127247 [07:04<103:06:19,  2.92s/it]  0%|          | 123/127247 [07:04<103:06:34,  2.92s/it]  0%|          | 124/127247 [07:05<89:06:02,  2.52s/it]   0%|          | 124/127247 [07:05<89:08:58,  2.52s/it]   0%|          | 125/127247 [07:07<79:23:40,  2.25s/it]  0%|          | 125/127247 [07:07<79:27:14,  2.25s/it]  0%|          | 126/127247 [07:12<102:34:05,  2.90s/it]  0%|          | 126/127247 [07:11<102:31:43,  2.90s/it]  0%|          | 127/127247 [07:17<133:30:55,  3.78s/it]  0%|          | 127/127247 [07:17<133:32:54,  3.78s/it]  0%|          | 128/127247 [07:24<159:46:04,  4.52s/it]  0%|          | 128/127247 [07:24<159:43:33,  4.52s/it]  0%|          | 129/127247 [07:25<128:47:08,  3.65s/it]  0%|          | 129/127247 [07:25<128:45:10,  3.65s/it]  0%|          | 130/127247 [07:27<107:06:39,  3.03s/it]  0%|          | 130/127247 [07:27<107:05:46,  3.03s/it]  0%|          | 131/127247 [07:28<91:58:43,  2.60s/it]   0%|          | 131/127247 [07:28<92:03:01,  2.61s/it]   0%|          | 132/127247 [07:33<109:48:10,  3.11s/it]  0%|          | 132/127247 [07:33<109:48:49,  3.11s/it]  0%|          | 133/127247 [07:38<136:44:34,  3.87s/it]  0%|          | 133/127247 [07:38<136:42:07,  3.87s/it]  0%|          | 134/127247 [07:44<158:33:53,  4.49s/it]  0%|          | 134/127247 [07:44<158:31:33,  4.49s/it]  0%|          | 135/127247 [07:50<174:39:21,  4.95s/it]  0%|          | 135/127247 [07:50<174:38:20,  4.95s/it]  0%|          | 136/127247 [07:57<189:34:46,  5.37s/it]  0%|          | 136/127247 [07:57<189:33:47,  5.37s/it]  0%|          | 137/127247 [07:58<149:40:55,  4.24s/it]  0%|          | 137/127247 [07:58<149:39:41,  4.24s/it]  0%|          | 138/127247 [08:00<121:45:14,  3.45s/it]  0%|          | 138/127247 [08:00<121:44:24,  3.45s/it]  0%|          | 139/127247 [08:01<102:09:54,  2.89s/it]  0%|          | 139/127247 [08:01<102:09:34,  2.89s/it]  0%|          | 140/127247 [08:03<88:38:37,  2.51s/it] 2025-08-18 15:15:20,026 - PixArt - INFO - Step/Epoch [140/1][140/127247]:total_eta: 10 days, 2:16:56, epoch_eta:5 days, 1:04:31, time_all:3.297, time_data:0.170, lr:1.400e-06, s:(32, 32), loss:2.4650, grad_norm:1.1584, repa_loss:-0.4178
  0%|          | 140/127247 [08:03<88:47:13,  2.51s/it]   0%|          | 141/127247 [08:06<97:58:21,  2.77s/it]  0%|          | 141/127247 [08:06<97:56:31,  2.77s/it]  0%|          | 142/127247 [08:11<113:49:39,  3.22s/it]  0%|          | 142/127247 [08:11<113:47:58,  3.22s/it]  0%|          | 143/127247 [08:16<139:05:09,  3.94s/it]  0%|          | 143/127247 [08:16<139:04:18,  3.94s/it]  0%|          | 144/127247 [08:22<162:08:40,  4.59s/it]  0%|          | 144/127247 [08:22<162:08:00,  4.59s/it]  0%|          | 145/127247 [08:24<130:27:42,  3.70s/it]  0%|          | 145/127247 [08:24<130:27:15,  3.69s/it]  0%|          | 146/127247 [08:26<108:10:37,  3.06s/it]  0%|          | 146/127247 [08:26<108:09:49,  3.06s/it]  0%|          | 147/127247 [08:27<93:19:08,  2.64s/it]   0%|          | 147/127247 [08:27<93:18:56,  2.64s/it]   0%|          | 148/127247 [08:32<111:36:17,  3.16s/it]  0%|          | 148/127247 [08:32<111:35:59,  3.16s/it]  0%|          | 149/127247 [08:38<145:13:24,  4.11s/it]  0%|          | 149/127247 [08:38<145:12:19,  4.11s/it]  0%|          | 150/127247 [08:44<168:48:21,  4.78s/it]  0%|          | 150/127247 [08:44<168:53:59,  4.78s/it]  0%|          | 151/127247 [08:51<184:30:24,  5.23s/it]  0%|          | 151/127247 [08:51<184:28:19,  5.23s/it]  0%|          | 152/127247 [08:57<194:58:37,  5.52s/it]  0%|          | 152/127247 [08:57<194:56:26,  5.52s/it]  0%|          | 153/127247 [08:59<161:48:19,  4.58s/it]  0%|          | 153/127247 [08:59<161:47:00,  4.58s/it]  0%|          | 154/127247 [09:01<130:22:20,  3.69s/it]  0%|          | 154/127247 [09:01<130:23:45,  3.69s/it]  0%|          | 155/127247 [09:02<108:16:04,  3.07s/it]  0%|          | 155/127247 [09:02<108:15:53,  3.07s/it]  0%|          | 156/127247 [09:04<92:47:57,  2.63s/it]   0%|          | 156/127247 [09:04<92:47:39,  2.63s/it]   0%|          | 157/127247 [09:06<82:05:16,  2.33s/it]  0%|          | 157/127247 [09:06<82:04:40,  2.32s/it]  0%|          | 158/127247 [09:07<74:27:19,  2.11s/it]  0%|          | 158/127247 [09:07<74:29:22,  2.11s/it]  0%|          | 159/127247 [09:12<100:19:19,  2.84s/it]  0%|          | 159/127247 [09:12<100:18:50,  2.84s/it]  0%|          | 160/127247 [09:16<117:21:46,  3.32s/it]2025-08-18 15:16:33,235 - PixArt - INFO - Step/Epoch [160/1][160/127247]:total_eta: 10 days, 4:17:35, epoch_eta:5 days, 2:04:14, time_all:3.660, time_data:0.170, lr:1.600e-06, s:(32, 32), loss:1.8827, grad_norm:1.5231, repa_loss:-0.3994
  0%|          | 160/127247 [09:16<117:20:50,  3.32s/it]  0%|          | 161/127247 [09:22<142:17:39,  4.03s/it]  0%|          | 161/127247 [09:22<142:16:57,  4.03s/it]  0%|          | 162/127247 [09:28<162:07:13,  4.59s/it]  0%|          | 162/127247 [09:28<162:07:53,  4.59s/it]  0%|          | 163/127247 [09:29<130:21:26,  3.69s/it]  0%|          | 163/127247 [09:29<130:22:54,  3.69s/it]  0%|          | 164/127247 [09:31<108:15:13,  3.07s/it]  0%|          | 164/127247 [09:31<108:13:53,  3.07s/it]  0%|          | 165/127247 [09:33<92:47:21,  2.63s/it]   0%|          | 165/127247 [09:33<92:47:35,  2.63s/it]   0%|          | 166/127247 [09:34<81:56:34,  2.32s/it]  0%|          | 166/127247 [09:34<81:58:12,  2.32s/it]  0%|          | 167/127247 [09:39<103:07:01,  2.92s/it]  0%|          | 167/127247 [09:39<103:08:10,  2.92s/it]  0%|          | 168/127247 [09:44<132:14:29,  3.75s/it]  0%|          | 168/127247 [09:44<132:13:17,  3.75s/it]  0%|          | 169/127247 [09:50<157:36:43,  4.47s/it]  0%|          | 169/127247 [09:50<157:35:32,  4.46s/it]  0%|          | 170/127247 [09:55<153:47:37,  4.36s/it]  0%|          | 170/127247 [09:54<153:47:26,  4.36s/it]  0%|          | 171/127247 [09:56<124:36:39,  3.53s/it]  0%|          | 171/127247 [09:56<124:39:50,  3.53s/it]  0%|          | 172/127247 [09:58<104:11:47,  2.95s/it]  0%|          | 172/127247 [09:58<104:10:11,  2.95s/it]  0%|          | 173/127247 [09:59<89:53:29,  2.55s/it]   0%|          | 173/127247 [09:59<89:56:09,  2.55s/it]   0%|          | 174/127247 [10:01<84:08:22,  2.38s/it]  0%|          | 174/127247 [10:01<84:07:27,  2.38s/it]  0%|          | 175/127247 [10:05<102:24:45,  2.90s/it]  0%|          | 175/127247 [10:05<102:24:52,  2.90s/it]  0%|          | 176/127247 [10:10<123:23:54,  3.50s/it]  0%|          | 176/127247 [10:10<123:22:09,  3.50s/it]  0%|          | 177/127247 [10:16<149:21:43,  4.23s/it]  0%|          | 177/127247 [10:16<149:22:14,  4.23s/it]  0%|          | 178/127247 [10:20<142:36:30,  4.04s/it]  0%|          | 178/127247 [10:20<142:35:30,  4.04s/it]  0%|          | 179/127247 [10:21<116:49:53,  3.31s/it]  0%|          | 179/127247 [10:21<116:48:01,  3.31s/it]  0%|          | 180/127247 [10:23<98:59:54,  2.80s/it] 2025-08-18 15:17:40,040 - PixArt - INFO - Step/Epoch [180/1][180/127247]:total_eta: 10 days, 3:21:20, epoch_eta:5 days, 1:35:33, time_all:3.340, time_data:0.170, lr:1.800e-06, s:(32, 32), loss:1.5070, grad_norm:1.6297, repa_loss:-0.4147
  0%|          | 180/127247 [10:23<98:59:25,  2.80s/it]   0%|          | 181/127247 [10:25<86:16:12,  2.44s/it]  0%|          | 181/127247 [10:25<86:15:19,  2.44s/it]  0%|          | 182/127247 [10:26<77:33:22,  2.20s/it]  0%|          | 182/127247 [10:26<77:37:04,  2.20s/it]  0%|          | 183/127247 [10:30<90:18:30,  2.56s/it]  0%|          | 183/127247 [10:30<90:17:25,  2.56s/it]  0%|          | 184/127247 [10:35<115:52:06,  3.28s/it]  0%|          | 184/127247 [10:35<115:50:58,  3.28s/it]  0%|          | 185/127247 [10:41<145:32:51,  4.12s/it]  0%|          | 185/127247 [10:41<145:31:44,  4.12s/it]  0%|          | 186/127247 [10:46<160:57:32,  4.56s/it]  0%|          | 186/127247 [10:46<160:57:39,  4.56s/it]  0%|          | 187/127247 [10:52<174:58:06,  4.96s/it]  0%|          | 187/127247 [10:52<174:58:13,  4.96s/it]  0%|          | 188/127247 [10:54<139:28:20,  3.95s/it]  0%|          | 188/127247 [10:54<139:27:28,  3.95s/it]  0%|          | 189/127247 [10:55<114:33:10,  3.25s/it]  0%|          | 189/127247 [10:55<114:32:36,  3.25s/it]  0%|          | 190/127247 [10:57<97:09:44,  2.75s/it]   0%|          | 190/127247 [10:57<97:09:43,  2.75s/it]   0%|          | 191/127247 [10:59<85:11:25,  2.41s/it]  0%|          | 191/127247 [10:59<85:11:35,  2.41s/it]  0%|          | 192/127247 [11:00<76:37:48,  2.17s/it]  0%|          | 192/127247 [11:00<76:41:39,  2.17s/it]  0%|          | 193/127247 [11:06<109:15:55,  3.10s/it]  0%|          | 193/127247 [11:05<109:13:52,  3.10s/it]  0%|          | 194/127247 [11:10<128:23:35,  3.64s/it]  0%|          | 194/127247 [11:10<128:22:38,  3.64s/it]  0%|          | 195/127247 [11:17<156:19:09,  4.43s/it]  0%|          | 195/127247 [11:17<156:18:37,  4.43s/it]  0%|          | 196/127247 [11:23<172:42:18,  4.89s/it]  0%|          | 196/127247 [11:23<172:42:03,  4.89s/it]  0%|          | 197/127247 [11:26<160:22:01,  4.54s/it]  0%|          | 197/127247 [11:26<160:22:09,  4.54s/it]  0%|          | 198/127247 [11:28<129:16:15,  3.66s/it]  0%|          | 198/127247 [11:28<129:17:40,  3.66s/it]  0%|          | 199/127247 [11:30<107:31:50,  3.05s/it]  0%|          | 199/127247 [11:30<107:31:12,  3.05s/it]  0%|          | 200/127247 [11:31<92:12:51,  2.61s/it] 2025-08-18 15:18:48,176 - PixArt - INFO - Step/Epoch [200/1][200/127247]:total_eta: 10 days, 3:04:07, epoch_eta:5 days, 1:26:23, time_all:3.407, time_data:0.170, lr:2.000e-06, s:(32, 32), loss:1.3208, grad_norm:1.8609, repa_loss:-0.4356
  0%|          | 200/127247 [11:31<92:11:53,  2.61s/it]   0%|          | 201/127247 [11:33<81:38:34,  2.31s/it]  0%|          | 201/127247 [11:33<81:38:01,  2.31s/it]  0%|          | 202/127247 [11:34<74:05:47,  2.10s/it]  0%|          | 202/127247 [11:34<74:06:05,  2.10s/it]  0%|          | 203/127247 [11:36<68:48:55,  1.95s/it]  0%|          | 203/127247 [11:36<68:49:09,  1.95s/it]  0%|          | 204/127247 [11:38<65:14:34,  1.85s/it]  0%|          | 204/127247 [11:38<65:16:09,  1.85s/it]  0%|          | 205/127247 [11:39<62:36:45,  1.77s/it]  0%|          | 205/127247 [11:39<62:37:05,  1.77s/it]  0%|          | 206/127247 [11:41<60:50:01,  1.72s/it]  0%|          | 206/127247 [11:41<61:01:11,  1.73s/it]  0%|          | 207/127247 [11:45<88:39:12,  2.51s/it]  0%|          | 207/127247 [11:45<88:39:28,  2.51s/it]  0%|          | 208/127247 [11:52<128:41:40,  3.65s/it]  0%|          | 208/127247 [11:51<128:37:33,  3.64s/it]  0%|          | 209/127247 [11:58<156:15:17,  4.43s/it]  0%|          | 209/127247 [11:58<156:12:17,  4.43s/it]  0%|          | 210/127247 [12:04<172:46:33,  4.90s/it]  0%|          | 210/127247 [12:04<172:44:33,  4.90s/it]  0%|          | 211/127247 [12:06<148:41:49,  4.21s/it]  0%|          | 211/127247 [12:06<148:40:30,  4.21s/it]  0%|          | 212/127247 [12:08<120:59:27,  3.43s/it]  0%|          | 212/127247 [12:08<120:59:30,  3.43s/it]  0%|          | 213/127247 [12:10<101:47:23,  2.88s/it]  0%|          | 213/127247 [12:10<101:50:39,  2.89s/it]  0%|          | 214/127247 [12:12<92:43:41,  2.63s/it]   0%|          | 214/127247 [12:12<92:42:13,  2.63s/it]   0%|          | 215/127247 [12:17<122:28:49,  3.47s/it]  0%|          | 215/127247 [12:17<122:28:00,  3.47s/it]  0%|          | 216/127247 [12:24<155:18:54,  4.40s/it]  0%|          | 216/127247 [12:24<155:17:24,  4.40s/it]  0%|          | 217/127247 [12:25<125:42:27,  3.56s/it]  0%|          | 217/127247 [12:25<125:41:19,  3.56s/it]  0%|          | 218/127247 [12:27<104:58:10,  2.97s/it]  0%|          | 218/127247 [12:27<104:58:24,  2.97s/it]  0%|          | 219/127247 [12:28<90:22:27,  2.56s/it]   0%|          | 219/127247 [12:28<90:21:40,  2.56s/it]   0%|          | 220/127247 [12:30<80:11:55,  2.27s/it]2025-08-18 15:19:46,975 - PixArt - INFO - Step/Epoch [220/1][220/127247]:total_eta: 9 days, 23:50:46, epoch_eta:4 days, 23:49:12, time_all:2.940, time_data:0.170, lr:2.200e-06, s:(32, 32), loss:1.1119, grad_norm:1.8333, repa_loss:-0.4499
  0%|          | 220/127247 [12:30<80:11:41,  2.27s/it]  0%|          | 221/127247 [12:32<73:12:28,  2.07s/it]  0%|          | 221/127247 [12:32<73:12:28,  2.07s/it]  0%|          | 222/127247 [12:33<68:14:19,  1.93s/it]  0%|          | 222/127247 [12:33<68:15:15,  1.93s/it]  0%|          | 223/127247 [12:35<64:50:51,  1.84s/it]  0%|          | 223/127247 [12:35<64:52:22,  1.84s/it]  0%|          | 224/127247 [12:40<103:58:40,  2.95s/it]  0%|          | 224/127247 [12:40<103:58:58,  2.95s/it]  0%|          | 225/127247 [12:46<133:24:54,  3.78s/it]  0%|          | 225/127247 [12:46<133:26:21,  3.78s/it]  0%|          | 226/127247 [12:52<153:49:20,  4.36s/it]  0%|          | 226/127247 [12:52<153:48:14,  4.36s/it]  0%|          | 227/127247 [12:53<124:40:27,  3.53s/it]  0%|          | 227/127247 [12:53<124:39:21,  3.53s/it]  0%|          | 228/127247 [12:55<104:19:15,  2.96s/it]  0%|          | 228/127247 [12:55<104:18:33,  2.96s/it]  0%|          | 229/127247 [12:57<89:54:56,  2.55s/it]   0%|          | 229/127247 [12:57<89:54:59,  2.55s/it]   0%|          | 230/127247 [12:58<79:55:00,  2.27s/it]  0%|          | 230/127247 [12:58<79:55:18,  2.27s/it]  0%|          | 231/127247 [13:00<72:53:54,  2.07s/it]  0%|          | 231/127247 [13:00<72:53:39,  2.07s/it]  0%|          | 232/127247 [13:01<67:59:59,  1.93s/it]  0%|          | 232/127247 [13:01<67:59:24,  1.93s/it]  0%|          | 233/127247 [13:06<95:00:21,  2.69s/it]  0%|          | 233/127247 [13:06<95:00:47,  2.69s/it]  0%|          | 234/127247 [13:11<118:59:57,  3.37s/it]  0%|          | 234/127247 [13:11<119:00:09,  3.37s/it]  0%|          | 235/127247 [13:18<154:39:17,  4.38s/it]  0%|          | 235/127247 [13:18<154:39:34,  4.38s/it]  0%|          | 236/127247 [13:24<176:06:41,  4.99s/it]  0%|          | 236/127247 [13:24<176:06:12,  4.99s/it]  0%|          | 237/127247 [13:30<185:57:11,  5.27s/it]  0%|          | 237/127247 [13:30<185:56:41,  5.27s/it]  0%|          | 238/127247 [13:32<147:05:29,  4.17s/it]  0%|          | 238/127247 [13:32<147:05:40,  4.17s/it]  0%|          | 239/127247 [13:33<119:58:04,  3.40s/it]  0%|          | 239/127247 [13:33<120:06:25,  3.40s/it]  0%|          | 240/127247 [13:35<101:04:21,  2.86s/it]2025-08-18 15:20:51,734 - PixArt - INFO - Step/Epoch [240/1][240/127247]:total_eta: 9 days, 22:54:08, epoch_eta:4 days, 23:20:21, time_all:3.238, time_data:0.170, lr:2.400e-06, s:(32, 32), loss:1.1501, grad_norm:1.4392, repa_loss:-0.4571
  0%|          | 240/127247 [13:35<101:04:17,  2.86s/it]  0%|          | 241/127247 [13:39<112:19:55,  3.18s/it]  0%|          | 241/127247 [13:39<112:18:23,  3.18s/it]  0%|          | 242/127247 [13:43<125:47:08,  3.57s/it]  0%|          | 242/127247 [13:43<125:45:21,  3.56s/it]  0%|          | 243/127247 [13:47<130:45:46,  3.71s/it]  0%|          | 243/127247 [13:47<130:47:24,  3.71s/it]  0%|          | 244/127247 [13:53<147:25:59,  4.18s/it]  0%|          | 244/127247 [13:52<147:24:12,  4.18s/it]  0%|          | 245/127247 [13:59<168:17:29,  4.77s/it]  0%|          | 245/127247 [13:59<168:17:21,  4.77s/it]  0%|          | 246/127247 [14:05<184:52:22,  5.24s/it]  0%|          | 246/127247 [14:05<184:51:03,  5.24s/it]  0%|          | 247/127247 [14:07<146:17:11,  4.15s/it]  0%|          | 247/127247 [14:07<146:16:32,  4.15s/it]  0%|          | 248/127247 [14:08<119:25:15,  3.39s/it]  0%|          | 248/127247 [14:08<119:23:33,  3.38s/it]  0%|          | 249/127247 [14:10<100:30:45,  2.85s/it]  0%|          | 249/127247 [14:10<100:30:37,  2.85s/it]  0%|          | 250/127247 [14:11<87:27:13,  2.48s/it]   0%|          | 250/127247 [14:11<87:29:46,  2.48s/it]   0%|          | 251/127247 [14:13<78:15:14,  2.22s/it]  0%|          | 251/127247 [14:13<78:16:02,  2.22s/it]  0%|          | 252/127247 [14:16<82:39:24,  2.34s/it]  0%|          | 252/127247 [14:16<82:40:25,  2.34s/it]  0%|          | 253/127247 [14:17<74:53:17,  2.12s/it]  0%|          | 253/127247 [14:17<74:53:09,  2.12s/it]  0%|          | 254/127247 [14:19<69:22:05,  1.97s/it]  0%|          | 254/127247 [14:19<69:20:27,  1.97s/it]  0%|          | 255/127247 [14:24<102:33:14,  2.91s/it]  0%|          | 255/127247 [14:24<102:32:22,  2.91s/it]  0%|          | 256/127247 [14:30<130:40:59,  3.70s/it]  0%|          | 256/127247 [14:29<130:39:31,  3.70s/it]  0%|          | 257/127247 [14:35<152:13:12,  4.32s/it]  0%|          | 257/127247 [14:35<152:13:50,  4.32s/it]  0%|          | 258/127247 [14:41<171:58:38,  4.88s/it]  0%|          | 258/127247 [14:41<171:57:21,  4.87s/it]  0%|          | 259/127247 [14:46<164:57:24,  4.68s/it]  0%|          | 259/127247 [14:46<164:57:07,  4.68s/it]2025-08-18 15:22:06,214 - PixArt - INFO - Step/Epoch [260/1][260/127247]:total_eta: 10 days, 0:43:50, epoch_eta:5 days, 0:14:35, time_all:3.724, time_data:0.170, lr:2.600e-06, s:(32, 32), loss:1.0534, grad_norm:1.4656, repa_loss:-0.4588
  0%|          | 260/127247 [14:49<153:44:46,  4.36s/it]  0%|          | 260/127247 [14:49<153:45:01,  4.36s/it]  0%|          | 261/127247 [14:51<124:32:37,  3.53s/it]  0%|          | 261/127247 [14:51<124:31:52,  3.53s/it]  0%|          | 262/127247 [14:52<104:14:03,  2.96s/it]  0%|          | 262/127247 [14:52<104:13:28,  2.95s/it]  0%|          | 263/127247 [14:54<90:02:36,  2.55s/it]   0%|          | 263/127247 [14:54<90:02:33,  2.55s/it]   0%|          | 264/127247 [14:56<79:54:12,  2.27s/it]  0%|          | 264/127247 [14:56<80:01:21,  2.27s/it]  0%|          | 265/127247 [14:57<73:05:11,  2.07s/it]  0%|          | 265/127247 [14:57<73:04:45,  2.07s/it]  0%|          | 266/127247 [15:01<93:48:54,  2.66s/it]  0%|          | 266/127247 [15:01<93:48:55,  2.66s/it]  0%|          | 267/127247 [15:07<120:40:52,  3.42s/it]  0%|          | 267/127247 [15:06<120:39:54,  3.42s/it]  0%|          | 268/127247 [15:13<149:31:09,  4.24s/it]  0%|          | 268/127247 [15:13<149:29:40,  4.24s/it]  0%|          | 269/127247 [15:19<168:18:40,  4.77s/it]  0%|          | 269/127247 [15:19<168:20:35,  4.77s/it]  0%|          | 270/127247 [15:24<177:28:39,  5.03s/it]  0%|          | 270/127247 [15:24<177:26:30,  5.03s/it]  0%|          | 271/127247 [15:26<141:09:05,  4.00s/it]  0%|          | 271/127247 [15:26<141:07:38,  4.00s/it]  0%|          | 272/127247 [15:28<116:03:41,  3.29s/it]  0%|          | 272/127247 [15:28<116:05:35,  3.29s/it]  0%|          | 273/127247 [15:29<98:15:31,  2.79s/it]   0%|          | 273/127247 [15:29<98:14:19,  2.79s/it]   0%|          | 274/127247 [15:32<101:14:42,  2.87s/it]  0%|          | 274/127247 [15:32<101:14:47,  2.87s/it]  0%|          | 275/127247 [15:37<121:01:27,  3.43s/it]  0%|          | 275/127247 [15:37<121:01:22,  3.43s/it]  0%|          | 276/127247 [15:43<147:30:07,  4.18s/it]  0%|          | 276/127247 [15:43<147:28:44,  4.18s/it]  0%|          | 277/127247 [15:49<170:39:13,  4.84s/it]  0%|          | 277/127247 [15:49<170:39:58,  4.84s/it]  0%|          | 278/127247 [15:51<136:28:10,  3.87s/it]  0%|          | 278/127247 [15:51<136:26:37,  3.87s/it]  0%|          | 279/127247 [15:53<112:27:07,  3.19s/it]  0%|          | 279/127247 [15:52<112:26:23,  3.19s/it]  0%|          | 280/127247 [15:54<95:56:45,  2.72s/it] 2025-08-18 15:23:11,070 - PixArt - INFO - Step/Epoch [280/1][280/127247]:total_eta: 9 days, 23:52:38, epoch_eta:4 days, 23:48:27, time_all:3.243, time_data:0.171, lr:2.800e-06, s:(32, 32), loss:1.0160, grad_norm:1.3887, repa_loss:-0.4642
  0%|          | 280/127247 [15:54<95:56:45,  2.72s/it]   0%|          | 281/127247 [15:56<84:09:40,  2.39s/it]  0%|          | 281/127247 [15:56<84:13:51,  2.39s/it]  0%|          | 282/127247 [16:00<104:30:52,  2.96s/it]  0%|          | 282/127247 [16:00<104:29:33,  2.96s/it]  0%|          | 283/127247 [16:05<125:17:18,  3.55s/it]  0%|          | 283/127247 [16:05<125:16:31,  3.55s/it]  0%|          | 284/127247 [16:11<146:30:23,  4.15s/it]  0%|          | 284/127247 [16:10<146:31:50,  4.15s/it]  0%|          | 285/127247 [16:17<172:19:32,  4.89s/it]  0%|          | 285/127247 [16:17<172:18:36,  4.89s/it]  0%|          | 286/127247 [16:21<164:31:57,  4.67s/it]  0%|          | 286/127247 [16:21<164:30:22,  4.66s/it]  0%|          | 287/127247 [16:23<132:05:57,  3.75s/it]  0%|          | 287/127247 [16:23<132:04:51,  3.75s/it]  0%|          | 288/127247 [16:24<109:25:41,  3.10s/it]  0%|          | 288/127247 [16:24<109:30:59,  3.11s/it]  0%|          | 289/127247 [16:26<93:38:03,  2.66s/it]   0%|          | 289/127247 [16:26<93:42:48,  2.66s/it]   0%|          | 290/127247 [16:31<113:36:24,  3.22s/it]  0%|          | 290/127247 [16:31<113:33:01,  3.22s/it]  0%|          | 291/127247 [16:36<135:56:52,  3.85s/it]  0%|          | 291/127247 [16:36<135:55:37,  3.85s/it]  0%|          | 292/127247 [16:42<155:04:27,  4.40s/it]  0%|          | 292/127247 [16:42<155:04:31,  4.40s/it]  0%|          | 293/127247 [16:47<169:00:27,  4.79s/it]  0%|          | 293/127247 [16:47<168:57:53,  4.79s/it]  0%|          | 294/127247 [16:51<160:29:37,  4.55s/it]  0%|          | 294/127247 [16:51<160:28:20,  4.55s/it]  0%|          | 295/127247 [16:53<129:15:47,  3.67s/it]  0%|          | 295/127247 [16:53<129:15:15,  3.67s/it]  0%|          | 296/127247 [16:55<107:31:56,  3.05s/it]  0%|          | 296/127247 [16:54<107:30:32,  3.05s/it]  0%|          | 297/127247 [16:56<92:10:00,  2.61s/it]   0%|          | 297/127247 [16:56<92:14:04,  2.62s/it]   0%|          | 298/127247 [17:00<107:15:03,  3.04s/it]  0%|          | 298/127247 [17:00<107:13:40,  3.04s/it]  0%|          | 299/127247 [17:05<120:47:33,  3.43s/it]  0%|          | 299/127247 [17:04<120:48:12,  3.43s/it]  0%|          | 300/127247 [17:10<138:10:52,  3.92s/it]2025-08-18 15:24:26,509 - PixArt - INFO - Step/Epoch [300/1][300/127247]:total_eta: 10 days, 1:37:03, epoch_eta:5 days, 0:40:01, time_all:3.772, time_data:0.170, lr:3.000e-06, s:(32, 32), loss:0.9357, grad_norm:1.0253, repa_loss:-0.4641
  0%|          | 300/127247 [17:10<138:10:27,  3.92s/it]  0%|          | 301/127247 [17:15<155:56:10,  4.42s/it]  0%|          | 301/127247 [17:15<155:55:02,  4.42s/it]  0%|          | 302/127247 [17:17<129:57:16,  3.69s/it]  0%|          | 302/127247 [17:17<129:56:02,  3.68s/it]  0%|          | 303/127247 [17:19<107:59:11,  3.06s/it]  0%|          | 303/127247 [17:19<107:58:51,  3.06s/it]  0%|          | 304/127247 [17:20<92:47:08,  2.63s/it]   0%|          | 304/127247 [17:20<92:46:24,  2.63s/it]   0%|          | 305/127247 [17:22<81:59:58,  2.33s/it]  0%|          | 305/127247 [17:22<82:08:41,  2.33s/it]  0%|          | 306/127247 [17:27<108:06:29,  3.07s/it]  0%|          | 306/127247 [17:27<108:03:34,  3.06s/it]  0%|          | 307/127247 [17:33<136:33:21,  3.87s/it]  0%|          | 307/127247 [17:32<136:32:27,  3.87s/it]  0%|          | 308/127247 [17:39<161:47:19,  4.59s/it]  0%|          | 308/127247 [17:39<161:46:37,  4.59s/it]  0%|          | 309/127247 [17:45<178:13:51,  5.05s/it]  0%|          | 309/127247 [17:45<179:27:31,  5.09s/it]  0%|          | 310/127247 [17:47<142:58:13,  4.05s/it]  0%|          | 310/127247 [17:47<142:36:14,  4.04s/it]  0%|          | 311/127247 [17:48<117:06:01,  3.32s/it]  0%|          | 311/127247 [17:48<116:48:51,  3.31s/it]  0%|          | 312/127247 [17:50<98:55:02,  2.81s/it]   0%|          | 312/127247 [17:50<98:43:51,  2.80s/it]   0%|          | 313/127247 [17:51<86:15:53,  2.45s/it]  0%|          | 313/127247 [17:51<86:07:12,  2.44s/it]  0%|          | 314/127247 [17:55<97:01:27,  2.75s/it]  0%|          | 314/127247 [17:55<96:59:22,  2.75s/it]  0%|          | 315/127247 [18:00<119:40:56,  3.39s/it]  0%|          | 315/127247 [18:00<119:36:02,  3.39s/it]  0%|          | 316/127247 [18:06<148:28:22,  4.21s/it]  0%|          | 316/127247 [18:06<148:25:33,  4.21s/it]  0%|          | 317/127247 [18:10<151:14:48,  4.29s/it]  0%|          | 317/127247 [18:10<151:11:45,  4.29s/it]  0%|          | 318/127247 [18:12<122:49:31,  3.48s/it]  0%|          | 318/127247 [18:12<122:47:47,  3.48s/it]  0%|          | 319/127247 [18:14<103:03:27,  2.92s/it]  0%|          | 319/127247 [18:14<103:09:15,  2.93s/it]  0%|          | 320/127247 [18:18<114:23:38,  3.24s/it]2025-08-18 15:25:34,570 - PixArt - INFO - Step/Epoch [320/1][320/127247]:total_eta: 10 days, 1:30:56, epoch_eta:5 days, 0:36:24, time_all:3.403, time_data:0.170, lr:3.200e-06, s:(32, 32), loss:0.9909, grad_norm:0.9042, repa_loss:-0.4706
  0%|          | 320/127247 [18:18<114:22:06,  3.24s/it]  0%|          | 321/127247 [18:22<128:46:23,  3.65s/it]  0%|          | 321/127247 [18:22<128:44:10,  3.65s/it]  0%|          | 322/127247 [18:28<152:39:13,  4.33s/it]  0%|          | 322/127247 [18:28<152:41:43,  4.33s/it]  0%|          | 323/127247 [18:35<177:11:45,  5.03s/it]  0%|          | 323/127247 [18:35<177:09:44,  5.02s/it]  0%|          | 324/127247 [18:39<164:10:19,  4.66s/it]  0%|          | 324/127247 [18:39<164:08:45,  4.66s/it]  0%|          | 325/127247 [18:40<131:50:16,  3.74s/it]  0%|          | 325/127247 [18:40<131:48:34,  3.74s/it]  0%|          | 326/127247 [18:42<109:19:27,  3.10s/it]  0%|          | 326/127247 [18:42<109:19:39,  3.10s/it]  0%|          | 327/127247 [18:43<93:37:20,  2.66s/it]   0%|          | 327/127247 [18:43<93:39:15,  2.66s/it]   0%|          | 328/127247 [18:45<82:38:58,  2.34s/it]  0%|          | 328/127247 [18:45<82:39:43,  2.34s/it]  0%|          | 329/127247 [18:47<75:16:27,  2.14s/it]  0%|          | 329/127247 [18:47<75:17:50,  2.14s/it]  0%|          | 330/127247 [18:49<76:33:59,  2.17s/it]  0%|          | 330/127247 [18:49<76:36:53,  2.17s/it]  0%|          | 331/127247 [18:54<102:48:35,  2.92s/it]  0%|          | 331/127247 [18:54<102:48:24,  2.92s/it]  0%|          | 332/127247 [18:59<130:40:55,  3.71s/it]  0%|          | 332/127247 [18:59<130:39:51,  3.71s/it]  0%|          | 333/127247 [19:05<155:43:38,  4.42s/it]  0%|          | 333/127247 [19:05<155:40:48,  4.42s/it]  0%|          | 334/127247 [19:08<138:38:32,  3.93s/it]  0%|          | 334/127247 [19:08<138:36:37,  3.93s/it]  0%|          | 335/127247 [19:10<114:10:27,  3.24s/it]  0%|          | 335/127247 [19:10<114:10:09,  3.24s/it]  0%|          | 336/127247 [19:11<96:56:00,  2.75s/it]   0%|          | 336/127247 [19:11<97:04:53,  2.75s/it]   0%|          | 337/127247 [19:15<106:37:05,  3.02s/it]  0%|          | 337/127247 [19:15<106:33:04,  3.02s/it]  0%|          | 338/127247 [19:21<134:10:40,  3.81s/it]  0%|          | 338/127247 [19:20<134:08:01,  3.80s/it]  0%|          | 339/127247 [19:26<155:30:30,  4.41s/it]  0%|          | 339/127247 [19:26<155:29:43,  4.41s/it]  0%|          | 340/127247 [19:32<172:27:23,  4.89s/it]2025-08-18 15:26:49,322 - PixArt - INFO - Step/Epoch [340/1][340/127247]:total_eta: 10 days, 2:48:30, epoch_eta:5 days, 1:14:34, time_all:3.738, time_data:0.170, lr:3.400e-06, s:(32, 32), loss:0.9548, grad_norm:0.8702, repa_loss:-0.4767
  0%|          | 340/127247 [19:32<172:25:58,  4.89s/it]  0%|          | 341/127247 [19:36<159:37:17,  4.53s/it]  0%|          | 341/127247 [19:36<159:35:55,  4.53s/it]  0%|          | 342/127247 [19:38<128:37:09,  3.65s/it]  0%|          | 342/127247 [19:38<128:36:12,  3.65s/it]  0%|          | 343/127247 [19:39<107:00:53,  3.04s/it]  0%|          | 343/127247 [19:39<107:01:45,  3.04s/it]  0%|          | 344/127247 [19:41<91:50:45,  2.61s/it]   0%|          | 344/127247 [19:41<91:49:47,  2.61s/it]   0%|          | 345/127247 [19:42<81:19:27,  2.31s/it]  0%|          | 345/127247 [19:42<81:18:23,  2.31s/it]  0%|          | 346/127247 [19:44<74:09:12,  2.10s/it]  0%|          | 346/127247 [19:44<74:18:29,  2.11s/it]  0%|          | 347/127247 [19:46<71:31:57,  2.03s/it]  0%|          | 347/127247 [19:46<71:35:36,  2.03s/it]  0%|          | 348/127247 [19:50<95:05:56,  2.70s/it]  0%|          | 348/127247 [19:50<95:02:29,  2.70s/it]  0%|          | 349/127247 [19:56<126:23:14,  3.59s/it]  0%|          | 349/127247 [19:56<126:20:14,  3.58s/it]  0%|          | 350/127247 [20:02<153:47:29,  4.36s/it]  0%|          | 350/127247 [20:02<153:45:58,  4.36s/it]  0%|          | 351/127247 [20:08<171:00:21,  4.85s/it]  0%|          | 351/127247 [20:08<170:58:37,  4.85s/it]  0%|          | 352/127247 [20:10<136:37:46,  3.88s/it]  0%|          | 352/127247 [20:10<136:36:19,  3.88s/it]  0%|          | 353/127247 [20:11<112:33:43,  3.19s/it]  0%|          | 353/127247 [20:11<112:32:50,  3.19s/it]  0%|          | 354/127247 [20:13<95:56:06,  2.72s/it]   0%|          | 354/127247 [20:13<95:59:12,  2.72s/it]   0%|          | 355/127247 [20:17<109:30:35,  3.11s/it]  0%|          | 355/127247 [20:17<109:30:30,  3.11s/it]  0%|          | 356/127247 [20:23<137:33:38,  3.90s/it]  0%|          | 356/127247 [20:23<137:33:08,  3.90s/it]  0%|          | 357/127247 [20:29<162:55:01,  4.62s/it]  0%|          | 357/127247 [20:29<162:53:50,  4.62s/it]  0%|          | 358/127247 [20:35<180:10:59,  5.11s/it]  0%|          | 358/127247 [20:35<180:10:18,  5.11s/it]  0%|          | 359/127247 [20:40<176:46:29,  5.02s/it]  0%|          | 359/127247 [20:40<176:45:24,  5.01s/it]  0%|          | 360/127247 [20:43<151:24:51,  4.30s/it]2025-08-18 15:27:59,532 - PixArt - INFO - Step/Epoch [360/1][360/127247]:total_eta: 10 days, 3:04:04, epoch_eta:5 days, 1:21:45, time_all:3.511, time_data:0.170, lr:3.600e-06, s:(32, 32), loss:0.9343, grad_norm:1.3484, repa_loss:-0.4766
  0%|          | 360/127247 [20:43<151:24:14,  4.30s/it]  0%|          | 361/127247 [20:44<123:00:00,  3.49s/it]  0%|          | 361/127247 [20:44<122:59:37,  3.49s/it]  0%|          | 362/127247 [20:46<103:06:33,  2.93s/it]  0%|          | 362/127247 [20:46<103:06:12,  2.93s/it]  0%|          | 363/127247 [20:47<89:11:43,  2.53s/it]   0%|          | 363/127247 [20:47<89:11:57,  2.53s/it]   0%|          | 364/127247 [20:49<79:29:19,  2.26s/it]  0%|          | 364/127247 [20:49<79:35:04,  2.26s/it]  0%|          | 365/127247 [20:54<110:43:03,  3.14s/it]  0%|          | 365/127247 [20:54<110:40:28,  3.14s/it]  0%|          | 366/127247 [21:00<141:05:58,  4.00s/it]  0%|          | 366/127247 [21:00<141:05:59,  4.00s/it]  0%|          | 367/127247 [21:06<161:17:31,  4.58s/it]  0%|          | 367/127247 [21:06<161:16:20,  4.58s/it]  0%|          | 368/127247 [21:10<149:29:52,  4.24s/it]  0%|          | 368/127247 [21:10<149:29:01,  4.24s/it]  0%|          | 369/127247 [21:11<121:36:19,  3.45s/it]  0%|          | 369/127247 [21:11<121:35:16,  3.45s/it]  0%|          | 370/127247 [21:13<102:06:39,  2.90s/it]  0%|          | 370/127247 [21:13<102:05:34,  2.90s/it]  0%|          | 371/127247 [21:14<88:40:25,  2.52s/it]   0%|          | 371/127247 [21:14<88:41:19,  2.52s/it]   0%|          | 372/127247 [21:20<122:40:49,  3.48s/it]  0%|          | 372/127247 [21:20<122:40:26,  3.48s/it]  0%|          | 373/127247 [21:26<151:07:25,  4.29s/it]  0%|          | 373/127247 [21:26<151:09:20,  4.29s/it]  0%|          | 374/127247 [21:33<170:45:18,  4.85s/it]  0%|          | 374/127247 [21:32<170:44:15,  4.84s/it]  0%|          | 375/127247 [21:34<136:25:11,  3.87s/it]  0%|          | 375/127247 [21:34<136:24:53,  3.87s/it]  0%|          | 376/127247 [21:36<112:15:06,  3.19s/it]  0%|          | 376/127247 [21:36<112:14:24,  3.18s/it]  0%|          | 377/127247 [21:37<95:35:05,  2.71s/it]   0%|          | 377/127247 [21:37<95:34:52,  2.71s/it]   0%|          | 378/127247 [21:39<83:54:36,  2.38s/it]  0%|          | 378/127247 [21:39<83:57:16,  2.38s/it]  0%|          | 379/127247 [21:43<101:06:19,  2.87s/it]  0%|          | 379/127247 [21:43<101:07:56,  2.87s/it]2025-08-18 15:29:05,443 - PixArt - INFO - Step/Epoch [380/1][380/127247]:total_eta: 10 days, 2:30:05, epoch_eta:5 days, 1:04:13, time_all:3.296, time_data:0.170, lr:3.800e-06, s:(32, 32), loss:0.8946, grad_norm:1.2555, repa_loss:-0.4811
  0%|          | 380/127247 [21:49<129:45:55,  3.68s/it]  0%|          | 380/127247 [21:48<129:46:12,  3.68s/it]  0%|          | 381/127247 [21:55<154:09:51,  4.37s/it]  0%|          | 381/127247 [21:54<154:09:10,  4.37s/it]  0%|          | 382/127247 [22:00<167:29:22,  4.75s/it]  0%|          | 382/127247 [22:00<167:28:18,  4.75s/it]  0%|          | 383/127247 [22:04<155:23:59,  4.41s/it]  0%|          | 383/127247 [22:04<155:22:39,  4.41s/it]  0%|          | 384/127247 [22:05<125:38:32,  3.57s/it]  0%|          | 384/127247 [22:05<125:37:30,  3.56s/it]  0%|          | 385/127247 [22:07<104:58:02,  2.98s/it]  0%|          | 385/127247 [22:07<104:58:46,  2.98s/it]  0%|          | 386/127247 [22:09<90:34:07,  2.57s/it]   0%|          | 386/127247 [22:09<90:44:44,  2.58s/it]   0%|          | 387/127247 [22:13<106:00:02,  3.01s/it]  0%|          | 387/127247 [22:13<105:56:02,  3.01s/it]  0%|          | 388/127247 [22:18<126:23:46,  3.59s/it]  0%|          | 388/127247 [22:17<126:23:00,  3.59s/it]  0%|          | 389/127247 [22:24<151:43:58,  4.31s/it]  0%|          | 389/127247 [22:23<151:42:44,  4.31s/it]  0%|          | 390/127247 [22:30<171:51:39,  4.88s/it]  0%|          | 390/127247 [22:30<171:49:43,  4.88s/it]  0%|          | 391/127247 [22:31<137:12:34,  3.89s/it]  0%|          | 391/127247 [22:31<137:11:33,  3.89s/it]  0%|          | 392/127247 [22:33<113:02:55,  3.21s/it]  0%|          | 392/127247 [22:33<113:02:38,  3.21s/it]  0%|          | 393/127247 [22:35<96:21:55,  2.73s/it]   0%|          | 393/127247 [22:34<96:21:24,  2.73s/it]   0%|          | 394/127247 [22:36<84:23:55,  2.40s/it]  0%|          | 394/127247 [22:36<84:24:27,  2.40s/it]  0%|          | 395/127247 [22:40<94:32:35,  2.68s/it]  0%|          | 395/127247 [22:39<94:38:53,  2.69s/it]  0%|          | 396/127247 [22:46<132:07:35,  3.75s/it]  0%|          | 396/127247 [22:46<132:05:16,  3.75s/it]  0%|          | 397/127247 [22:52<157:18:37,  4.46s/it]  0%|          | 397/127247 [22:52<157:19:43,  4.46s/it]  0%|          | 398/127247 [22:53<127:03:35,  3.61s/it]  0%|          | 398/127247 [22:53<127:02:29,  3.61s/it]  0%|          | 399/127247 [22:55<105:54:52,  3.01s/it]  0%|          | 399/127247 [22:55<105:52:43,  3.00s/it]  0%|          | 400/127247 [22:58<109:41:36,  3.11s/it]2025-08-18 15:30:15,399 - PixArt - INFO - Step/Epoch [400/1][400/127247]:total_eta: 10 days, 2:42:05, epoch_eta:5 days, 1:09:38, time_all:3.498, time_data:0.170, lr:4.000e-06, s:(32, 32), loss:0.8149, grad_norm:1.1628, repa_loss:-0.4820
  0%|          | 400/127247 [22:58<109:41:35,  3.11s/it]  0%|          | 401/127247 [23:04<136:08:29,  3.86s/it]  0%|          | 401/127247 [23:04<136:07:59,  3.86s/it]  0%|          | 402/127247 [23:10<153:47:25,  4.36s/it]  0%|          | 402/127247 [23:10<153:47:35,  4.36s/it]  0%|          | 403/127247 [23:16<173:15:58,  4.92s/it]  0%|          | 403/127247 [23:16<173:15:29,  4.92s/it]  0%|          | 404/127247 [23:17<138:13:23,  3.92s/it]  0%|          | 404/127247 [23:17<138:12:12,  3.92s/it]  0%|          | 405/127247 [23:19<113:43:24,  3.23s/it]  0%|          | 405/127247 [23:19<113:43:40,  3.23s/it]  0%|          | 406/127247 [23:24<129:18:55,  3.67s/it]  0%|          | 406/127247 [23:24<129:19:20,  3.67s/it]  0%|          | 407/127247 [23:29<141:49:54,  4.03s/it]  0%|          | 407/127247 [23:29<141:49:08,  4.03s/it]  0%|          | 408/127247 [23:34<157:26:35,  4.47s/it]  0%|          | 408/127247 [23:34<157:26:15,  4.47s/it]  0%|          | 409/127247 [23:40<172:41:32,  4.90s/it]  0%|          | 409/127247 [23:40<172:41:10,  4.90s/it]  0%|          | 410/127247 [23:46<181:46:57,  5.16s/it]  0%|          | 410/127247 [23:46<181:46:26,  5.16s/it]  0%|          | 411/127247 [23:52<194:19:25,  5.52s/it]  0%|          | 411/127247 [23:52<194:19:08,  5.52s/it]  0%|          | 412/127247 [23:54<160:27:23,  4.55s/it]  0%|          | 412/127247 [23:54<160:28:12,  4.55s/it]  0%|          | 413/127247 [23:56<129:15:27,  3.67s/it]  0%|          | 413/127247 [23:56<129:15:03,  3.67s/it]  0%|          | 414/127247 [23:58<107:31:47,  3.05s/it]  0%|          | 414/127247 [23:58<107:33:19,  3.05s/it]  0%|          | 415/127247 [23:59<92:15:04,  2.62s/it]   0%|          | 415/127247 [23:59<92:16:02,  2.62s/it]   0%|          | 416/127247 [24:04<111:55:55,  3.18s/it]  0%|          | 416/127247 [24:04<111:55:03,  3.18s/it]  0%|          | 417/127247 [24:10<143:24:32,  4.07s/it]  0%|          | 417/127247 [24:10<143:24:48,  4.07s/it]  0%|          | 418/127247 [24:16<164:36:51,  4.67s/it]  0%|          | 418/127247 [24:16<164:36:13,  4.67s/it]  0%|          | 419/127247 [24:18<132:07:26,  3.75s/it]  0%|          | 419/127247 [24:17<132:06:17,  3.75s/it]  0%|          | 420/127247 [24:19<109:27:46,  3.11s/it]2025-08-18 15:31:36,094 - PixArt - INFO - Step/Epoch [420/1][420/127247]:total_eta: 10 days, 4:40:52, epoch_eta:5 days, 2:08:21, time_all:4.035, time_data:0.170, lr:4.200e-06, s:(32, 32), loss:0.8413, grad_norm:1.4012, repa_loss:-0.4777
  0%|          | 420/127247 [24:19<109:30:52,  3.11s/it]  0%|          | 421/127247 [24:21<93:44:04,  2.66s/it]   0%|          | 421/127247 [24:21<93:45:55,  2.66s/it]   0%|          | 422/127247 [24:26<116:46:23,  3.31s/it]  0%|          | 422/127247 [24:26<116:45:41,  3.31s/it]  0%|          | 423/127247 [24:32<144:01:52,  4.09s/it]  0%|          | 423/127247 [24:31<144:00:28,  4.09s/it]  0%|          | 424/127247 [24:38<164:49:07,  4.68s/it]  0%|          | 424/127247 [24:37<164:47:59,  4.68s/it]  0%|          | 425/127247 [24:39<132:12:39,  3.75s/it]  0%|          | 425/127247 [24:39<132:12:09,  3.75s/it]  0%|          | 426/127247 [24:41<109:36:07,  3.11s/it]  0%|          | 426/127247 [24:41<109:35:34,  3.11s/it]  0%|          | 427/127247 [24:43<96:27:27,  2.74s/it]   0%|          | 427/127247 [24:43<96:28:15,  2.74s/it]   0%|          | 428/127247 [24:48<121:18:55,  3.44s/it]  0%|          | 428/127247 [24:48<121:19:08,  3.44s/it]  0%|          | 429/127247 [24:53<143:37:21,  4.08s/it]  0%|          | 429/127247 [24:53<143:36:19,  4.08s/it]  0%|          | 430/127247 [24:59<162:07:33,  4.60s/it]  0%|          | 430/127247 [24:59<162:07:40,  4.60s/it]  0%|          | 431/127247 [25:03<155:11:25,  4.41s/it]  0%|          | 431/127247 [25:03<155:10:49,  4.41s/it]  0%|          | 432/127247 [25:05<125:29:42,  3.56s/it]  0%|          | 432/127247 [25:05<125:28:44,  3.56s/it]  0%|          | 433/127247 [25:07<110:06:23,  3.13s/it]  0%|          | 433/127247 [25:07<110:06:30,  3.13s/it]  0%|          | 434/127247 [25:12<128:29:40,  3.65s/it]  0%|          | 434/127247 [25:12<128:30:23,  3.65s/it]  0%|          | 435/127247 [25:18<153:45:53,  4.37s/it]  0%|          | 435/127247 [25:18<153:47:29,  4.37s/it]  0%|          | 436/127247 [25:24<173:48:55,  4.93s/it]  0%|          | 436/127247 [25:24<173:48:02,  4.93s/it]  0%|          | 437/127247 [25:27<149:35:24,  4.25s/it]  0%|          | 437/127247 [25:27<149:34:26,  4.25s/it]  0%|          | 438/127247 [25:28<121:39:44,  3.45s/it]  0%|          | 438/127247 [25:28<121:39:19,  3.45s/it]  0%|          | 439/127247 [25:30<107:58:37,  3.07s/it]  0%|          | 439/127247 [25:30<107:59:13,  3.07s/it]2025-08-18 15:32:48,879 - PixArt - INFO - Step/Epoch [440/1][440/127247]:total_eta: 10 days, 5:12:48, epoch_eta:5 days, 2:23:43, time_all:3.639, time_data:0.170, lr:4.400e-06, s:(32, 32), loss:0.8594, grad_norm:1.4942, repa_loss:-0.4843
  0%|          | 440/127247 [25:32<92:34:35,  2.63s/it]   0%|          | 440/127247 [25:32<92:34:07,  2.63s/it]   0%|          | 441/127247 [25:34<81:46:10,  2.32s/it]  0%|          | 441/127247 [25:33<81:45:55,  2.32s/it]  0%|          | 442/127247 [25:35<74:21:34,  2.11s/it]  0%|          | 442/127247 [25:35<74:21:33,  2.11s/it]  0%|          | 443/127247 [25:39<95:29:58,  2.71s/it]  0%|          | 443/127247 [25:39<95:31:30,  2.71s/it]  0%|          | 444/127247 [25:46<134:00:08,  3.80s/it]  0%|          | 444/127247 [25:46<134:00:13,  3.80s/it]  0%|          | 445/127247 [25:52<159:38:40,  4.53s/it]  0%|          | 445/127247 [25:52<159:39:25,  4.53s/it]  0%|          | 446/127247 [25:59<182:11:03,  5.17s/it]  0%|          | 446/127247 [25:58<182:11:09,  5.17s/it]  0%|          | 447/127247 [26:01<153:10:02,  4.35s/it]  0%|          | 447/127247 [26:01<153:08:40,  4.35s/it]  0%|          | 448/127247 [26:03<124:14:15,  3.53s/it]  0%|          | 448/127247 [26:03<124:13:36,  3.53s/it]  0%|          | 449/127247 [26:04<103:53:08,  2.95s/it]  0%|          | 449/127247 [26:04<103:54:18,  2.95s/it]  0%|          | 450/127247 [26:09<118:39:58,  3.37s/it]  0%|          | 450/127247 [26:08<118:40:37,  3.37s/it]  0%|          | 451/127247 [26:14<141:58:15,  4.03s/it]  0%|          | 451/127247 [26:14<141:59:44,  4.03s/it]  0%|          | 452/127247 [26:20<160:59:11,  4.57s/it]  0%|          | 452/127247 [26:20<161:00:40,  4.57s/it]  0%|          | 453/127247 [26:22<129:38:17,  3.68s/it]  0%|          | 453/127247 [26:21<129:37:13,  3.68s/it]  0%|          | 454/127247 [26:23<107:40:17,  3.06s/it]  0%|          | 454/127247 [26:23<107:39:06,  3.06s/it]  0%|          | 455/127247 [26:26<107:46:59,  3.06s/it]  0%|          | 455/127247 [26:26<107:50:01,  3.06s/it]  0%|          | 456/127247 [26:32<138:11:16,  3.92s/it]  0%|          | 456/127247 [26:32<138:09:06,  3.92s/it]  0%|          | 457/127247 [26:38<162:52:30,  4.62s/it]  0%|          | 457/127247 [26:38<162:51:53,  4.62s/it]  0%|          | 458/127247 [26:44<177:09:42,  5.03s/it]  0%|          | 458/127247 [26:44<177:07:38,  5.03s/it]  0%|          | 459/127247 [26:46<140:48:59,  4.00s/it]  0%|          | 459/127247 [26:46<140:48:01,  4.00s/it]  0%|          | 460/127247 [26:48<115:30:15,  3.28s/it]2025-08-18 15:34:04,503 - PixArt - INFO - Step/Epoch [460/1][460/127247]:total_eta: 10 days, 6:07:56, epoch_eta:5 days, 2:50:39, time_all:3.781, time_data:0.170, lr:4.600e-06, s:(32, 32), loss:0.8121, grad_norm:1.0523, repa_loss:-0.4870
  0%|          | 460/127247 [26:47<115:29:50,  3.28s/it]  0%|          | 461/127247 [26:49<97:49:47,  2.78s/it]   0%|          | 461/127247 [26:49<97:53:59,  2.78s/it]   0%|          | 462/127247 [26:51<85:33:26,  2.43s/it]  0%|          | 462/127247 [26:51<85:35:09,  2.43s/it]  0%|          | 463/127247 [26:56<109:45:57,  3.12s/it]  0%|          | 463/127247 [26:55<109:44:22,  3.12s/it]  0%|          | 464/127247 [27:02<141:02:04,  4.00s/it]  0%|          | 464/127247 [27:02<141:00:45,  4.00s/it]  0%|          | 465/127247 [27:08<162:02:01,  4.60s/it]  0%|          | 465/127247 [27:08<162:01:15,  4.60s/it]  0%|          | 466/127247 [27:10<134:05:04,  3.81s/it]  0%|          | 466/127247 [27:09<134:04:28,  3.81s/it]  0%|          | 467/127247 [27:11<110:42:18,  3.14s/it]  0%|          | 467/127247 [27:11<110:41:43,  3.14s/it]  0%|          | 468/127247 [27:13<94:33:47,  2.69s/it]   0%|          | 468/127247 [27:13<94:34:01,  2.69s/it]   0%|          | 469/127247 [27:14<83:15:56,  2.36s/it]  0%|          | 469/127247 [27:14<83:16:59,  2.36s/it]  0%|          | 470/127247 [27:18<101:04:00,  2.87s/it]  0%|          | 470/127247 [27:18<101:04:16,  2.87s/it]  0%|          | 471/127247 [27:24<130:34:43,  3.71s/it]  0%|          | 471/127247 [27:24<130:34:54,  3.71s/it]  0%|          | 472/127247 [27:30<154:49:24,  4.40s/it]  0%|          | 472/127247 [27:30<154:48:30,  4.40s/it]  0%|          | 473/127247 [27:35<162:24:31,  4.61s/it]  0%|          | 473/127247 [27:35<162:23:21,  4.61s/it]  0%|          | 474/127247 [27:37<130:36:15,  3.71s/it]  0%|          | 474/127247 [27:37<130:36:29,  3.71s/it]  0%|          | 475/127247 [27:38<108:25:54,  3.08s/it]  0%|          | 475/127247 [27:38<108:25:41,  3.08s/it]  0%|          | 476/127247 [27:40<92:55:03,  2.64s/it]   0%|          | 476/127247 [27:40<92:56:46,  2.64s/it]   0%|          | 477/127247 [27:46<123:59:41,  3.52s/it]  0%|          | 477/127247 [27:46<123:59:52,  3.52s/it]  0%|          | 478/127247 [27:52<154:18:21,  4.38s/it]  0%|          | 478/127247 [27:52<154:17:18,  4.38s/it]  0%|          | 479/127247 [27:55<141:03:35,  4.01s/it]  0%|          | 479/127247 [27:55<141:02:57,  4.01s/it]  0%|          | 480/127247 [27:57<115:39:33,  3.28s/it]2025-08-18 15:35:13,651 - PixArt - INFO - Step/Epoch [480/1][480/127247]:total_eta: 10 days, 6:01:22, epoch_eta:5 days, 2:46:48, time_all:3.457, time_data:0.170, lr:4.800e-06, s:(32, 32), loss:0.7990, grad_norm:1.4809, repa_loss:-0.4867
  0%|          | 480/127247 [27:57<115:39:22,  3.28s/it]  0%|          | 481/127247 [27:58<97:50:49,  2.78s/it]   0%|          | 481/127247 [27:58<97:49:59,  2.78s/it]   0%|          | 482/127247 [28:00<85:20:40,  2.42s/it]  0%|          | 482/127247 [28:00<85:23:33,  2.43s/it]  0%|          | 483/127247 [28:02<76:44:50,  2.18s/it]  0%|          | 483/127247 [28:01<76:49:00,  2.18s/it]  0%|          | 484/127247 [28:03<70:41:49,  2.01s/it]  0%|          | 484/127247 [28:03<70:42:05,  2.01s/it]  0%|          | 485/127247 [28:07<92:21:09,  2.62s/it]  0%|          | 485/127247 [28:07<92:19:29,  2.62s/it]  0%|          | 486/127247 [28:13<126:35:37,  3.60s/it]  0%|          | 486/127247 [28:13<126:34:54,  3.59s/it]  0%|          | 487/127247 [28:19<147:22:00,  4.19s/it]  0%|          | 487/127247 [28:19<147:30:47,  4.19s/it]  0%|          | 488/127247 [28:25<168:14:25,  4.78s/it]  0%|          | 488/127247 [28:25<168:10:05,  4.78s/it]  0%|          | 489/127247 [28:27<145:34:33,  4.13s/it]  0%|          | 489/127247 [28:27<145:30:24,  4.13s/it]  0%|          | 490/127247 [28:29<118:53:15,  3.38s/it]  0%|          | 490/127247 [28:29<118:51:27,  3.38s/it]  0%|          | 491/127247 [28:31<100:09:08,  2.84s/it]  0%|          | 491/127247 [28:31<100:07:47,  2.84s/it]  0%|          | 492/127247 [28:32<87:04:07,  2.47s/it]   0%|          | 492/127247 [28:32<87:05:24,  2.47s/it]   0%|          | 493/127247 [28:34<77:59:53,  2.22s/it]  0%|          | 493/127247 [28:34<78:05:43,  2.22s/it]  0%|          | 494/127247 [28:38<97:18:27,  2.76s/it]  0%|          | 494/127247 [28:38<97:21:35,  2.77s/it]  0%|          | 495/127247 [28:44<129:30:05,  3.68s/it]  0%|          | 495/127247 [28:44<129:26:36,  3.68s/it]  0%|          | 496/127247 [28:50<156:55:28,  4.46s/it]  0%|          | 496/127247 [28:50<156:52:51,  4.46s/it]  0%|          | 497/127247 [28:52<126:47:51,  3.60s/it]  0%|          | 497/127247 [28:51<126:45:28,  3.60s/it]  0%|          | 498/127247 [28:53<105:36:49,  3.00s/it]  0%|          | 498/127247 [28:53<105:35:23,  3.00s/it]  0%|          | 499/127247 [28:55<91:09:25,  2.59s/it] 2025-08-18 15:36:11,871 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.24it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.50it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.87it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.76it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.01it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.80it/s][A100%|██████████| 13/13 [00:00<00:00, 16.07it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.58it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.49it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.83it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.89it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.97it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.35it/s][A100%|██████████| 13/13 [00:00<00:00, 15.71it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.38it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.00it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.21it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.32it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.23it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.67it/s][A100%|██████████| 13/13 [00:00<00:00, 16.06it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.56it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.68it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.66it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.68it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.51it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.61it/s][A100%|██████████| 13/13 [00:00<00:00, 16.83it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.81it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.62it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.62it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.75it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.84it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.90it/s][A100%|██████████| 13/13 [00:00<00:00, 16.99it/s]
  0%|          | 499/127247 [29:02<166:43:07,  4.74s/it]  0%|          | 499/127247 [29:03<123:01:28,  3.49s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 795, in <module>
[rank0]:     train()
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 206, in train
[rank0]:     posterior = vae.encode(batch[0]).latent_dist
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py", line 46, in wrapper
[rank0]:     return method(self, *args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py", line 278, in encode
[rank0]:     h = self._encode(x)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py", line 252, in _encode
[rank0]:     enc = self.encoder(x)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py", line 168, in forward
[rank0]:     sample = down_block(sample)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1442, in forward
[rank0]:     hidden_states = resnet(hidden_states, temb=None)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/resnet.py", line 327, in forward
[rank0]:     hidden_states = self.norm1(hidden_states)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 313, in forward
[rank0]:     return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/functional.py", line 2965, in group_norm
[rank0]:     return torch.group_norm(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 124.25 MiB is free. Process 177263 has 960.00 MiB memory in use. Including non-PyTorch memory, this process has 30.38 GiB memory in use. Process 65985 has 7.92 GiB memory in use. Of the allocated memory 28.45 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W818 15:36:22.277824536 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[E818 15:45:30.807257558 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
[rank0]:[E818 15:45:30.807541981 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[F818 15:53:30.814407761 ProcessGroupNCCL.cpp:1557] [PG ID 0 PG GUID 0(default_pg) Rank 0] [PG ID 0 PG GUID 0(default_pg) Rank 0] Terminating the process after attempting to dump debug info, due to ProcessGroupNCCL watchdog hang.
W0818 15:53:31.185000 51953 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 51980 closing signal SIGTERM
E0818 15:53:32.865000 51953 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 0 (pid: 51979) of binary: /export/scratch/sheid/miniconda3/envs/mmcv_env/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 207, in <module>
    main()
  File "/export/home/sheid/.local/lib/python3.10/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 203, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 188, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_15:53:31
  host      : compgpu10
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 51979)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 51979
============================================================
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-18 15:53:50,225 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-18 15:53:50,345 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root='/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M',
    image_list_json=['data_info_fixed.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 6
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17]
trainable_blocks = []
skip_connections = False
repa_flag = True
repa_depth = 8
reserve_memory = False
image_list_json = ['data_info_fixed.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False
dino_version = 'dinov2_vitg14'

2025-08-18 15:53:50,345 - PixArt - INFO - World_size: 2, seed: 43
2025-08-18 15:53:50,345 - PixArt - INFO - Initializing: DDP for training
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:28<00:28, 28.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:28<00:28, 28.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 26.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.12s/it]
2025-08-18 15:55:04,873 - PixArt - INFO - vae scale factor: 0.13025
2025-08-18 15:55:04,875 - PixArt - INFO - Preparing Visualization prompt embeddings...
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /export/scratch/sheid/.cache/torch/hub/facebookresearch_dinov2_main
2025-08-18 15:55:47,026 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:55:47,027 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:55:59,324 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-18 15:55:59,324 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-18 15:56:05,955 - PixArt - INFO - PixArtMS Model Parameters: 620,561,056
2025-08-18 15:56:09,541 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/repa_distillation_attempt/PixArt_sigma_xl2_img512_laion_17_15_8/checkpoints/epoch_1_step_38000.pth. Load ema: False.
2025-08-18 15:56:13,708 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-18 15:56:13,712 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias']
2025-08-18 15:56:13,712 - PixArt - WARNING - Unexpected keys: []
2025-08-18 15:56:13,714 - PixArt - INFO - PixArtMS Model Parameters: 556,794,400
2025-08-18 15:56:13,715 - PixArt - INFO - PixArtMS Trainable Model Parameters: 556,794,400
2025-08-18 15:56:13,716 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-18 15:56:13,716 - PixArt - INFO - T5 max token length: 300
2025-08-18 15:56:13,716 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-18 15:56:23,466 - PixArt - INFO - data_info_fixed.json data volume: 2035947
2025-08-18 15:56:47,506 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 33.79 s, length (use/ori): 2035937/2035947
2025-08-18 15:56:47,507 - PixArt - INFO - Automatically adapt lr to 0.00001 (using sqrt scaling rule).
2025-08-18 15:56:47,544 - PixArt - INFO - CAMEWrapper Optimizer: total 396 param groups, 396 are learnable, 0 are fix. Lr group: 396 params with lr 0.00001; Weight decay group: 396 params with weight decay 0.03.
2025-08-18 15:56:47,544 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/127247 [00:00<?, ?it/s]  0%|          | 0/127247 [00:00<?, ?it/s]2025-08-18 15:56:57,108 - PixArt - INFO - Step/Epoch [1/1][1/127247]:total_eta: 10 days, 2:09:10, epoch_eta:5 days, 1:04:37, time_all:0.343, time_data:0.202, lr:1.000e-08, s:(32, 32), loss:12.5751, grad_norm:2.7826, repa_loss:-0.2871
  0%|          | 1/127247 [00:06<241:42:21,  6.84s/it]2025-08-18 15:56:57,134 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.28it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.58it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.59it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.79it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.67it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.62it/s][A100%|██████████| 13/13 [00:00<00:00, 15.77it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.02it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.36it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 13.64it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 13.68it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 13.99it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.19it/s][A100%|██████████| 13/13 [00:00<00:00, 15.19it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.77it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.38it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.30it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.28it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 14.50it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 14.63it/s][A100%|██████████| 13/13 [00:00<00:00, 15.65it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 14.55it/s][A
 31%|███       | 4/13 [00:00<00:00, 14.40it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 14.72it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.13it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.38it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.44it/s][A100%|██████████| 13/13 [00:00<00:00, 16.36it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 15.90it/s][A
 31%|███       | 4/13 [00:00<00:00, 15.44it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 15.56it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 15.55it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 15.51it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 15.59it/s][A100%|██████████| 13/13 [00:00<00:00, 16.78it/s]
  0%|          | 1/127247 [00:13<481:36:00, 13.63s/it]  0%|          | 1/127247 [00:14<510:04:36, 14.43s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 795, in <module>
[rank0]:     train()
[rank0]:   File "/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py", line 206, in train
[rank0]:     posterior = vae.encode(batch[0]).latent_dist
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py", line 46, in wrapper
[rank0]:     return method(self, *args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py", line 278, in encode
[rank0]:     h = self._encode(x)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py", line 252, in _encode
[rank0]:     enc = self.encoder(x)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py", line 168, in forward
[rank0]:     sample = down_block(sample)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1442, in forward
[rank0]:     hidden_states = resnet(hidden_states, temb=None)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/diffusers/models/resnet.py", line 327, in forward
[rank0]:     hidden_states = self.norm1(hidden_states)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 313, in forward
[rank0]:     return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/functional.py", line 2965, in group_norm
[rank0]:     return torch.group_norm(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 650.25 MiB is free. Process 177263 has 960.00 MiB memory in use. Process 79222 has 7.92 GiB memory in use. Including non-PyTorch memory, this process has 29.87 GiB memory in use. Of the allocated memory 28.45 GiB is allocated by PyTorch, and 763.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W818 15:57:07.390968793 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[E818 16:09:50.747226108 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
[rank0]:[E818 16:09:50.747917405 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[F818 16:17:50.751675986 ProcessGroupNCCL.cpp:1557] [PG ID 0 PG GUID 0(default_pg) Rank 0] [PG ID 0 PG GUID 0(default_pg) Rank 0] Terminating the process after attempting to dump debug info, due to ProcessGroupNCCL watchdog hang.
W0818 16:17:51.780000 81146 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 81183 closing signal SIGTERM
E0818 16:17:53.457000 81146 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 0 (pid: 81182) of binary: /export/scratch/sheid/miniconda3/envs/mmcv_env/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 207, in <module>
    main()
  File "/export/home/sheid/.local/lib/python3.10/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 203, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 188, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/pixart_repa/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_16:17:51
  host      : compgpu10
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 81182)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 81182
============================================================
