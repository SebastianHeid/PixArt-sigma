/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-07-23 08:26:22,718 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-07-23 08:26:22,833 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root='/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M',
    image_list_json=['data_info_fixed.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/'
)
image_size = 512
train_batch_size = 16
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = True
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = True
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 12
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=0.0002,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 50000
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/third_pruning_attempt/PixArt_sigma_xl2_img512_laion_17_15_8'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17]
trainable_blocks = [7, 14, 16]
image_list_json = ['data_info_fixed.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False

2025-07-23 08:26:22,833 - PixArt - INFO - World_size: 2, seed: 43
2025-07-23 08:26:22,833 - PixArt - INFO - Initializing: DDP for training
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.58s/it]
2025-07-23 08:27:54,385 - PixArt - INFO - vae scale factor: 0.13025
2025-07-23 08:27:54,386 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-07-23 08:28:02,977 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-07-23 08:28:02,978 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-07-23 08:28:16,519 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-07-23 08:28:16,520 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-07-23 08:28:23,185 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-07-23 08:28:28,578 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-07-23 08:28:29,560 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-07-23 08:28:29,563 - PixArt - WARNING - Missing keys: ['pos_embed']
2025-07-23 08:28:29,563 - PixArt - WARNING - Unexpected keys: []
2025-07-23 08:28:29,567 - PixArt - INFO - PixArtMS Model Parameters: 547,089,440
2025-07-23 08:28:29,568 - PixArt - INFO - PixArtMS Trainable Model Parameters: 63,766,656
2025-07-23 08:28:29,569 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-07-23 08:28:29,569 - PixArt - INFO - T5 max token length: 300
2025-07-23 08:28:29,569 - PixArt - INFO - ratio of real user prompt: 1.0
2025-07-23 08:28:38,795 - PixArt - INFO - data_info_fixed.json data volume: 2035947
2025-07-23 08:29:07,931 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 38.36 s, length (use/ori): 2035937/2035947
2025-07-23 08:29:07,932 - PixArt - WARNING - Using valid_num=0 in config file. Available 40 aspect_ratios: ['0.25', '0.26', '0.27', '0.28', '0.32', '0.33', '0.35', '0.4', '0.42', '0.48', '0.5', '0.52', '0.57', '0.6', '0.68', '0.72', '0.78', '0.82', '0.88', '0.94', '1.0', '1.07', '1.13', '1.21', '1.29', '1.38', '1.46', '1.67', '1.75', '2.0', '2.09', '2.4', '2.5', '2.89', '3.0', '3.11', '3.62', '3.75', '3.88', '4.0']
2025-07-23 08:29:07,933 - PixArt - INFO - Automatically adapt lr to 0.00007 (using sqrt scaling rule).
2025-07-23 08:29:07,990 - PixArt - INFO - CAMEWrapper Optimizer: total 390 param groups, 45 are learnable, 345 are fix. Lr group: 45 params with lr 0.00007; Weight decay group: 45 params with weight decay 0.03.
2025-07-23 08:29:07,990 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/63623 [00:00<?, ?it/s]  0%|          | 0/63623 [00:00<?, ?it/s]2025-07-23 08:29:22,779 - PixArt - INFO - Step/Epoch [1/1][1/63623]:total_eta: 7 days, 18:40:54, epoch_eta:3 days, 21:20:27, time_all:0.528, time_data:0.416, lr:1.414e-07, s:(26, 38), loss:0.6511, grad_norm:1.2578
  0%|          | 1/63623 [00:10<185:54:03, 10.52s/it]2025-07-23 08:29:23,289 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 13.59it/s][A
 31%|███       | 4/13 [00:00<00:00, 13.99it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 13.53it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 13.21it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 13.09it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 13.14it/s][A100%|██████████| 13/13 [00:00<00:00, 14.31it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 12.54it/s][A
 31%|███       | 4/13 [00:00<00:00, 12.69it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 12.40it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 12.62it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 12.87it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 12.89it/s][A100%|██████████| 13/13 [00:00<00:00, 13.77it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 13.97it/s][A
 31%|███       | 4/13 [00:00<00:00, 13.96it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 12.94it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 12.62it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 12.91it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 12.78it/s][A100%|██████████| 13/13 [00:00<00:00, 13.96it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 13.18it/s][A
 31%|███       | 4/13 [00:00<00:00, 13.84it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 13.86it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 14.07it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 13.55it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 12.98it/s][A100%|██████████| 13/13 [00:00<00:00, 14.40it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|█▌        | 2/13 [00:00<00:00, 11.93it/s][A
 31%|███       | 4/13 [00:00<00:00, 12.71it/s][A
 46%|████▌     | 6/13 [00:00<00:00, 13.23it/s][A
 62%|██████▏   | 8/13 [00:00<00:00, 13.64it/s][A
 77%|███████▋  | 10/13 [00:00<00:00, 13.87it/s][A
 92%|█████████▏| 12/13 [00:00<00:00, 13.71it/s][A100%|██████████| 13/13 [00:00<00:00, 14.54it/s]
  0%|          | 1/63623 [00:21<387:28:38, 21.93s/it]  0%|          | 2/63623 [00:24<226:27:43, 12.81s/it]  0%|          | 2/63623 [00:25<191:34:48, 10.84s/it]  0%|          | 3/63623 [00:27<146:23:59,  8.28s/it]  0%|          | 3/63623 [00:27<127:27:51,  7.21s/it]  0%|          | 4/63623 [00:31<111:03:20,  6.28s/it]  0%|          | 4/63623 [00:31<99:33:47,  5.63s/it]   0%|          | 5/63623 [00:33<89:36:52,  5.07s/it]   0%|          | 5/63623 [00:34<82:21:29,  4.66s/it]  0%|          | 6/63623 [00:36<76:53:59,  4.35s/it]  0%|          | 6/63623 [00:36<72:02:13,  4.08s/it]  0%|          | 7/63623 [00:39<68:42:06,  3.89s/it]  0%|          | 7/63623 [00:39<65:28:25,  3.71s/it]  0%|          | 8/63623 [00:42<62:32:12,  3.54s/it]  0%|          | 8/63623 [00:42<60:18:36,  3.41s/it]  0%|          | 9/63623 [00:45<59:23:11,  3.36s/it]  0%|          | 9/63623 [00:45<57:49:53,  3.27s/it]  0%|          | 10/63623 [00:48<57:00:38,  3.23s/it]  0%|          | 10/63623 [00:48<55:58:44,  3.17s/it]  0%|          | 11/63623 [00:51<54:45:17,  3.10s/it]  0%|          | 11/63623 [00:51<54:03:46,  3.06s/it]  0%|          | 12/63623 [00:54<53:48:00,  3.04s/it]  0%|          | 12/63623 [00:54<53:15:19,  3.01s/it]  0%|          | 13/63623 [00:57<53:21:17,  3.02s/it]  0%|          | 13/63623 [00:57<52:59:29,  3.00s/it]  0%|          | 14/63623 [01:00<53:09:53,  3.01s/it]  0%|          | 14/63623 [01:00<52:53:18,  2.99s/it]  0%|          | 15/63623 [01:03<52:56:21,  3.00s/it]  0%|          | 15/63623 [01:03<52:47:40,  2.99s/it]  0%|          | 16/63623 [01:06<52:45:13,  2.99s/it]  0%|          | 16/63623 [01:06<52:36:01,  2.98s/it]  0%|          | 17/63623 [01:09<52:41:55,  2.98s/it]  0%|          | 17/63623 [01:09<52:37:45,  2.98s/it]  0%|          | 18/63623 [01:12<52:41:54,  2.98s/it]  0%|          | 18/63623 [01:12<52:36:57,  2.98s/it]  0%|          | 19/63623 [01:15<52:28:58,  2.97s/it]  0%|          | 19/63623 [01:15<52:26:55,  2.97s/it]2025-07-23 08:30:30,306 - PixArt - INFO - Step/Epoch [20/1][20/63623]:total_eta: 5 days, 11:24:52, epoch_eta:2 days, 17:41:51, time_all:3.376, time_data:0.974, lr:2.828e-06, s:(32, 32), loss:0.4865, grad_norm:0.7874
  0%|          | 20/63623 [01:18<52:31:41,  2.97s/it]  0%|          | 20/63623 [01:18<52:29:55,  2.97s/it]  0%|          | 21/63623 [01:20<52:25:58,  2.97s/it]  0%|          | 21/63623 [01:21<52:24:24,  2.97s/it]  0%|          | 22/63623 [01:23<52:26:30,  2.97s/it]  0%|          | 22/63623 [01:24<52:24:32,  2.97s/it]  0%|          | 23/63623 [01:26<52:28:24,  2.97s/it]  0%|          | 23/63623 [01:27<52:30:10,  2.97s/it]  0%|          | 24/63623 [01:29<52:25:23,  2.97s/it]  0%|          | 24/63623 [01:29<52:24:36,  2.97s/it]  0%|          | 25/63623 [01:32<52:18:59,  2.96s/it]  0%|          | 25/63623 [01:32<52:17:21,  2.96s/it]  0%|          | 26/63623 [01:35<52:20:55,  2.96s/it]  0%|          | 26/63623 [01:35<52:21:56,  2.96s/it]  0%|          | 27/63623 [01:38<52:21:29,  2.96s/it]  0%|          | 27/63623 [01:38<52:21:11,  2.96s/it]  0%|          | 28/63623 [01:41<52:24:41,  2.97s/it]  0%|          | 28/63623 [01:41<52:23:36,  2.97s/it]  0%|          | 29/63623 [01:44<52:19:10,  2.96s/it]  0%|          | 29/63623 [01:44<52:18:08,  2.96s/it]  0%|          | 30/63623 [01:47<52:19:54,  2.96s/it]  0%|          | 30/63623 [01:47<52:19:47,  2.96s/it]  0%|          | 31/63623 [01:50<52:18:14,  2.96s/it]  0%|          | 31/63623 [01:50<52:19:41,  2.96s/it]  0%|          | 32/63623 [01:53<52:28:00,  2.97s/it]  0%|          | 32/63623 [01:53<52:27:20,  2.97s/it]  0%|          | 33/63623 [01:56<52:26:46,  2.97s/it]  0%|          | 33/63623 [01:56<52:27:22,  2.97s/it]  0%|          | 34/63623 [01:59<52:23:02,  2.97s/it]  0%|          | 34/63623 [01:59<52:22:19,  2.96s/it]