/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-21 14:03:12,943 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-21 14:03:13,075 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 4
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/equidistant/PixArt_sigma_xl2_img512_laion_4_6_8_finetuning_on_Pixart/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/equidistant/PixArt_sigma_xl2_img512_laion_4_6_8_10_12'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    4, 6, 8, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [4, 6, 8, 10, 12]
trainable_blocks = [9, 11]
reserve_memory = True
stable_loss = False
self_att_feat_loss_flag = False
image_list_json = ['data_info.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False

2025-08-21 14:03:13,075 - PixArt - INFO - World_size: 1, seed: 43
2025-08-21 14:03:13,075 - PixArt - INFO - Initializing: DDP for training
2025-08-21 14:03:13,075 - PixArt - INFO - vae scale factor: 0.13025
2025-08-21 14:03:13,076 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-21 14:03:13,076 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.75s/it]
2025-08-21 14:03:36,869 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-21 14:03:36,869 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-21 14:03:51,651 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-21 14:03:51,651 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-21 14:03:58,795 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-08-21 14:04:12,168 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/equidistant/PixArt_sigma_xl2_img512_laion_4_6_8_finetuning_on_Pixart/checkpoints/epoch_2_step_12500.pth. Load ema: False.
2025-08-21 14:04:19,407 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-21 14:04:19,410 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.4.scale_shift_table', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.cross_attn.q_linear.weight', 'blocks.4.cross_attn.q_linear.bias', 'blocks.4.cross_attn.kv_linear.weight', 'blocks.4.cross_attn.kv_linear.bias', 'blocks.4.cross_attn.proj.weight', 'blocks.4.cross_attn.proj.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.6.scale_shift_table', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.cross_attn.q_linear.weight', 'blocks.6.cross_attn.q_linear.bias', 'blocks.6.cross_attn.kv_linear.weight', 'blocks.6.cross_attn.kv_linear.bias', 'blocks.6.cross_attn.proj.weight', 'blocks.6.cross_attn.proj.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias']
2025-08-21 14:04:19,410 - PixArt - WARNING - Unexpected keys: []
2025-08-21 14:04:19,415 - PixArt - INFO - PixArtMS Model Parameters: 504,578,336
2025-08-21 14:04:19,416 - PixArt - INFO - PixArtMS Trainable Model Parameters: 42,511,104
2025-08-21 14:04:19,416 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-21 14:04:19,417 - PixArt - INFO - T5 max token length: 300
2025-08-21 14:04:19,417 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-21 14:04:21,887 - PixArt - INFO - data_info.json data volume: 608000
2025-08-21 14:04:30,104 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 10.69 s, length (use/ori): 607997/608000
2025-08-21 14:04:30,114 - PixArt - INFO - Automatically adapt lr to 0.00000 (using sqrt scaling rule).
2025-08-21 14:04:30,162 - PixArt - INFO - CAMEWrapper Optimizer: total 360 param groups, 30 are learnable, 330 are fix. Lr group: 30 params with lr 0.00000; Weight decay group: 30 params with weight decay 0.03.
2025-08-21 14:04:30,162 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/76000 [00:00<?, ?it/s]2025-08-21 14:04:35,963 - PixArt - INFO - Step/Epoch [1/1][1/76000]:total_eta: 1 day, 8:25:53, epoch_eta:1 day, 8:25:54, time_all:0.154, time_data:0.066, lr:3.536e-09, s:(32, 32), loss:0.6468, grad_norm:1.8165
2025-08-21 14:04:36,351 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 11.71it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.68it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.64it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.69it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.71it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.63it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:01, 10.83it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.13it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.38it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.49it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.54it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.57it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.36it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 11.73it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.70it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.70it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.67it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.65it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.59it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.56it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 11.67it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.69it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.55it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.61it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.65it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.67it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.57it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 11.58it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.66it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.53it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.62it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.66it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.55it/s]
  0%|          | 1/76000 [00:12<267:01:44, 12.65s/it]  0%|          | 2/76000 [00:13<124:58:45,  5.92s/it]  0%|          | 3/76000 [00:15<79:06:29,  3.75s/it]   0%|          | 4/76000 [00:16<57:36:46,  2.73s/it]  0%|          | 5/76000 [00:17<45:45:13,  2.17s/it]  0%|          | 6/76000 [00:18<38:33:50,  1.83s/it]  0%|          | 7/76000 [00:19<34:00:37,  1.61s/it]  0%|          | 8/76000 [00:20<31:01:42,  1.47s/it]  0%|          | 9/76000 [00:22<29:03:16,  1.38s/it]  0%|          | 10/76000 [00:23<27:41:43,  1.31s/it]  0%|          | 11/76000 [00:24<26:46:57,  1.27s/it]  0%|          | 12/76000 [00:25<26:07:21,  1.24s/it]  0%|          | 13/76000 [00:26<25:40:56,  1.22s/it]  0%|          | 14/76000 [00:27<25:21:54,  1.20s/it]  0%|          | 15/76000 [00:29<25:10:38,  1.19s/it]  0%|          | 16/76000 [00:30<24:59:27,  1.18s/it]  0%|          | 17/76000 [00:31<24:57:56,  1.18s/it]  0%|          | 18/76000 [00:32<24:55:06,  1.18s/it]  0%|          | 19/76000 [00:33<24:55:37,  1.18s/it]2025-08-21 14:05:07,808 - PixArt - INFO - Step/Epoch [20/1][20/76000]:total_eta: 1 day, 11:05:32, epoch_eta:1 day, 11:05:33, time_all:1.592, time_data:0.481, lr:7.071e-08, s:(32, 32), loss:0.5919, grad_norm:1.8807
  0%|          | 20/76000 [00:34<24:55:23,  1.18s/it]  0%|          | 21/76000 [00:36<24:50:57,  1.18s/it]  0%|          | 22/76000 [00:37<24:49:41,  1.18s/it]  0%|          | 23/76000 [00:38<24:46:47,  1.17s/it]  0%|          | 24/76000 [00:39<24:44:47,  1.17s/it]  0%|          | 25/76000 [00:40<24:47:37,  1.17s/it]  0%|          | 26/76000 [00:41<24:48:39,  1.18s/it]  0%|          | 27/76000 [00:43<24:45:52,  1.17s/it]  0%|          | 28/76000 [00:44<24:48:03,  1.18s/it]  0%|          | 29/76000 [00:45<25:05:00,  1.19s/it]  0%|          | 30/76000 [00:46<25:00:06,  1.18s/it]  0%|          | 31/76000 [00:47<24:56:59,  1.18s/it]  0%|          | 32/76000 [00:49<25:01:49,  1.19s/it]  0%|          | 33/76000 [00:50<24:55:51,  1.18s/it]  0%|          | 34/76000 [00:51<24:57:19,  1.18s/it]  0%|          | 35/76000 [00:52<25:15:43,  1.20s/it]  0%|          | 36/76000 [00:53<25:07:27,  1.19s/it]  0%|          | 37/76000 [00:55<25:00:42,  1.19s/it]  0%|          | 38/76000 [00:56<24:58:20,  1.18s/it]  0%|          | 39/76000 [00:57<24:55:28,  1.18s/it]2025-08-21 14:05:31,441 - PixArt - INFO - Step/Epoch [40/1][40/76000]:total_eta: 1 day, 6:07:54, epoch_eta:1 day, 6:07:55, time_all:1.182, time_data:0.003, lr:1.414e-07, s:(32, 32), loss:0.5682, grad_norm:1.3059
  0%|          | 40/76000 [00:58<24:57:45,  1.18s/it]  0%|          | 41/76000 [00:59<24:57:24,  1.18s/it]  0%|          | 42/76000 [01:00<24:53:45,  1.18s/it]  0%|          | 43/76000 [01:02<24:54:47,  1.18s/it]  0%|          | 44/76000 [01:03<24:56:35,  1.18s/it]  0%|          | 45/76000 [01:04<24:55:04,  1.18s/it]  0%|          | 46/76000 [01:05<24:52:29,  1.18s/it]  0%|          | 47/76000 [01:06<24:53:53,  1.18s/it]  0%|          | 48/76000 [01:07<24:51:56,  1.18s/it]  0%|          | 49/76000 [01:09<24:53:34,  1.18s/it]  0%|          | 50/76000 [01:10<24:55:13,  1.18s/it]  0%|          | 51/76000 [01:11<24:51:51,  1.18s/it]  0%|          | 52/76000 [01:12<24:55:27,  1.18s/it]  0%|          | 53/76000 [01:13<24:55:57,  1.18s/it]  0%|          | 54/76000 [01:15<24:56:56,  1.18s/it]  0%|          | 55/76000 [01:16<24:56:22,  1.18s/it]  0%|          | 56/76000 [01:17<24:55:02,  1.18s/it]  0%|          | 57/76000 [01:18<24:57:20,  1.18s/it]  0%|          | 58/76000 [01:19<24:54:54,  1.18s/it]  0%|          | 59/76000 [01:20<24:55:24,  1.18s/it]2025-08-21 14:05:55,059 - PixArt - INFO - Step/Epoch [60/1][60/76000]:total_eta: 1 day, 4:24:51, epoch_eta:1 day, 4:24:52, time_all:1.181, time_data:0.003, lr:2.121e-07, s:(32, 32), loss:0.4921, grad_norm:1.0023
  0%|          | 60/76000 [01:22<24:55:31,  1.18s/it]  0%|          | 61/76000 [01:23<24:54:30,  1.18s/it]  0%|          | 62/76000 [01:24<24:57:54,  1.18s/it]  0%|          | 63/76000 [01:25<24:59:31,  1.18s/it]  0%|          | 64/76000 [01:26<25:03:22,  1.19s/it]  0%|          | 65/76000 [01:28<25:06:21,  1.19s/it]  0%|          | 66/76000 [01:29<25:05:31,  1.19s/it]  0%|          | 67/76000 [01:30<25:03:10,  1.19s/it]  0%|          | 68/76000 [01:31<24:59:24,  1.18s/it]  0%|          | 69/76000 [01:32<24:56:09,  1.18s/it]  0%|          | 70/76000 [01:34<24:57:15,  1.18s/it]  0%|          | 71/76000 [01:35<24:59:12,  1.18s/it]  0%|          | 72/76000 [01:36<25:00:36,  1.19s/it]  0%|          | 73/76000 [01:37<25:00:06,  1.19s/it]  0%|          | 74/76000 [01:38<25:00:32,  1.19s/it]  0%|          | 75/76000 [01:39<24:59:40,  1.19s/it]  0%|          | 76/76000 [01:41<24:59:42,  1.19s/it]  0%|          | 77/76000 [01:42<24:59:46,  1.19s/it]  0%|          | 78/76000 [01:43<25:00:09,  1.19s/it]  0%|          | 79/76000 [01:44<24:59:33,  1.19s/it]2025-08-21 14:06:18,774 - PixArt - INFO - Step/Epoch [80/1][80/76000]:total_eta: 1 day, 3:34:00, epoch_eta:1 day, 3:34:02, time_all:1.186, time_data:0.003, lr:2.828e-07, s:(32, 32), loss:0.4536, grad_norm:0.8810
  0%|          | 80/76000 [01:45<24:59:43,  1.19s/it]  0%|          | 81/76000 [01:47<25:01:00,  1.19s/it]  0%|          | 82/76000 [01:48<25:03:36,  1.19s/it]  0%|          | 83/76000 [01:49<24:59:51,  1.19s/it]  0%|          | 84/76000 [01:50<25:05:58,  1.19s/it]  0%|          | 85/76000 [01:51<25:04:26,  1.19s/it]  0%|          | 86/76000 [01:53<25:04:53,  1.19s/it]  0%|          | 87/76000 [01:54<25:04:06,  1.19s/it]  0%|          | 88/76000 [01:55<25:00:42,  1.19s/it]  0%|          | 89/76000 [01:56<25:04:04,  1.19s/it]  0%|          | 90/76000 [01:57<25:03:53,  1.19s/it]  0%|          | 91/76000 [01:58<25:03:05,  1.19s/it]  0%|          | 92/76000 [02:00<25:03:46,  1.19s/it]  0%|          | 93/76000 [02:01<25:12:50,  1.20s/it]  0%|          | 94/76000 [02:02<25:06:43,  1.19s/it]  0%|          | 95/76000 [02:03<25:01:30,  1.19s/it]  0%|          | 96/76000 [02:04<25:00:20,  1.19s/it]  0%|          | 97/76000 [02:06<25:01:20,  1.19s/it]  0%|          | 98/76000 [02:07<25:03:19,  1.19s/it]  0%|          | 99/76000 [02:08<25:01:37,  1.19s/it]2025-08-21 14:06:42,552 - PixArt - INFO - Step/Epoch [100/1][100/76000]:total_eta: 1 day, 3:03:57, epoch_eta:1 day, 3:03:58, time_all:1.189, time_data:0.003, lr:3.536e-07, s:(32, 32), loss:0.4123, grad_norm:0.7333
  0%|          | 100/76000 [02:09<25:04:57,  1.19s/it]  0%|          | 101/76000 [02:10<25:09:18,  1.19s/it]  0%|          | 102/76000 [02:12<25:04:45,  1.19s/it]  0%|          | 103/76000 [02:13<25:01:24,  1.19s/it]  0%|          | 104/76000 [02:14<25:03:38,  1.19s/it]  0%|          | 105/76000 [02:15<25:03:19,  1.19s/it]  0%|          | 106/76000 [02:16<25:00:47,  1.19s/it]  0%|          | 107/76000 [02:17<25:00:13,  1.19s/it]  0%|          | 108/76000 [02:19<24:56:22,  1.18s/it]  0%|          | 109/76000 [02:20<24:54:48,  1.18s/it]