/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/distributed/launch.py:207: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/export/home/sheid/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-08-14 16:51:14,448 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

2025-08-14 16:51:14,563 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 2
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'bf16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12501.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 11, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11]
trainable_blocks = []
reserve_memory = False
stable_loss = False
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root='/export/home/sheid/PixArt-sigma/distillation',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-14 16:51:14,563 - PixArt - INFO - World_size: 2, seed: 43
2025-08-14 16:51:14,563 - PixArt - INFO - Initializing: DDP for training
2025-08-14 16:51:14,563 - PixArt - INFO - vae scale factor: 0.13025
2025-08-14 16:51:14,564 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-14 16:51:14,564 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]
2025-08-14 16:51:33,141 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-14 16:51:33,142 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-14 16:51:46,799 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-14 16:51:46,799 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-14 16:51:53,418 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-08-14 16:51:55,739 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12501.pth. Load ema: False.
2025-08-14 16:51:56,641 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-14 16:51:56,643 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias']
2025-08-14 16:51:56,643 - PixArt - WARNING - Unexpected keys: []
2025-08-14 16:51:56,644 - PixArt - INFO - PixArtMS Model Parameters: 504,578,336
2025-08-14 16:51:56,645 - PixArt - INFO - PixArtMS Trainable Model Parameters: 504,578,336
2025-08-14 16:51:56,645 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-14 16:51:56,646 - PixArt - INFO - T5 max token length: 300
2025-08-14 16:51:56,646 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-14 16:51:58,581 - PixArt - INFO - data_info.json data volume: 608000
2025-08-14 16:52:05,318 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 8.67 s, length (use/ori): 607997/608000
2025-08-14 16:52:05,319 - PixArt - INFO - Automatically adapt lr to 0.00001 (using sqrt scaling rule).
2025-08-14 16:52:05,349 - PixArt - INFO - CAMEWrapper Optimizer: total 360 param groups, 360 are learnable, 0 are fix. Lr group: 360 params with lr 0.00001; Weight decay group: 360 params with weight decay 0.03.
2025-08-14 16:52:05,349 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/38000 [00:00<?, ?it/s]  0%|          | 0/38000 [00:00<?, ?it/s]  0%|          | 0/38000 [00:02<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 837, in <module>
[rank0]:     train()
[rank0]:   File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 301, in train
[rank0]:     loss_term = train_diffusion.training_losses(
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 97, in training_losses
[rank0]:     return super().training_losses(self._wrap_model(model), *args, **kwargs)
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/gaussian_diffusion.py", line 830, in training_losses
[rank0]:     model_output, feat_list = model(x_t, t, **model_kwargs, train=True)
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 134, in __call__
[rank0]:     return self.model(x, timestep=new_ts, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 279, in forward
[rank0]:     x = auto_grad_checkpoint(
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/utils.py", line 44, in auto_grad_checkpoint
[rank0]:     return checkpoint(module, *args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 488, in checkpoint
[rank0]:     return CheckpointFunction.apply(function, preserve, *args)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 263, in forward
[rank0]:     outputs = run_function(*args)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 113, in forward
[rank0]:     * self.attn(t2i_modulate(self.norm1(x), shift_msa, scale_msa), HW=HW)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArt_blocks.py", line 153, in forward
[rank0]:     x = xformers.ops.memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 308, in memory_efficient_attention
[rank0]:     return _memory_efficient_attention(
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 469, in _memory_efficient_attention
[rank0]:     return _memory_efficient_attention_forward(
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 488, in _memory_efficient_attention_forward
[rank0]:     op = _dispatch_fw(inp, False)
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py", line 142, in _dispatch_fw
[rank0]:     return _run_priority_list(
[rank0]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py", line 83, in _run_priority_list
[rank0]:     raise NotImplementedError(msg)
[rank0]: NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:
[rank0]:      query       : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank0]:      key         : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank0]:      value       : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank0]:      attn_bias   : <class 'NoneType'>
[rank0]:      p           : 0.0
[rank0]: `fa3F@2.8.0.post2-3-g3ba6f82` is not supported because:
[rank0]:     requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)
[rank0]:     bf16 is only supported on A100+ GPUs
[rank0]: `fa2F@2.5.7-pt` is not supported because:
[rank0]:     requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)
[rank0]:     bf16 is only supported on A100+ GPUs
[rank0]: `cutlassF-pt` is not supported because:
[rank0]:     bf16 is only supported on A100+ GPUs
  0%|          | 0/38000 [00:01<?, ?it/s]
[rank1]: Traceback (most recent call last):
[rank1]:   File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 837, in <module>
[rank1]:     train()
[rank1]:   File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 301, in train
[rank1]:     loss_term = train_diffusion.training_losses(
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 97, in training_losses
[rank1]:     return super().training_losses(self._wrap_model(model), *args, **kwargs)
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/gaussian_diffusion.py", line 830, in training_losses
[rank1]:     model_output, feat_list = model(x_t, t, **model_kwargs, train=True)
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 134, in __call__
[rank1]:     return self.model(x, timestep=new_ts, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 279, in forward
[rank1]:     x = auto_grad_checkpoint(
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/utils.py", line 44, in auto_grad_checkpoint
[rank1]:     return checkpoint(module, *args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/_compile.py", line 51, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 488, in checkpoint
[rank1]:     return CheckpointFunction.apply(function, preserve, *args)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 263, in forward
[rank1]:     outputs = run_function(*args)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 113, in forward
[rank1]:     * self.attn(t2i_modulate(self.norm1(x), shift_msa, scale_msa), HW=HW)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArt_blocks.py", line 153, in forward
[rank1]:     x = xformers.ops.memory_efficient_attention(q, k, v, p=self.attn_drop.p, attn_bias=attn_bias)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 308, in memory_efficient_attention
[rank1]:     return _memory_efficient_attention(
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 469, in _memory_efficient_attention
[rank1]:     return _memory_efficient_attention_forward(
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 488, in _memory_efficient_attention_forward
[rank1]:     op = _dispatch_fw(inp, False)
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py", line 142, in _dispatch_fw
[rank1]:     return _run_priority_list(
[rank1]:   File "/export/scratch/sheid/miniconda3/envs/mmcv_env/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py", line 83, in _run_priority_list
[rank1]:     raise NotImplementedError(msg)
[rank1]: NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:
[rank1]:      query       : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank1]:      key         : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank1]:      value       : shape=(8, 1024, 16, 72) (torch.bfloat16)
[rank1]:      attn_bias   : <class 'NoneType'>
[rank1]:      p           : 0.0
[rank1]: `fa3F@2.8.0.post2-3-g3ba6f82` is not supported because:
[rank1]:     requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)
[rank1]:     bf16 is only supported on A100+ GPUs
[rank1]: `fa2F@2.5.7-pt` is not supported because:
[rank1]:     requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)
[rank1]:     bf16 is only supported on A100+ GPUs
[rank1]: `cutlassF-pt` is not supported because:
[rank1]:     bf16 is only supported on A100+ GPUs
[rank0]:[W814 16:52:11.046716298 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
