/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:01:19,381 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:01:19,517 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 11, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16]
trainable_blocks = [14]
reserve_memory = False
stable_loss = False
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:01:19,517 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:01:19,517 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:01:19,517 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:01:19,518 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:01:19,518 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:24<00:24, 24.33s/it]Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 685, in <module>
    missing, unexpected = load_checkpoint(
  File "/export/home/sheid/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
    checkpoint = torch.load(ckpt_file, map_location="cpu")
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
[2025-08-19 11:01:51,853] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2388040 closing signal SIGTERM
[2025-08-19 11:01:53,923] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2388041) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:01:51
  host      : hcigpu07
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2388041)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:02:18,399 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:02:18,542 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning_old_test'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 11, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16]
trainable_blocks = []
reserve_memory = False
stable_loss = False
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:02:18,543 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:02:18,543 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:02:18,543 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:02:18,543 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:02:18,543 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  4.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.07s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 685, in <module>
    missing, unexpected = load_checkpoint(
  File "/export/home/sheid/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
    checkpoint = torch.load(ckpt_file, map_location="cpu")
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16/checkpoints/epoch_1_step_37990.pth'
[2025-08-19 11:02:51,811] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2388636 closing signal SIGTERM
[2025-08-19 11:02:52,277] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2388637) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:02:51
  host      : hcigpu07
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2388637)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:03:16,193 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:03:16,339 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 11, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16]
trainable_blocks = []
reserve_memory = False
stable_loss = False
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:03:16,339 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:03:16,339 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:03:16,339 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:03:16,339 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:03:16,339 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.95s/it]
2025-08-19 11:03:38,971 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-19 11:03:38,971 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 685, in <module>
    missing, unexpected = load_checkpoint(
  File "/export/home/sheid/PixArt-sigma/diffusion/utils/checkpoint.py", line 52, in load_checkpoint
    checkpoint = torch.load(ckpt_file, map_location="cpu")
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
[2025-08-19 11:03:49,813] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2389017 closing signal SIGTERM
[2025-08-19 11:03:50,528] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2389018) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:03:49
  host      : hcigpu07
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2389018)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:04:14,931 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:04:15,062 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 11, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:04:15,062 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:04:15,062 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:04:15,062 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:04:15,063 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:04:15,063 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.50s/it]
2025-08-19 11:04:35,601 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-19 11:04:35,601 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-19 11:04:50,025 - PixArt - WARNING - position embed interpolation: 1.0, base size: 32
2025-08-19 11:04:50,025 - PixArt - WARNING - kv compress config: {'sampling': None, 'scale_factor': 1, 'kv_compress_layer': []}
2025-08-19 11:04:57,223 - PixArt - INFO - PixArtMS Model Parameters: 610,856,096
2025-08-19 11:04:59,359 - PixArt - INFO - Load checkpoint from /export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning/checkpoints/epoch_1_step_37990.pth. Load ema: False.
2025-08-19 11:05:00,657 - PixArt - INFO - Load checkpoint from /export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth. Load ema: False.
2025-08-19 11:05:00,659 - PixArt - WARNING - Missing keys: ['pos_embed', 'blocks.8.scale_shift_table', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.cross_attn.q_linear.weight', 'blocks.8.cross_attn.q_linear.bias', 'blocks.8.cross_attn.kv_linear.weight', 'blocks.8.cross_attn.kv_linear.bias', 'blocks.8.cross_attn.proj.weight', 'blocks.8.cross_attn.proj.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.11.scale_shift_table', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.cross_attn.q_linear.weight', 'blocks.11.cross_attn.q_linear.bias', 'blocks.11.cross_attn.kv_linear.weight', 'blocks.11.cross_attn.kv_linear.bias', 'blocks.11.cross_attn.proj.weight', 'blocks.11.cross_attn.proj.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.15.scale_shift_table', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.cross_attn.q_linear.weight', 'blocks.15.cross_attn.q_linear.bias', 'blocks.15.cross_attn.kv_linear.weight', 'blocks.15.cross_attn.kv_linear.bias', 'blocks.15.cross_attn.proj.weight', 'blocks.15.cross_attn.proj.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.scale_shift_table', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.cross_attn.q_linear.weight', 'blocks.16.cross_attn.q_linear.bias', 'blocks.16.cross_attn.kv_linear.weight', 'blocks.16.cross_attn.kv_linear.bias', 'blocks.16.cross_attn.proj.weight', 'blocks.16.cross_attn.proj.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.scale_shift_table', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.cross_attn.q_linear.weight', 'blocks.17.cross_attn.q_linear.bias', 'blocks.17.cross_attn.kv_linear.weight', 'blocks.17.cross_attn.kv_linear.bias', 'blocks.17.cross_attn.proj.weight', 'blocks.17.cross_attn.proj.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.20.scale_shift_table', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.cross_attn.q_linear.weight', 'blocks.20.cross_attn.q_linear.bias', 'blocks.20.cross_attn.kv_linear.weight', 'blocks.20.cross_attn.kv_linear.bias', 'blocks.20.cross_attn.proj.weight', 'blocks.20.cross_attn.proj.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias']
2025-08-19 11:05:00,659 - PixArt - WARNING - Unexpected keys: []
2025-08-19 11:05:00,661 - PixArt - INFO - PixArtMS Model Parameters: 483,322,784
2025-08-19 11:05:00,663 - PixArt - INFO - PixArtMS Trainable Model Parameters: 483,322,784
2025-08-19 11:05:00,663 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-19 11:05:00,663 - PixArt - INFO - T5 max token length: 300
2025-08-19 11:05:00,664 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-19 11:05:00,972 - PixArt - INFO - data_info.json data volume: 100000
2025-08-19 11:05:02,305 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 1.64 s, length (use/ori): 100000/100000
2025-08-19 11:05:02,305 - PixArt - INFO - Constructing dataset InternalDataMSSigma...
2025-08-19 11:05:02,305 - PixArt - INFO - T5 max token length: 300
2025-08-19 11:05:02,305 - PixArt - INFO - ratio of real user prompt: 1.0
2025-08-19 11:05:02,308 - PixArt - INFO - data_info_stable_loss.json data volume: 499
2025-08-19 11:05:02,314 - PixArt - INFO - Dataset InternalDataMSSigma constructed. time: 0.01 s, length (use/ori): 499/499
2025-08-19 11:05:02,316 - PixArt - INFO - Automatically adapt lr to 0.00001 (using sqrt scaling rule).
2025-08-19 11:05:02,362 - PixArt - INFO - CAMEWrapper Optimizer: total 345 param groups, 345 are learnable, 0 are fix. Lr group: 345 params with lr 0.00001; Weight decay group: 345 params with weight decay 0.03.
2025-08-19 11:05:02,362 - PixArt - INFO - Lr schedule: constant, num_warmup_steps:1000.
  0%|          | 0/6250 [00:00<?, ?it/s]  0%|          | 0/6250 [00:00<?, ?it/s]

  0%|          | 0/32 [00:00<?, ?it/s][A  0%|          | 0/32 [00:00<?, ?it/s][A
  3%|â–Ž         | 1/32 [00:01<00:45,  1.48s/it][A
  3%|â–Ž         | 1/32 [00:01<00:48,  1.58s/it][A
  6%|â–‹         | 2/32 [00:01<00:27,  1.10it/s][A
  6%|â–‹         | 2/32 [00:02<00:28,  1.05it/s][A
  9%|â–‰         | 3/32 [00:02<00:21,  1.37it/s][A
  9%|â–‰         | 3/32 [00:02<00:21,  1.33it/s][A
 12%|â–ˆâ–Ž        | 4/32 [00:03<00:17,  1.56it/s][A
 12%|â–ˆâ–Ž        | 4/32 [00:03<00:18,  1.53it/s][A
 16%|â–ˆâ–Œ        | 5/32 [00:03<00:16,  1.68it/s][A
 16%|â–ˆâ–Œ        | 5/32 [00:03<00:16,  1.66it/s][A
 19%|â–ˆâ–‰        | 6/32 [00:04<00:14,  1.77it/s][A
 19%|â–ˆâ–‰        | 6/32 [00:04<00:14,  1.75it/s][A
 22%|â–ˆâ–ˆâ–       | 7/32 [00:04<00:13,  1.82it/s][A
 22%|â–ˆâ–ˆâ–       | 7/32 [00:04<00:13,  1.81it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:05<00:12,  1.86it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:05<00:12,  1.86it/s][A
 28%|â–ˆâ–ˆâ–Š       | 9/32 [00:05<00:12,  1.89it/s][A
 28%|â–ˆâ–ˆâ–Š       | 9/32 [00:05<00:12,  1.89it/s][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:06<00:11,  1.91it/s][A
 31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:06<00:11,  1.91it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:06<00:10,  1.92it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:06<00:10,  1.92it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:07<00:10,  1.93it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:07<00:10,  1.93it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:07<00:09,  1.94it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:07<00:09,  1.94it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:08<00:09,  1.94it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:08<00:09,  1.95it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:08<00:08,  1.95it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:08<00:08,  1.95it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:09<00:08,  1.95it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:09<00:08,  1.95it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:09<00:07,  1.95it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:09<00:07,  1.95it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:10<00:07,  1.95it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:10<00:07,  1.95it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:10<00:06,  1.95it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:10<00:06,  1.96it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:11<00:06,  1.95it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:11<00:06,  1.95it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:11<00:05,  1.95it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:11<00:05,  1.96it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:12<00:05,  1.95it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:12<00:05,  1.96it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:12<00:04,  1.95it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:12<00:04,  1.96it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:13<00:04,  1.95it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:13<00:04,  1.96it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:13<00:03,  1.94it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:13<00:03,  1.95it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:14<00:03,  1.94it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:14<00:03,  1.96it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:14<00:02,  1.94it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:14<00:02,  1.96it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:15<00:02,  1.95it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:15<00:02,  1.96it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:15<00:01,  1.95it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:15<00:01,  1.95it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:16<00:01,  1.95it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:16<00:01,  1.96it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:16<00:00,  1.95it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:16<00:00,  1.96it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.65it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.81it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.69it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.81it/s]
2025-08-19 11:05:25,263 - PixArt - INFO - Step/Epoch [1/1][1/6250]:total_eta: 3 days, 0:08:20, epoch_eta:18:01:59, time_all:1.039, time_data:0.050, lr:0.000e+00, s:(32, 32), loss:7.7699, grad_norm:nan, stable_loss:7.1545
  0%|          | 1/6250 [00:20<36:05:23, 20.79s/it]2025-08-19 11:05:25,675 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.18it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.17it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.19it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.17it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.16it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.16it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.13it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.10it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.15it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.13it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.16it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.08it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.07it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.20it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.16it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.17it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.17it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.01it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.06it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.05it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.18it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.97it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.06it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.10it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.11it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.11it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.05it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.13it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.16it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.13it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.15it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.15it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.10it/s]
  0%|          | 1/6250 [00:29<51:20:12, 29.57s/it]  0%|          | 2/6250 [00:31<25:17:29, 14.57s/it]  0%|          | 2/6250 [00:31<22:36:08, 13.02s/it]  0%|          | 3/6250 [00:32<14:50:37,  8.55s/it]  0%|          | 3/6250 [00:32<13:22:55,  7.71s/it]  0%|          | 4/6250 [00:33<9:56:16,  5.73s/it]   0%|          | 4/6250 [00:33<9:03:14,  5.22s/it]   0%|          | 5/6250 [00:35<7:13:37,  4.17s/it]  0%|          | 5/6250 [00:35<6:39:41,  3.84s/it]  0%|          | 6/6250 [00:36<5:35:20,  3.22s/it]  0%|          | 6/6250 [00:36<5:12:57,  3.01s/it]  0%|          | 7/6250 [00:37<4:32:56,  2.62s/it]  0%|          | 7/6250 [00:37<4:17:54,  2.48s/it]  0%|          | 8/6250 [00:39<3:52:15,  2.23s/it]  0%|          | 8/6250 [00:39<3:41:59,  2.13s/it]  0%|          | 9/6250 [00:40<3:24:57,  1.97s/it]  0%|          | 9/6250 [00:40<3:17:58,  1.90s/it]  0%|          | 10/6250 [00:42<3:06:44,  1.80s/it]  0%|          | 10/6250 [00:42<3:01:48,  1.75s/it]  0%|          | 11/6250 [00:43<3:02:12,  1.75s/it]  0%|          | 11/6250 [00:43<2:58:49,  1.72s/it]  0%|          | 12/6250 [00:45<2:57:34,  1.71s/it]  0%|          | 12/6250 [00:45<2:55:14,  1.69s/it]  0%|          | 13/6250 [00:47<2:53:42,  1.67s/it]  0%|          | 13/6250 [00:47<2:52:05,  1.66s/it]  0%|          | 14/6250 [00:48<2:51:13,  1.65s/it]  0%|          | 14/6250 [00:48<2:50:06,  1.64s/it]  0%|          | 15/6250 [00:50<2:49:23,  1.63s/it]  0%|          | 15/6250 [00:50<2:48:35,  1.62s/it]  0%|          | 16/6250 [00:51<2:48:05,  1.62s/it]  0%|          | 16/6250 [00:51<2:47:36,  1.61s/it]  0%|          | 17/6250 [00:53<2:47:06,  1.61s/it]  0%|          | 17/6250 [00:53<2:46:41,  1.60s/it]  0%|          | 18/6250 [00:54<2:46:33,  1.60s/it]  0%|          | 18/6250 [00:54<2:46:15,  1.60s/it]  0%|          | 19/6250 [00:56<2:46:02,  1.60s/it]  0%|          | 19/6250 [00:56<2:46:00,  1.60s/it]2025-08-19 11:06:02,637 - PixArt - INFO - Step/Epoch [20/1][20/6250]:total_eta: 19:12:50, epoch_eta:4:47:31, time_all:1.869, time_data:0.442, lr:1.000e-07, s:(32, 32), loss:7.0535, grad_norm:nan
  0%|          | 20/6250 [00:58<2:46:05,  1.60s/it]  0%|          | 20/6250 [00:58<2:45:56,  1.60s/it]  0%|          | 21/6250 [00:59<2:45:55,  1.60s/it]  0%|          | 21/6250 [00:59<2:45:45,  1.60s/it]  0%|          | 22/6250 [01:01<2:39:36,  1.54s/it]  0%|          | 22/6250 [01:01<2:39:33,  1.54s/it]  0%|          | 23/6250 [01:02<2:41:18,  1.55s/it]  0%|          | 23/6250 [01:02<2:41:15,  1.55s/it]  0%|          | 24/6250 [01:04<2:42:31,  1.57s/it]  0%|          | 24/6250 [01:04<2:42:27,  1.57s/it]  0%|          | 25/6250 [01:05<2:43:09,  1.57s/it]  0%|          | 25/6250 [01:05<2:43:06,  1.57s/it]  0%|          | 26/6250 [01:07<2:43:31,  1.58s/it]  0%|          | 26/6250 [01:07<2:43:27,  1.58s/it]  0%|          | 27/6250 [01:09<2:44:00,  1.58s/it]  0%|          | 27/6250 [01:09<2:43:59,  1.58s/it]  0%|          | 28/6250 [01:10<2:44:20,  1.58s/it]  0%|          | 28/6250 [01:10<2:44:22,  1.59s/it]  0%|          | 29/6250 [01:12<2:44:50,  1.59s/it]  0%|          | 29/6250 [01:12<2:44:47,  1.59s/it]  0%|          | 30/6250 [01:13<2:44:53,  1.59s/it]  0%|          | 30/6250 [01:13<2:44:52,  1.59s/it]  0%|          | 31/6250 [01:15<2:44:59,  1.59s/it]  0%|          | 31/6250 [01:15<2:44:59,  1.59s/it]  1%|          | 32/6250 [01:17<2:44:46,  1.59s/it]  1%|          | 32/6250 [01:17<2:44:45,  1.59s/it]  1%|          | 33/6250 [01:18<2:53:56,  1.68s/it]  1%|          | 33/6250 [01:18<2:53:57,  1.68s/it]  1%|          | 34/6250 [01:20<2:51:17,  1.65s/it]  1%|          | 34/6250 [01:20<2:51:16,  1.65s/it]  1%|          | 35/6250 [01:22<2:49:26,  1.64s/it]  1%|          | 35/6250 [01:22<2:49:25,  1.64s/it]  1%|          | 36/6250 [01:23<2:48:01,  1.62s/it]  1%|          | 36/6250 [01:23<2:48:02,  1.62s/it]  1%|          | 37/6250 [01:25<2:47:12,  1.61s/it]  1%|          | 37/6250 [01:25<2:47:13,  1.61s/it]  1%|          | 38/6250 [01:26<2:46:42,  1.61s/it]  1%|          | 38/6250 [01:26<2:46:53,  1.61s/it]  1%|          | 39/6250 [01:28<2:46:12,  1.61s/it]  1%|          | 39/6250 [01:28<2:46:14,  1.61s/it]2025-08-19 11:06:34,605 - PixArt - INFO - Step/Epoch [40/1][40/6250]:total_eta: 15:14:20, epoch_eta:3:47:29, time_all:1.598, time_data:0.002, lr:2.900e-07, s:(32, 32), loss:6.6607, grad_norm:nan
  1%|          | 40/6250 [01:30<2:45:59,  1.60s/it]  1%|          | 40/6250 [01:30<2:45:54,  1.60s/it]  1%|          | 41/6250 [01:31<2:45:42,  1.60s/it]  1%|          | 41/6250 [01:31<2:45:42,  1.60s/it]  1%|          | 42/6250 [01:33<2:45:30,  1.60s/it]  1%|          | 42/6250 [01:33<2:45:26,  1.60s/it]  1%|          | 43/6250 [01:34<2:45:31,  1.60s/it]  1%|          | 43/6250 [01:34<2:45:29,  1.60s/it]  1%|          | 44/6250 [01:36<2:45:32,  1.60s/it]  1%|          | 44/6250 [01:36<2:45:31,  1.60s/it]  1%|          | 45/6250 [01:38<2:45:23,  1.60s/it]  1%|          | 45/6250 [01:38<2:45:28,  1.60s/it]  1%|          | 46/6250 [01:39<2:45:20,  1.60s/it]  1%|          | 46/6250 [01:39<2:45:17,  1.60s/it]  1%|          | 47/6250 [01:41<2:45:05,  1.60s/it]  1%|          | 47/6250 [01:41<2:45:04,  1.60s/it]  1%|          | 48/6250 [01:42<2:45:09,  1.60s/it]  1%|          | 48/6250 [01:42<2:45:09,  1.60s/it]  1%|          | 49/6250 [01:44<2:45:31,  1.60s/it]  1%|          | 49/6250 [01:44<2:45:30,  1.60s/it]  1%|          | 50/6250 [01:46<2:45:20,  1.60s/it]  1%|          | 50/6250 [01:46<2:45:19,  1.60s/it]  1%|          | 51/6250 [01:47<2:45:10,  1.60s/it]  1%|          | 51/6250 [01:47<2:45:08,  1.60s/it]  1%|          | 52/6250 [01:49<2:45:00,  1.60s/it]  1%|          | 52/6250 [01:49<2:44:58,  1.60s/it]  1%|          | 53/6250 [01:50<2:44:45,  1.60s/it]  1%|          | 53/6250 [01:50<2:44:45,  1.60s/it]  1%|          | 54/6250 [01:52<2:44:34,  1.59s/it]  1%|          | 54/6250 [01:52<2:44:37,  1.59s/it]  1%|          | 55/6250 [01:54<2:44:32,  1.59s/it]  1%|          | 55/6250 [01:54<2:44:35,  1.59s/it]  1%|          | 56/6250 [01:55<2:44:26,  1.59s/it]  1%|          | 56/6250 [01:55<2:44:25,  1.59s/it]  1%|          | 57/6250 [01:57<2:44:27,  1.59s/it]  1%|          | 57/6250 [01:57<2:44:28,  1.59s/it]  1%|          | 58/6250 [01:58<2:44:17,  1.59s/it]  1%|          | 58/6250 [01:58<2:44:17,  1.59s/it]  1%|          | 59/6250 [02:00<2:44:01,  1.59s/it]  1%|          | 59/6250 [02:00<2:44:00,  1.59s/it]2025-08-19 11:07:06,500 - PixArt - INFO - Step/Epoch [60/1][60/6250]:total_eta: 13:51:23, epoch_eta:3:26:21, time_all:1.595, time_data:0.002, lr:4.900e-07, s:(32, 32), loss:6.9020, grad_norm:1561.7826
  1%|          | 60/6250 [02:02<2:43:50,  1.59s/it]  1%|          | 60/6250 [02:02<2:43:52,  1.59s/it]  1%|          | 61/6250 [02:03<2:43:54,  1.59s/it]  1%|          | 61/6250 [02:03<2:43:54,  1.59s/it]  1%|          | 62/6250 [02:05<2:44:07,  1.59s/it]  1%|          | 62/6250 [02:05<2:44:08,  1.59s/it]  1%|          | 63/6250 [02:06<2:44:08,  1.59s/it]  1%|          | 63/6250 [02:06<2:44:10,  1.59s/it]  1%|          | 64/6250 [02:08<2:44:13,  1.59s/it]  1%|          | 64/6250 [02:08<2:44:15,  1.59s/it]  1%|          | 65/6250 [02:09<2:44:21,  1.59s/it]  1%|          | 65/6250 [02:09<2:44:18,  1.59s/it]  1%|          | 66/6250 [02:11<2:44:25,  1.60s/it]  1%|          | 66/6250 [02:11<2:44:24,  1.60s/it]  1%|          | 67/6250 [02:13<2:44:18,  1.59s/it]  1%|          | 67/6250 [02:13<2:44:16,  1.59s/it]  1%|          | 68/6250 [02:14<2:44:15,  1.59s/it]  1%|          | 68/6250 [02:14<2:44:16,  1.59s/it]  1%|          | 69/6250 [02:16<2:44:26,  1.60s/it]  1%|          | 69/6250 [02:16<2:44:26,  1.60s/it]  1%|          | 70/6250 [02:17<2:38:30,  1.54s/it]  1%|          | 70/6250 [02:17<2:38:30,  1.54s/it]  1%|          | 71/6250 [02:19<2:40:13,  1.56s/it]  1%|          | 71/6250 [02:19<2:40:12,  1.56s/it]  1%|          | 72/6250 [02:20<2:41:22,  1.57s/it]  1%|          | 72/6250 [02:20<2:41:20,  1.57s/it]  1%|          | 73/6250 [02:22<2:42:08,  1.57s/it]  1%|          | 73/6250 [02:22<2:42:07,  1.57s/it]  1%|          | 74/6250 [02:24<2:42:38,  1.58s/it]  1%|          | 74/6250 [02:24<2:42:37,  1.58s/it]  1%|          | 75/6250 [02:25<2:43:13,  1.59s/it]  1%|          | 75/6250 [02:25<2:43:18,  1.59s/it]  1%|          | 76/6250 [02:27<2:43:40,  1.59s/it]  1%|          | 76/6250 [02:27<2:43:38,  1.59s/it]  1%|          | 77/6250 [02:28<2:43:49,  1.59s/it]  1%|          | 77/6250 [02:28<2:43:50,  1.59s/it]  1%|          | 78/6250 [02:30<2:43:53,  1.59s/it]  1%|          | 78/6250 [02:30<2:43:52,  1.59s/it]  1%|â–         | 79/6250 [02:32<2:43:53,  1.59s/it]  1%|â–         | 79/6250 [02:32<2:43:55,  1.59s/it]2025-08-19 11:07:38,221 - PixArt - INFO - Step/Epoch [80/1][80/6250]:total_eta: 13:08:15, epoch_eta:3:15:10, time_all:1.586, time_data:0.002, lr:6.800e-07, s:(32, 32), loss:6.3160, grad_norm:nan
  1%|â–         | 80/6250 [02:33<2:43:57,  1.59s/it]  1%|â–         | 80/6250 [02:33<2:43:57,  1.59s/it]  1%|â–         | 81/6250 [02:35<2:43:57,  1.59s/it]  1%|â–         | 81/6250 [02:35<2:43:56,  1.59s/it]  1%|â–         | 82/6250 [02:36<2:44:09,  1.60s/it]  1%|â–         | 82/6250 [02:36<2:44:08,  1.60s/it]  1%|â–         | 83/6250 [02:38<2:43:58,  1.60s/it]  1%|â–         | 83/6250 [02:38<2:43:57,  1.60s/it]  1%|â–         | 84/6250 [02:40<2:43:57,  1.60s/it]  1%|â–         | 84/6250 [02:40<2:43:57,  1.60s/it]  1%|â–         | 85/6250 [02:41<2:44:03,  1.60s/it]  1%|â–         | 85/6250 [02:41<2:44:00,  1.60s/it]  1%|â–         | 86/6250 [02:43<2:44:01,  1.60s/it]  1%|â–         | 86/6250 [02:43<2:44:01,  1.60s/it]  1%|â–         | 87/6250 [02:44<2:43:54,  1.60s/it]  1%|â–         | 87/6250 [02:44<2:43:55,  1.60s/it]  1%|â–         | 88/6250 [02:46<2:43:40,  1.59s/it]  1%|â–         | 88/6250 [02:46<2:43:40,  1.59s/it]  1%|â–         | 89/6250 [02:48<2:43:54,  1.60s/it]  1%|â–         | 89/6250 [02:48<2:43:54,  1.60s/it]  1%|â–         | 90/6250 [02:49<2:43:58,  1.60s/it]  1%|â–         | 90/6250 [02:49<2:43:58,  1.60s/it]  1%|â–         | 91/6250 [02:51<2:43:53,  1.60s/it]  1%|â–         | 91/6250 [02:51<2:43:53,  1.60s/it]  1%|â–         | 92/6250 [02:52<2:43:52,  1.60s/it]  1%|â–         | 92/6250 [02:52<2:43:51,  1.60s/it]  1%|â–         | 93/6250 [02:54<2:43:48,  1.60s/it]  1%|â–         | 93/6250 [02:54<2:43:48,  1.60s/it]  2%|â–         | 94/6250 [02:56<2:43:42,  1.60s/it]  2%|â–         | 94/6250 [02:56<2:43:48,  1.60s/it]  2%|â–         | 95/6250 [02:57<2:43:46,  1.60s/it]  2%|â–         | 95/6250 [02:57<2:43:43,  1.60s/it]  2%|â–         | 96/6250 [02:59<2:43:41,  1.60s/it]  2%|â–         | 96/6250 [02:59<2:43:39,  1.60s/it]  2%|â–         | 97/6250 [03:00<2:43:43,  1.60s/it]  2%|â–         | 97/6250 [03:00<2:43:42,  1.60s/it]  2%|â–         | 98/6250 [03:02<2:43:41,  1.60s/it]  2%|â–         | 98/6250 [03:02<2:43:41,  1.60s/it]  2%|â–         | 99/6250 [03:04<2:43:36,  1.60s/it]  2%|â–         | 99/6250 [03:04<2:43:34,  1.60s/it]2025-08-19 11:08:10,148 - PixArt - INFO - Step/Epoch [100/1][100/6250]:total_eta: 12:42:50, epoch_eta:3:08:25, time_all:1.596, time_data:0.002, lr:8.800e-07, s:(32, 32), loss:5.7546, grad_norm:1618.8796
  2%|â–         | 100/6250 [03:05<2:43:39,  1.60s/it]  2%|â–         | 100/6250 [03:05<2:43:41,  1.60s/it]  2%|â–         | 101/6250 [03:07<2:43:43,  1.60s/it]  2%|â–         | 101/6250 [03:07<2:43:43,  1.60s/it]  2%|â–         | 102/6250 [03:08<2:43:44,  1.60s/it]  2%|â–         | 102/6250 [03:08<2:43:56,  1.60s/it]  2%|â–         | 103/6250 [03:10<2:44:01,  1.60s/it]  2%|â–         | 103/6250 [03:10<2:43:57,  1.60s/it]  2%|â–         | 104/6250 [03:12<2:43:42,  1.60s/it]  2%|â–         | 104/6250 [03:12<2:43:40,  1.60s/it]  2%|â–         | 105/6250 [03:13<2:43:44,  1.60s/it]  2%|â–         | 105/6250 [03:13<2:43:44,  1.60s/it]  2%|â–         | 106/6250 [03:15<2:43:43,  1.60s/it]  2%|â–         | 106/6250 [03:15<2:43:44,  1.60s/it]  2%|â–         | 107/6250 [03:16<2:43:41,  1.60s/it]  2%|â–         | 107/6250 [03:16<2:43:38,  1.60s/it]  2%|â–         | 108/6250 [03:18<2:43:34,  1.60s/it]  2%|â–         | 108/6250 [03:18<2:43:33,  1.60s/it]  2%|â–         | 109/6250 [03:20<2:43:26,  1.60s/it]  2%|â–         | 109/6250 [03:20<2:43:31,  1.60s/it]  2%|â–         | 110/6250 [03:21<2:43:31,  1.60s/it]  2%|â–         | 110/6250 [03:21<2:43:34,  1.60s/it]  2%|â–         | 111/6250 [03:23<2:43:32,  1.60s/it]  2%|â–         | 111/6250 [03:23<2:43:31,  1.60s/it]  2%|â–         | 112/6250 [03:24<2:43:28,  1.60s/it]  2%|â–         | 112/6250 [03:24<2:43:27,  1.60s/it]  2%|â–         | 113/6250 [03:26<2:43:28,  1.60s/it]  2%|â–         | 113/6250 [03:26<2:43:28,  1.60s/it]  2%|â–         | 114/6250 [03:28<2:43:19,  1.60s/it]  2%|â–         | 114/6250 [03:28<2:43:17,  1.60s/it]  2%|â–         | 115/6250 [03:29<2:43:01,  1.59s/it]  2%|â–         | 115/6250 [03:29<2:43:01,  1.59s/it]  2%|â–         | 116/6250 [03:31<2:43:21,  1.60s/it]  2%|â–         | 116/6250 [03:31<2:43:20,  1.60s/it]  2%|â–         | 117/6250 [03:32<2:44:08,  1.61s/it]  2%|â–         | 117/6250 [03:32<2:44:07,  1.61s/it]  2%|â–         | 118/6250 [03:34<2:43:49,  1.60s/it]  2%|â–         | 118/6250 [03:34<2:43:44,  1.60s/it]  2%|â–         | 119/6250 [03:36<2:43:28,  1.60s/it]  2%|â–         | 119/6250 [03:36<2:43:31,  1.60s/it]2025-08-19 11:08:42,148 - PixArt - INFO - Step/Epoch [120/1][120/6250]:total_eta: 12:25:54, epoch_eta:3:03:47, time_all:1.600, time_data:0.002, lr:1.080e-06, s:(32, 32), loss:5.5196, grad_norm:1064.9565
  2%|â–         | 120/6250 [03:37<2:43:58,  1.60s/it]  2%|â–         | 120/6250 [03:37<2:43:58,  1.60s/it]  2%|â–         | 121/6250 [03:39<2:43:45,  1.60s/it]  2%|â–         | 121/6250 [03:39<2:43:45,  1.60s/it]  2%|â–         | 122/6250 [03:40<2:43:47,  1.60s/it]  2%|â–         | 122/6250 [03:40<2:43:47,  1.60s/it]  2%|â–         | 123/6250 [03:42<2:43:39,  1.60s/it]  2%|â–         | 123/6250 [03:42<2:43:38,  1.60s/it]  2%|â–         | 124/6250 [03:44<2:43:17,  1.60s/it]  2%|â–         | 124/6250 [03:44<2:43:19,  1.60s/it]  2%|â–         | 125/6250 [03:45<2:43:20,  1.60s/it]  2%|â–         | 125/6250 [03:45<2:43:17,  1.60s/it]  2%|â–         | 126/6250 [03:47<2:43:32,  1.60s/it]  2%|â–         | 126/6250 [03:47<2:43:30,  1.60s/it]  2%|â–         | 127/6250 [03:48<2:43:05,  1.60s/it]  2%|â–         | 127/6250 [03:48<2:43:18,  1.60s/it]  2%|â–         | 128/6250 [03:50<2:43:19,  1.60s/it]  2%|â–         | 128/6250 [03:50<2:43:15,  1.60s/it]  2%|â–         | 129/6250 [03:52<2:43:09,  1.60s/it]  2%|â–         | 129/6250 [03:52<2:43:04,  1.60s/it]  2%|â–         | 130/6250 [03:53<2:43:15,  1.60s/it]  2%|â–         | 130/6250 [03:53<2:43:13,  1.60s/it]  2%|â–         | 131/6250 [03:55<2:43:06,  1.60s/it]  2%|â–         | 131/6250 [03:55<2:43:05,  1.60s/it]  2%|â–         | 132/6250 [03:56<2:42:49,  1.60s/it]  2%|â–         | 132/6250 [03:56<2:42:46,  1.60s/it]  2%|â–         | 133/6250 [03:58<2:42:50,  1.60s/it]  2%|â–         | 133/6250 [03:58<2:42:48,  1.60s/it]  2%|â–         | 134/6250 [04:00<2:42:57,  1.60s/it]  2%|â–         | 134/6250 [04:00<2:42:57,  1.60s/it]  2%|â–         | 135/6250 [04:01<2:42:38,  1.60s/it]  2%|â–         | 135/6250 [04:01<2:42:38,  1.60s/it]  2%|â–         | 136/6250 [04:03<2:42:42,  1.60s/it]  2%|â–         | 136/6250 [04:03<2:42:42,  1.60s/it]  2%|â–         | 137/6250 [04:04<2:42:58,  1.60s/it]  2%|â–         | 137/6250 [04:04<2:42:58,  1.60s/it]  2%|â–         | 138/6250 [04:06<2:43:08,  1.60s/it]  2%|â–         | 138/6250 [04:06<2:43:10,  1.60s/it]  2%|â–         | 139/6250 [04:08<2:42:49,  1.60s/it]  2%|â–         | 139/6250 [04:08<2:42:48,  1.60s/it]2025-08-19 11:09:14,132 - PixArt - INFO - Step/Epoch [140/1][140/6250]:total_eta: 12:13:33, epoch_eta:3:00:18, time_all:1.599, time_data:0.002, lr:1.280e-06, s:(32, 32), loss:4.5805, grad_norm:1101.5717
  2%|â–         | 140/6250 [04:09<2:42:57,  1.60s/it]  2%|â–         | 140/6250 [04:09<2:43:02,  1.60s/it]  2%|â–         | 141/6250 [04:11<2:42:39,  1.60s/it]  2%|â–         | 141/6250 [04:11<2:42:42,  1.60s/it]  2%|â–         | 142/6250 [04:12<2:42:41,  1.60s/it]  2%|â–         | 142/6250 [04:12<2:42:40,  1.60s/it]  2%|â–         | 143/6250 [04:14<2:42:43,  1.60s/it]  2%|â–         | 143/6250 [04:14<2:42:43,  1.60s/it]  2%|â–         | 144/6250 [04:16<2:42:42,  1.60s/it]  2%|â–         | 144/6250 [04:16<2:42:42,  1.60s/it]  2%|â–         | 145/6250 [04:17<2:42:30,  1.60s/it]  2%|â–         | 145/6250 [04:17<2:42:28,  1.60s/it]  2%|â–         | 146/6250 [04:19<2:42:26,  1.60s/it]  2%|â–         | 146/6250 [04:19<2:42:25,  1.60s/it]  2%|â–         | 147/6250 [04:20<2:42:14,  1.60s/it]  2%|â–         | 147/6250 [04:20<2:42:12,  1.59s/it]  2%|â–         | 148/6250 [04:22<2:42:20,  1.60s/it]  2%|â–         | 148/6250 [04:22<2:42:21,  1.60s/it]  2%|â–         | 149/6250 [04:24<2:42:30,  1.60s/it]  2%|â–         | 149/6250 [04:24<2:42:31,  1.60s/it]  2%|â–         | 150/6250 [04:25<2:42:30,  1.60s/it]  2%|â–         | 150/6250 [04:25<2:42:31,  1.60s/it]  2%|â–         | 151/6250 [04:27<2:42:26,  1.60s/it]  2%|â–         | 151/6250 [04:27<2:42:28,  1.60s/it]  2%|â–         | 152/6250 [04:28<2:42:24,  1.60s/it]  2%|â–         | 152/6250 [04:28<2:42:23,  1.60s/it]  2%|â–         | 153/6250 [04:30<2:42:15,  1.60s/it]  2%|â–         | 153/6250 [04:30<2:42:14,  1.60s/it]  2%|â–         | 154/6250 [04:32<2:42:15,  1.60s/it]  2%|â–         | 154/6250 [04:32<2:42:15,  1.60s/it]  2%|â–         | 155/6250 [04:33<2:42:21,  1.60s/it]  2%|â–         | 155/6250 [04:33<2:42:21,  1.60s/it]  2%|â–         | 156/6250 [04:35<2:42:23,  1.60s/it]  2%|â–         | 156/6250 [04:35<2:42:22,  1.60s/it]  3%|â–Ž         | 157/6250 [04:36<2:42:22,  1.60s/it]  3%|â–Ž         | 157/6250 [04:36<2:42:22,  1.60s/it]  3%|â–Ž         | 158/6250 [04:38<2:42:17,  1.60s/it]  3%|â–Ž         | 158/6250 [04:38<2:42:21,  1.60s/it]  3%|â–Ž         | 159/6250 [04:40<2:42:10,  1.60s/it]  3%|â–Ž         | 159/6250 [04:40<2:42:21,  1.60s/it]2025-08-19 11:09:46,091 - PixArt - INFO - Step/Epoch [160/1][160/6250]:total_eta: 12:04:05, epoch_eta:2:57:32, time_all:1.598, time_data:0.002, lr:1.480e-06, s:(32, 32), loss:4.6663, grad_norm:636.3338
  3%|â–Ž         | 160/6250 [04:41<2:42:27,  1.60s/it]  3%|â–Ž         | 160/6250 [04:41<2:42:24,  1.60s/it]  3%|â–Ž         | 161/6250 [04:43<2:42:22,  1.60s/it]  3%|â–Ž         | 161/6250 [04:43<2:42:19,  1.60s/it]  3%|â–Ž         | 162/6250 [04:44<2:42:05,  1.60s/it]  3%|â–Ž         | 162/6250 [04:44<2:42:01,  1.60s/it]  3%|â–Ž         | 163/6250 [04:46<2:41:56,  1.60s/it]  3%|â–Ž         | 163/6250 [04:46<2:41:55,  1.60s/it]  3%|â–Ž         | 164/6250 [04:47<2:41:52,  1.60s/it]  3%|â–Ž         | 164/6250 [04:47<2:41:51,  1.60s/it]  3%|â–Ž         | 165/6250 [04:49<2:41:48,  1.60s/it]  3%|â–Ž         | 165/6250 [04:49<2:41:48,  1.60s/it]  3%|â–Ž         | 166/6250 [04:51<2:41:49,  1.60s/it]  3%|â–Ž         | 166/6250 [04:51<2:41:47,  1.60s/it]  3%|â–Ž         | 167/6250 [04:52<2:41:43,  1.60s/it]  3%|â–Ž         | 167/6250 [04:52<2:41:44,  1.60s/it]  3%|â–Ž         | 168/6250 [04:54<2:41:41,  1.60s/it]  3%|â–Ž         | 168/6250 [04:54<2:41:42,  1.60s/it]  3%|â–Ž         | 169/6250 [04:55<2:41:40,  1.60s/it]  3%|â–Ž         | 169/6250 [04:55<2:41:40,  1.60s/it]  3%|â–Ž         | 170/6250 [04:57<2:41:41,  1.60s/it]  3%|â–Ž         | 170/6250 [04:57<2:41:40,  1.60s/it]  3%|â–Ž         | 171/6250 [04:59<2:41:46,  1.60s/it]  3%|â–Ž         | 171/6250 [04:59<2:41:46,  1.60s/it]  3%|â–Ž         | 172/6250 [05:00<2:41:36,  1.60s/it]  3%|â–Ž         | 172/6250 [05:00<2:41:34,  1.60s/it]  3%|â–Ž         | 173/6250 [05:02<2:41:36,  1.60s/it]  3%|â–Ž         | 173/6250 [05:02<2:41:41,  1.60s/it]  3%|â–Ž         | 174/6250 [05:03<2:41:52,  1.60s/it]  3%|â–Ž         | 174/6250 [05:03<2:41:52,  1.60s/it]  3%|â–Ž         | 175/6250 [05:05<2:41:55,  1.60s/it]  3%|â–Ž         | 175/6250 [05:05<2:41:59,  1.60s/it]  3%|â–Ž         | 176/6250 [05:07<2:42:01,  1.60s/it]  3%|â–Ž         | 176/6250 [05:07<2:41:59,  1.60s/it]  3%|â–Ž         | 177/6250 [05:08<2:41:57,  1.60s/it]  3%|â–Ž         | 177/6250 [05:08<2:41:54,  1.60s/it]  3%|â–Ž         | 178/6250 [05:10<2:41:39,  1.60s/it]  3%|â–Ž         | 178/6250 [05:10<2:41:39,  1.60s/it]  3%|â–Ž         | 179/6250 [05:11<2:41:24,  1.60s/it]  3%|â–Ž         | 179/6250 [05:11<2:41:23,  1.60s/it]2025-08-19 11:10:18,010 - PixArt - INFO - Step/Epoch [180/1][180/6250]:total_eta: 11:56:30, epoch_eta:2:55:14, time_all:1.596, time_data:0.002, lr:1.680e-06, s:(32, 32), loss:3.7063, grad_norm:502.8572
  3%|â–Ž         | 180/6250 [05:13<2:41:15,  1.59s/it]  3%|â–Ž         | 180/6250 [05:13<2:41:12,  1.59s/it]  3%|â–Ž         | 181/6250 [05:15<2:41:10,  1.59s/it]  3%|â–Ž         | 181/6250 [05:15<2:41:10,  1.59s/it]  3%|â–Ž         | 182/6250 [05:16<2:41:19,  1.60s/it]  3%|â–Ž         | 182/6250 [05:16<2:41:22,  1.60s/it]  3%|â–Ž         | 183/6250 [05:18<2:41:25,  1.60s/it]  3%|â–Ž         | 183/6250 [05:18<2:41:23,  1.60s/it]  3%|â–Ž         | 184/6250 [05:19<2:41:15,  1.60s/it]  3%|â–Ž         | 184/6250 [05:19<2:41:14,  1.59s/it]  3%|â–Ž         | 185/6250 [05:21<2:41:12,  1.59s/it]  3%|â–Ž         | 185/6250 [05:21<2:41:10,  1.59s/it]  3%|â–Ž         | 186/6250 [05:23<2:41:15,  1.60s/it]  3%|â–Ž         | 186/6250 [05:23<2:41:16,  1.60s/it]  3%|â–Ž         | 187/6250 [05:24<2:41:31,  1.60s/it]  3%|â–Ž         | 187/6250 [05:24<2:41:32,  1.60s/it]  3%|â–Ž         | 188/6250 [05:26<2:41:18,  1.60s/it]  3%|â–Ž         | 188/6250 [05:26<2:41:19,  1.60s/it]  3%|â–Ž         | 189/6250 [05:27<2:41:15,  1.60s/it]  3%|â–Ž         | 189/6250 [05:27<2:41:14,  1.60s/it]  3%|â–Ž         | 190/6250 [05:29<2:41:17,  1.60s/it]  3%|â–Ž         | 190/6250 [05:29<2:41:19,  1.60s/it]  3%|â–Ž         | 191/6250 [05:31<2:41:06,  1.60s/it]  3%|â–Ž         | 191/6250 [05:31<2:41:05,  1.60s/it]  3%|â–Ž         | 192/6250 [05:32<2:41:08,  1.60s/it]  3%|â–Ž         | 192/6250 [05:32<2:41:07,  1.60s/it]  3%|â–Ž         | 193/6250 [05:34<2:40:59,  1.59s/it]  3%|â–Ž         | 193/6250 [05:34<2:40:59,  1.59s/it]  3%|â–Ž         | 194/6250 [05:35<2:41:06,  1.60s/it]  3%|â–Ž         | 194/6250 [05:35<2:41:07,  1.60s/it]  3%|â–Ž         | 195/6250 [05:37<2:41:03,  1.60s/it]  3%|â–Ž         | 195/6250 [05:37<2:41:03,  1.60s/it]  3%|â–Ž         | 196/6250 [05:39<2:41:16,  1.60s/it]  3%|â–Ž         | 196/6250 [05:39<2:41:15,  1.60s/it]  3%|â–Ž         | 197/6250 [05:40<2:41:29,  1.60s/it]  3%|â–Ž         | 197/6250 [05:40<2:41:29,  1.60s/it]  3%|â–Ž         | 198/6250 [05:42<2:41:24,  1.60s/it]  3%|â–Ž         | 198/6250 [05:42<2:41:24,  1.60s/it]  3%|â–Ž         | 199/6250 [05:43<2:41:11,  1.60s/it]  3%|â–Ž         | 199/6250 [05:43<2:41:11,  1.60s/it]2025-08-19 11:10:49,949 - PixArt - INFO - Step/Epoch [200/1][200/6250]:total_eta: 11:50:22, epoch_eta:2:53:18, time_all:1.597, time_data:0.002, lr:1.880e-06, s:(32, 32), loss:3.5380, grad_norm:409.0084
  3%|â–Ž         | 200/6250 [05:45<2:41:03,  1.60s/it]  3%|â–Ž         | 200/6250 [05:45<2:41:03,  1.60s/it]  3%|â–Ž         | 201/6250 [05:47<2:41:03,  1.60s/it]  3%|â–Ž         | 201/6250 [05:47<2:41:09,  1.60s/it]  3%|â–Ž         | 202/6250 [05:48<2:41:08,  1.60s/it]  3%|â–Ž         | 202/6250 [05:48<2:41:17,  1.60s/it]  3%|â–Ž         | 203/6250 [05:50<2:41:09,  1.60s/it]  3%|â–Ž         | 203/6250 [05:50<2:41:04,  1.60s/it]  3%|â–Ž         | 204/6250 [05:51<2:41:06,  1.60s/it]  3%|â–Ž         | 204/6250 [05:51<2:41:01,  1.60s/it]  3%|â–Ž         | 205/6250 [05:53<2:41:16,  1.60s/it]  3%|â–Ž         | 205/6250 [05:53<2:41:17,  1.60s/it]  3%|â–Ž         | 206/6250 [05:55<2:41:26,  1.60s/it]  3%|â–Ž         | 206/6250 [05:55<2:41:24,  1.60s/it]  3%|â–Ž         | 207/6250 [05:56<2:41:10,  1.60s/it]  3%|â–Ž         | 207/6250 [05:56<2:41:07,  1.60s/it]  3%|â–Ž         | 208/6250 [05:58<2:40:52,  1.60s/it]  3%|â–Ž         | 208/6250 [05:58<2:40:51,  1.60s/it]  3%|â–Ž         | 209/6250 [05:59<2:40:44,  1.60s/it]  3%|â–Ž         | 209/6250 [05:59<2:40:45,  1.60s/it]  3%|â–Ž         | 210/6250 [06:01<2:40:54,  1.60s/it]  3%|â–Ž         | 210/6250 [06:01<2:40:54,  1.60s/it]  3%|â–Ž         | 211/6250 [06:03<2:41:02,  1.60s/it]  3%|â–Ž         | 211/6250 [06:03<2:41:02,  1.60s/it]  3%|â–Ž         | 212/6250 [06:04<2:40:59,  1.60s/it]  3%|â–Ž         | 212/6250 [06:04<2:41:00,  1.60s/it]  3%|â–Ž         | 213/6250 [06:06<2:40:54,  1.60s/it]  3%|â–Ž         | 213/6250 [06:06<2:40:53,  1.60s/it]  3%|â–Ž         | 214/6250 [06:07<2:40:35,  1.60s/it]  3%|â–Ž         | 214/6250 [06:07<2:40:34,  1.60s/it]  3%|â–Ž         | 215/6250 [06:09<2:40:33,  1.60s/it]  3%|â–Ž         | 215/6250 [06:09<2:40:36,  1.60s/it]  3%|â–Ž         | 216/6250 [06:11<2:40:42,  1.60s/it]  3%|â–Ž         | 216/6250 [06:11<2:40:43,  1.60s/it]  3%|â–Ž         | 217/6250 [06:12<2:40:37,  1.60s/it]  3%|â–Ž         | 217/6250 [06:12<2:40:36,  1.60s/it]  3%|â–Ž         | 218/6250 [06:14<2:40:32,  1.60s/it]  3%|â–Ž         | 218/6250 [06:14<2:40:37,  1.60s/it]  4%|â–Ž         | 219/6250 [06:15<2:40:38,  1.60s/it]  4%|â–Ž         | 219/6250 [06:15<2:40:35,  1.60s/it]2025-08-19 11:11:21,929 - PixArt - INFO - Step/Epoch [220/1][220/6250]:total_eta: 11:45:19, epoch_eta:2:51:38, time_all:1.599, time_data:0.002, lr:2.080e-06, s:(32, 32), loss:3.2073, grad_norm:134.2343
  4%|â–Ž         | 220/6250 [06:17<2:40:48,  1.60s/it]  4%|â–Ž         | 220/6250 [06:17<2:40:44,  1.60s/it]  4%|â–Ž         | 221/6250 [06:19<2:40:42,  1.60s/it]  4%|â–Ž         | 221/6250 [06:19<2:40:41,  1.60s/it]  4%|â–Ž         | 222/6250 [06:20<2:40:29,  1.60s/it]  4%|â–Ž         | 222/6250 [06:20<2:40:28,  1.60s/it]  4%|â–Ž         | 223/6250 [06:22<2:40:22,  1.60s/it]  4%|â–Ž         | 223/6250 [06:22<2:40:21,  1.60s/it]  4%|â–Ž         | 224/6250 [06:23<2:40:17,  1.60s/it]  4%|â–Ž         | 224/6250 [06:23<2:40:20,  1.60s/it]  4%|â–Ž         | 225/6250 [06:25<2:40:16,  1.60s/it]  4%|â–Ž         | 225/6250 [06:25<2:40:17,  1.60s/it]  4%|â–Ž         | 226/6250 [06:27<2:40:10,  1.60s/it]  4%|â–Ž         | 226/6250 [06:27<2:40:08,  1.60s/it]  4%|â–Ž         | 227/6250 [06:28<2:39:59,  1.59s/it]  4%|â–Ž         | 227/6250 [06:28<2:39:57,  1.59s/it]  4%|â–Ž         | 228/6250 [06:30<2:40:04,  1.59s/it]  4%|â–Ž         | 228/6250 [06:30<2:40:05,  1.60s/it]  4%|â–Ž         | 229/6250 [06:31<2:40:08,  1.60s/it]  4%|â–Ž         | 229/6250 [06:31<2:40:08,  1.60s/it]  4%|â–Ž         | 230/6250 [06:33<2:40:05,  1.60s/it]  4%|â–Ž         | 230/6250 [06:33<2:40:04,  1.60s/it]  4%|â–Ž         | 231/6250 [06:34<2:40:07,  1.60s/it]  4%|â–Ž         | 231/6250 [06:34<2:40:08,  1.60s/it]  4%|â–Ž         | 232/6250 [06:36<2:40:07,  1.60s/it]  4%|â–Ž         | 232/6250 [06:36<2:40:08,  1.60s/it]  4%|â–Ž         | 233/6250 [06:38<2:39:59,  1.60s/it]  4%|â–Ž         | 233/6250 [06:38<2:39:59,  1.60s/it]  4%|â–Ž         | 234/6250 [06:39<2:40:00,  1.60s/it]  4%|â–Ž         | 234/6250 [06:39<2:40:00,  1.60s/it]  4%|â–         | 235/6250 [06:41<2:39:52,  1.59s/it]  4%|â–         | 235/6250 [06:41<2:39:51,  1.59s/it]  4%|â–         | 236/6250 [06:42<2:39:52,  1.60s/it]  4%|â–         | 236/6250 [06:42<2:39:52,  1.60s/it]  4%|â–         | 237/6250 [06:44<2:39:55,  1.60s/it]  4%|â–         | 237/6250 [06:44<2:39:53,  1.60s/it]  4%|â–         | 238/6250 [06:46<2:39:58,  1.60s/it]  4%|â–         | 238/6250 [06:46<2:39:58,  1.60s/it]  4%|â–         | 239/6250 [06:47<2:39:56,  1.60s/it]  4%|â–         | 239/6250 [06:47<2:39:55,  1.60s/it]2025-08-19 11:11:53,833 - PixArt - INFO - Step/Epoch [240/1][240/6250]:total_eta: 11:40:54, epoch_eta:2:50:08, time_all:1.595, time_data:0.002, lr:2.280e-06, s:(32, 32), loss:2.8890, grad_norm:75.1346
  4%|â–         | 240/6250 [06:49<2:39:42,  1.59s/it]  4%|â–         | 240/6250 [06:49<2:39:41,  1.59s/it]  4%|â–         | 241/6250 [06:50<2:39:41,  1.59s/it]  4%|â–         | 241/6250 [06:50<2:39:48,  1.60s/it]  4%|â–         | 242/6250 [06:52<2:39:37,  1.59s/it]  4%|â–         | 242/6250 [06:52<2:39:35,  1.59s/it]  4%|â–         | 243/6250 [06:54<2:39:43,  1.60s/it]  4%|â–         | 243/6250 [06:54<2:39:41,  1.59s/it]  4%|â–         | 244/6250 [06:55<2:39:37,  1.59s/it]  4%|â–         | 244/6250 [06:55<2:39:38,  1.59s/it]  4%|â–         | 245/6250 [06:57<2:39:40,  1.60s/it]  4%|â–         | 245/6250 [06:57<2:39:40,  1.60s/it]  4%|â–         | 246/6250 [06:58<2:39:50,  1.60s/it]  4%|â–         | 246/6250 [06:58<2:39:48,  1.60s/it]  4%|â–         | 247/6250 [07:00<2:39:32,  1.59s/it]  4%|â–         | 247/6250 [07:00<2:39:30,  1.59s/it]  4%|â–         | 248/6250 [07:02<2:39:26,  1.59s/it]  4%|â–         | 248/6250 [07:02<2:39:26,  1.59s/it]  4%|â–         | 249/6250 [07:03<2:39:18,  1.59s/it]  4%|â–         | 249/6250 [07:03<2:39:19,  1.59s/it]  4%|â–         | 250/6250 [07:05<2:39:15,  1.59s/it]  4%|â–         | 250/6250 [07:05<2:39:13,  1.59s/it]  4%|â–         | 251/6250 [07:06<2:39:08,  1.59s/it]  4%|â–         | 251/6250 [07:06<2:39:07,  1.59s/it]  4%|â–         | 252/6250 [07:08<2:39:07,  1.59s/it]  4%|â–         | 252/6250 [07:08<2:39:07,  1.59s/it]  4%|â–         | 253/6250 [07:10<2:39:32,  1.60s/it]  4%|â–         | 253/6250 [07:10<2:39:34,  1.60s/it]  4%|â–         | 254/6250 [07:11<2:39:43,  1.60s/it]  4%|â–         | 254/6250 [07:11<2:39:44,  1.60s/it]  4%|â–         | 255/6250 [07:13<2:39:27,  1.60s/it]  4%|â–         | 255/6250 [07:13<2:39:28,  1.60s/it]  4%|â–         | 256/6250 [07:14<2:39:35,  1.60s/it]  4%|â–         | 256/6250 [07:14<2:39:34,  1.60s/it]  4%|â–         | 257/6250 [07:16<2:39:37,  1.60s/it]  4%|â–         | 257/6250 [07:16<2:39:38,  1.60s/it]  4%|â–         | 258/6250 [07:18<2:39:45,  1.60s/it]  4%|â–         | 258/6250 [07:18<2:39:43,  1.60s/it]  4%|â–         | 259/6250 [07:19<2:39:25,  1.60s/it]  4%|â–         | 259/6250 [07:19<2:39:24,  1.60s/it]2025-08-19 11:12:25,743 - PixArt - INFO - Step/Epoch [260/1][260/6250]:total_eta: 11:37:04, epoch_eta:2:48:46, time_all:1.596, time_data:0.002, lr:2.480e-06, s:(32, 32), loss:2.4934, grad_norm:81.4961
  4%|â–         | 260/6250 [07:21<2:39:18,  1.60s/it]  4%|â–         | 260/6250 [07:21<2:39:18,  1.60s/it]  4%|â–         | 261/6250 [07:22<2:39:17,  1.60s/it]  4%|â–         | 261/6250 [07:22<2:39:18,  1.60s/it]  4%|â–         | 262/6250 [07:24<2:39:19,  1.60s/it]  4%|â–         | 262/6250 [07:24<2:39:21,  1.60s/it]  4%|â–         | 263/6250 [07:26<2:39:04,  1.59s/it]  4%|â–         | 263/6250 [07:26<2:39:04,  1.59s/it]  4%|â–         | 264/6250 [07:27<2:39:06,  1.59s/it]  4%|â–         | 264/6250 [07:27<2:39:06,  1.59s/it]  4%|â–         | 265/6250 [07:29<2:39:11,  1.60s/it]  4%|â–         | 265/6250 [07:29<2:39:09,  1.60s/it]  4%|â–         | 266/6250 [07:30<2:38:57,  1.59s/it]  4%|â–         | 266/6250 [07:30<2:38:57,  1.59s/it]  4%|â–         | 267/6250 [07:32<2:38:54,  1.59s/it]  4%|â–         | 267/6250 [07:32<2:38:55,  1.59s/it]  4%|â–         | 268/6250 [07:34<2:39:00,  1.59s/it]  4%|â–         | 268/6250 [07:34<2:39:01,  1.59s/it]  4%|â–         | 269/6250 [07:35<2:38:51,  1.59s/it]  4%|â–         | 269/6250 [07:35<2:38:49,  1.59s/it]  4%|â–         | 270/6250 [07:37<2:38:54,  1.59s/it]  4%|â–         | 270/6250 [07:37<2:38:53,  1.59s/it]  4%|â–         | 271/6250 [07:38<2:38:56,  1.59s/it]  4%|â–         | 271/6250 [07:38<2:38:55,  1.59s/it]  4%|â–         | 272/6250 [07:40<2:38:55,  1.60s/it]  4%|â–         | 272/6250 [07:40<2:39:00,  1.60s/it]  4%|â–         | 273/6250 [07:41<2:38:45,  1.59s/it]  4%|â–         | 273/6250 [07:41<2:38:43,  1.59s/it]  4%|â–         | 274/6250 [07:43<2:38:43,  1.59s/it]  4%|â–         | 274/6250 [07:43<2:38:40,  1.59s/it]  4%|â–         | 275/6250 [07:45<2:38:35,  1.59s/it]  4%|â–         | 275/6250 [07:45<2:38:36,  1.59s/it]  4%|â–         | 276/6250 [07:46<2:38:43,  1.59s/it]  4%|â–         | 276/6250 [07:46<2:38:45,  1.59s/it]  4%|â–         | 277/6250 [07:48<2:38:53,  1.60s/it]  4%|â–         | 277/6250 [07:48<2:38:50,  1.60s/it]  4%|â–         | 278/6250 [07:49<2:38:48,  1.60s/it]  4%|â–         | 278/6250 [07:49<2:38:47,  1.60s/it]  4%|â–         | 279/6250 [07:51<2:38:53,  1.60s/it]  4%|â–         | 279/6250 [07:51<2:39:05,  1.60s/it]  4%|â–         | 280/6250 [07:53<2:39:04,  1.60s/it]2025-08-19 11:12:57,649 - PixArt - INFO - Step/Epoch [280/1][280/6250]:total_eta: 11:33:43, epoch_eta:2:47:32, time_all:1.595, time_data:0.002, lr:2.680e-06, s:(32, 32), loss:2.4204, grad_norm:90.2642
  4%|â–         | 280/6250 [07:53<2:39:07,  1.60s/it]  4%|â–         | 281/6250 [07:54<2:38:58,  1.60s/it]  4%|â–         | 281/6250 [07:54<2:38:53,  1.60s/it]  5%|â–         | 282/6250 [07:56<2:39:13,  1.60s/it]  5%|â–         | 282/6250 [07:56<2:39:12,  1.60s/it]  5%|â–         | 283/6250 [07:57<2:39:09,  1.60s/it]  5%|â–         | 283/6250 [07:57<2:39:10,  1.60s/it]  5%|â–         | 284/6250 [07:59<2:39:01,  1.60s/it]  5%|â–         | 284/6250 [07:59<2:39:02,  1.60s/it]  5%|â–         | 285/6250 [08:01<2:39:36,  1.61s/it]  5%|â–         | 285/6250 [08:01<2:39:37,  1.61s/it]  5%|â–         | 286/6250 [08:02<2:39:19,  1.60s/it]  5%|â–         | 286/6250 [08:02<2:39:21,  1.60s/it]  5%|â–         | 287/6250 [08:04<2:39:06,  1.60s/it]  5%|â–         | 287/6250 [08:04<2:39:03,  1.60s/it]  5%|â–         | 288/6250 [08:05<2:38:46,  1.60s/it]  5%|â–         | 288/6250 [08:05<2:38:44,  1.60s/it]  5%|â–         | 289/6250 [08:07<2:38:58,  1.60s/it]  5%|â–         | 289/6250 [08:07<2:38:58,  1.60s/it]  5%|â–         | 290/6250 [08:09<2:38:39,  1.60s/it]  5%|â–         | 290/6250 [08:09<2:38:37,  1.60s/it]  5%|â–         | 291/6250 [08:10<2:38:24,  1.59s/it]  5%|â–         | 291/6250 [08:10<2:38:23,  1.59s/it]  5%|â–         | 292/6250 [08:12<2:38:34,  1.60s/it]  5%|â–         | 292/6250 [08:12<2:38:35,  1.60s/it]  5%|â–         | 293/6250 [08:13<2:38:33,  1.60s/it]  5%|â–         | 293/6250 [08:13<2:38:31,  1.60s/it]  5%|â–         | 294/6250 [08:15<2:38:27,  1.60s/it]  5%|â–         | 294/6250 [08:15<2:38:27,  1.60s/it]  5%|â–         | 295/6250 [08:17<2:38:27,  1.60s/it]  5%|â–         | 295/6250 [08:17<2:38:32,  1.60s/it]  5%|â–         | 296/6250 [08:18<2:38:31,  1.60s/it]  5%|â–         | 296/6250 [08:18<2:38:27,  1.60s/it]  5%|â–         | 297/6250 [08:20<2:38:10,  1.59s/it]  5%|â–         | 297/6250 [08:20<2:38:09,  1.59s/it]  5%|â–         | 298/6250 [08:21<2:37:56,  1.59s/it]  5%|â–         | 298/6250 [08:21<2:37:56,  1.59s/it]  5%|â–         | 299/6250 [08:23<2:38:06,  1.59s/it]  5%|â–         | 299/6250 [08:23<2:38:07,  1.59s/it]2025-08-19 11:13:29,601 - PixArt - INFO - Step/Epoch [300/1][300/6250]:total_eta: 11:30:48, epoch_eta:2:46:24, time_all:1.598, time_data:0.002, lr:2.880e-06, s:(32, 32), loss:2.2141, grad_norm:56.3520
  5%|â–         | 300/6250 [08:25<2:38:14,  1.60s/it]  5%|â–         | 300/6250 [08:25<2:38:14,  1.60s/it]  5%|â–         | 301/6250 [08:26<2:38:39,  1.60s/it]  5%|â–         | 301/6250 [08:26<2:38:39,  1.60s/it]  5%|â–         | 302/6250 [08:28<2:38:35,  1.60s/it]  5%|â–         | 302/6250 [08:28<2:38:37,  1.60s/it]  5%|â–         | 303/6250 [08:29<2:38:36,  1.60s/it]  5%|â–         | 303/6250 [08:29<2:38:36,  1.60s/it]  5%|â–         | 304/6250 [08:31<2:38:29,  1.60s/it]  5%|â–         | 304/6250 [08:31<2:38:28,  1.60s/it]  5%|â–         | 305/6250 [08:33<2:38:25,  1.60s/it]  5%|â–         | 305/6250 [08:33<2:38:25,  1.60s/it]  5%|â–         | 306/6250 [08:34<2:38:05,  1.60s/it]  5%|â–         | 306/6250 [08:34<2:38:05,  1.60s/it]  5%|â–         | 307/6250 [08:36<2:38:13,  1.60s/it]  5%|â–         | 307/6250 [08:36<2:38:13,  1.60s/it]  5%|â–         | 308/6250 [08:37<2:38:18,  1.60s/it]  5%|â–         | 308/6250 [08:37<2:38:18,  1.60s/it]  5%|â–         | 309/6250 [08:39<2:38:23,  1.60s/it]  5%|â–         | 309/6250 [08:39<2:38:21,  1.60s/it]  5%|â–         | 310/6250 [08:41<2:38:41,  1.60s/it]  5%|â–         | 310/6250 [08:41<2:38:41,  1.60s/it]  5%|â–         | 311/6250 [08:42<2:38:43,  1.60s/it]  5%|â–         | 311/6250 [08:42<2:38:49,  1.60s/it]  5%|â–         | 312/6250 [08:44<2:38:43,  1.60s/it]  5%|â–         | 312/6250 [08:44<2:38:41,  1.60s/it]  5%|â–Œ         | 313/6250 [08:45<2:38:22,  1.60s/it]  5%|â–Œ         | 313/6250 [08:45<2:38:20,  1.60s/it]  5%|â–Œ         | 314/6250 [08:47<2:38:21,  1.60s/it]  5%|â–Œ         | 314/6250 [08:47<2:38:20,  1.60s/it]  5%|â–Œ         | 315/6250 [08:49<2:38:11,  1.60s/it]  5%|â–Œ         | 315/6250 [08:49<2:38:09,  1.60s/it]  5%|â–Œ         | 316/6250 [08:50<2:37:58,  1.60s/it]  5%|â–Œ         | 316/6250 [08:50<2:37:57,  1.60s/it]  5%|â–Œ         | 317/6250 [08:52<2:37:54,  1.60s/it]  5%|â–Œ         | 317/6250 [08:52<2:37:52,  1.60s/it]  5%|â–Œ         | 318/6250 [08:53<2:37:58,  1.60s/it]  5%|â–Œ         | 318/6250 [08:53<2:37:59,  1.60s/it]  5%|â–Œ         | 319/6250 [08:55<2:38:09,  1.60s/it]  5%|â–Œ         | 319/6250 [08:55<2:38:09,  1.60s/it]2025-08-19 11:14:01,604 - PixArt - INFO - Step/Epoch [320/1][320/6250]:total_eta: 11:28:14, epoch_eta:2:45:22, time_all:1.600, time_data:0.002, lr:3.080e-06, s:(32, 32), loss:2.1508, grad_norm:44.4239
  5%|â–Œ         | 320/6250 [08:57<2:38:08,  1.60s/it]  5%|â–Œ         | 320/6250 [08:57<2:38:17,  1.60s/it]  5%|â–Œ         | 321/6250 [08:58<2:39:31,  1.61s/it]  5%|â–Œ         | 321/6250 [08:58<2:39:31,  1.61s/it]  5%|â–Œ         | 322/6250 [09:00<2:50:28,  1.73s/it]  5%|â–Œ         | 322/6250 [09:00<2:50:30,  1.73s/it]  5%|â–Œ         | 323/6250 [09:02<2:50:37,  1.73s/it]  5%|â–Œ         | 323/6250 [09:02<2:50:34,  1.73s/it]  5%|â–Œ         | 324/6250 [09:04<2:46:32,  1.69s/it]  5%|â–Œ         | 324/6250 [09:04<2:46:30,  1.69s/it]  5%|â–Œ         | 325/6250 [09:05<2:43:47,  1.66s/it]  5%|â–Œ         | 325/6250 [09:05<2:43:47,  1.66s/it]  5%|â–Œ         | 326/6250 [09:07<2:42:03,  1.64s/it]  5%|â–Œ         | 326/6250 [09:07<2:42:01,  1.64s/it]  5%|â–Œ         | 327/6250 [09:08<2:40:41,  1.63s/it]  5%|â–Œ         | 327/6250 [09:08<2:40:42,  1.63s/it]  5%|â–Œ         | 328/6250 [09:10<2:39:41,  1.62s/it]  5%|â–Œ         | 328/6250 [09:10<2:39:41,  1.62s/it]  5%|â–Œ         | 329/6250 [09:12<2:39:11,  1.61s/it]  5%|â–Œ         | 329/6250 [09:12<2:39:15,  1.61s/it]  5%|â–Œ         | 330/6250 [09:13<2:41:14,  1.63s/it]  5%|â–Œ         | 330/6250 [09:13<2:41:13,  1.63s/it]  5%|â–Œ         | 331/6250 [09:15<2:40:20,  1.63s/it]  5%|â–Œ         | 331/6250 [09:15<2:40:18,  1.63s/it]  5%|â–Œ         | 332/6250 [09:16<2:39:25,  1.62s/it]  5%|â–Œ         | 332/6250 [09:16<2:39:22,  1.62s/it]  5%|â–Œ         | 333/6250 [09:18<2:38:54,  1.61s/it]  5%|â–Œ         | 333/6250 [09:18<2:38:57,  1.61s/it]  5%|â–Œ         | 334/6250 [09:20<2:38:27,  1.61s/it]  5%|â–Œ         | 334/6250 [09:20<2:38:24,  1.61s/it]  5%|â–Œ         | 335/6250 [09:21<2:38:24,  1.61s/it]  5%|â–Œ         | 335/6250 [09:21<2:38:21,  1.61s/it]  5%|â–Œ         | 336/6250 [09:23<2:38:06,  1.60s/it]  5%|â–Œ         | 336/6250 [09:23<2:38:05,  1.60s/it]  5%|â–Œ         | 337/6250 [09:24<2:37:57,  1.60s/it]  5%|â–Œ         | 337/6250 [09:24<2:37:57,  1.60s/it]  5%|â–Œ         | 338/6250 [09:26<2:37:57,  1.60s/it]  5%|â–Œ         | 338/6250 [09:26<2:37:55,  1.60s/it]  5%|â–Œ         | 339/6250 [09:28<2:37:51,  1.60s/it]  5%|â–Œ         | 339/6250 [09:28<2:37:51,  1.60s/it]  5%|â–Œ         | 340/6250 [09:29<2:37:40,  1.60s/it]2025-08-19 11:14:34,232 - PixArt - INFO - Step/Epoch [340/1][340/6250]:total_eta: 11:26:40, epoch_eta:2:44:34, time_all:1.631, time_data:0.002, lr:3.280e-06, s:(32, 32), loss:2.1694, grad_norm:44.3026
  5%|â–Œ         | 340/6250 [09:29<2:37:41,  1.60s/it]  5%|â–Œ         | 341/6250 [09:31<2:37:31,  1.60s/it]  5%|â–Œ         | 341/6250 [09:31<2:37:31,  1.60s/it]  5%|â–Œ         | 342/6250 [09:32<2:37:20,  1.60s/it]  5%|â–Œ         | 342/6250 [09:32<2:37:22,  1.60s/it]  5%|â–Œ         | 343/6250 [09:34<2:37:54,  1.60s/it]  5%|â–Œ         | 343/6250 [09:34<2:37:53,  1.60s/it]  6%|â–Œ         | 344/6250 [09:36<2:37:34,  1.60s/it]  6%|â–Œ         | 344/6250 [09:36<2:37:32,  1.60s/it]  6%|â–Œ         | 345/6250 [09:37<2:37:23,  1.60s/it]  6%|â–Œ         | 345/6250 [09:37<2:37:23,  1.60s/it]  6%|â–Œ         | 346/6250 [09:39<2:37:26,  1.60s/it]  6%|â–Œ         | 346/6250 [09:39<2:37:26,  1.60s/it]  6%|â–Œ         | 347/6250 [09:40<2:37:32,  1.60s/it]  6%|â–Œ         | 347/6250 [09:40<2:37:33,  1.60s/it]  6%|â–Œ         | 348/6250 [09:42<2:37:34,  1.60s/it]  6%|â–Œ         | 348/6250 [09:42<2:37:33,  1.60s/it]  6%|â–Œ         | 349/6250 [09:44<2:37:34,  1.60s/it]  6%|â–Œ         | 349/6250 [09:44<2:37:41,  1.60s/it]  6%|â–Œ         | 350/6250 [09:45<2:37:21,  1.60s/it]  6%|â–Œ         | 350/6250 [09:45<2:37:18,  1.60s/it]  6%|â–Œ         | 351/6250 [09:47<2:37:07,  1.60s/it]  6%|â–Œ         | 351/6250 [09:47<2:37:06,  1.60s/it]  6%|â–Œ         | 352/6250 [09:48<2:37:11,  1.60s/it]  6%|â–Œ         | 352/6250 [09:48<2:37:11,  1.60s/it]  6%|â–Œ         | 353/6250 [09:50<2:37:14,  1.60s/it]  6%|â–Œ         | 353/6250 [09:50<2:37:13,  1.60s/it]  6%|â–Œ         | 354/6250 [09:52<2:37:12,  1.60s/it]  6%|â–Œ         | 354/6250 [09:52<2:37:10,  1.60s/it]  6%|â–Œ         | 355/6250 [09:53<2:37:13,  1.60s/it]  6%|â–Œ         | 355/6250 [09:53<2:37:13,  1.60s/it]  6%|â–Œ         | 356/6250 [09:55<2:37:02,  1.60s/it]  6%|â–Œ         | 356/6250 [09:55<2:37:01,  1.60s/it]  6%|â–Œ         | 357/6250 [09:56<2:37:02,  1.60s/it]  6%|â–Œ         | 357/6250 [09:56<2:37:03,  1.60s/it]  6%|â–Œ         | 358/6250 [09:58<2:37:42,  1.61s/it]  6%|â–Œ         | 358/6250 [09:58<2:37:41,  1.61s/it]  6%|â–Œ         | 359/6250 [10:00<2:37:20,  1.60s/it]  6%|â–Œ         | 359/6250 [10:00<2:37:19,  1.60s/it]2025-08-19 11:15:06,253 - PixArt - INFO - Step/Epoch [360/1][360/6250]:total_eta: 11:24:31, epoch_eta:2:43:38, time_all:1.601, time_data:0.002, lr:3.480e-06, s:(32, 32), loss:2.1498, grad_norm:55.8473
  6%|â–Œ         | 360/6250 [10:01<2:37:25,  1.60s/it]  6%|â–Œ         | 360/6250 [10:01<2:37:25,  1.60s/it]  6%|â–Œ         | 361/6250 [10:03<2:37:08,  1.60s/it]  6%|â–Œ         | 361/6250 [10:03<2:37:11,  1.60s/it]  6%|â–Œ         | 362/6250 [10:04<2:37:05,  1.60s/it]  6%|â–Œ         | 362/6250 [10:04<2:37:05,  1.60s/it]  6%|â–Œ         | 363/6250 [10:06<2:36:55,  1.60s/it]  6%|â–Œ         | 363/6250 [10:06<2:36:55,  1.60s/it]  6%|â–Œ         | 364/6250 [10:08<2:37:06,  1.60s/it]  6%|â–Œ         | 364/6250 [10:08<2:37:07,  1.60s/it]  6%|â–Œ         | 365/6250 [10:09<2:36:59,  1.60s/it]  6%|â–Œ         | 365/6250 [10:09<2:37:06,  1.60s/it]  6%|â–Œ         | 366/6250 [10:11<2:36:54,  1.60s/it]  6%|â–Œ         | 366/6250 [10:11<2:36:50,  1.60s/it]  6%|â–Œ         | 367/6250 [10:12<2:36:32,  1.60s/it]  6%|â–Œ         | 367/6250 [10:12<2:36:34,  1.60s/it]  6%|â–Œ         | 368/6250 [10:14<2:36:35,  1.60s/it]  6%|â–Œ         | 368/6250 [10:14<2:36:32,  1.60s/it]  6%|â–Œ         | 369/6250 [10:16<2:36:22,  1.60s/it]  6%|â–Œ         | 369/6250 [10:16<2:36:20,  1.60s/it]  6%|â–Œ         | 370/6250 [10:17<2:36:32,  1.60s/it]  6%|â–Œ         | 370/6250 [10:17<2:36:34,  1.60s/it]  6%|â–Œ         | 371/6250 [10:19<2:36:21,  1.60s/it]  6%|â–Œ         | 371/6250 [10:19<2:36:18,  1.60s/it]  6%|â–Œ         | 372/6250 [10:20<2:36:12,  1.59s/it]  6%|â–Œ         | 372/6250 [10:20<2:36:11,  1.59s/it]  6%|â–Œ         | 373/6250 [10:22<2:36:12,  1.59s/it]  6%|â–Œ         | 373/6250 [10:22<2:36:13,  1.59s/it]  6%|â–Œ         | 374/6250 [10:24<2:36:20,  1.60s/it]  6%|â–Œ         | 374/6250 [10:24<2:36:20,  1.60s/it]  6%|â–Œ         | 375/6250 [10:25<2:36:17,  1.60s/it]  6%|â–Œ         | 375/6250 [10:25<2:36:19,  1.60s/it]  6%|â–Œ         | 376/6250 [10:27<2:36:19,  1.60s/it]  6%|â–Œ         | 376/6250 [10:27<2:36:16,  1.60s/it]  6%|â–Œ         | 377/6250 [10:28<2:36:16,  1.60s/it]  6%|â–Œ         | 377/6250 [10:28<2:36:16,  1.60s/it]  6%|â–Œ         | 378/6250 [10:30<2:36:15,  1.60s/it]  6%|â–Œ         | 378/6250 [10:30<2:36:15,  1.60s/it]  6%|â–Œ         | 379/6250 [10:32<2:36:19,  1.60s/it]  6%|â–Œ         | 379/6250 [10:32<2:36:19,  1.60s/it]2025-08-19 11:15:38,202 - PixArt - INFO - Step/Epoch [380/1][380/6250]:total_eta: 11:22:28, epoch_eta:2:42:43, time_all:1.597, time_data:0.002, lr:3.680e-06, s:(32, 32), loss:2.0240, grad_norm:99.0773
  6%|â–Œ         | 380/6250 [10:33<2:36:35,  1.60s/it]  6%|â–Œ         | 380/6250 [10:33<2:36:36,  1.60s/it]  6%|â–Œ         | 381/6250 [10:35<2:36:31,  1.60s/it]  6%|â–Œ         | 381/6250 [10:35<2:36:38,  1.60s/it]  6%|â–Œ         | 382/6250 [10:36<2:36:36,  1.60s/it]  6%|â–Œ         | 382/6250 [10:36<2:36:33,  1.60s/it]  6%|â–Œ         | 383/6250 [10:38<2:36:25,  1.60s/it]  6%|â–Œ         | 383/6250 [10:38<2:36:28,  1.60s/it]  6%|â–Œ         | 384/6250 [10:40<2:36:42,  1.60s/it]  6%|â–Œ         | 384/6250 [10:40<2:36:40,  1.60s/it]  6%|â–Œ         | 385/6250 [10:41<2:36:36,  1.60s/it]  6%|â–Œ         | 385/6250 [10:41<2:36:35,  1.60s/it]  6%|â–Œ         | 386/6250 [10:43<2:36:48,  1.60s/it]  6%|â–Œ         | 386/6250 [10:43<2:36:46,  1.60s/it]  6%|â–Œ         | 387/6250 [10:44<2:36:43,  1.60s/it]  6%|â–Œ         | 387/6250 [10:44<2:36:41,  1.60s/it]  6%|â–Œ         | 388/6250 [10:46<2:36:35,  1.60s/it]  6%|â–Œ         | 388/6250 [10:46<2:36:33,  1.60s/it]  6%|â–Œ         | 389/6250 [10:48<2:38:05,  1.62s/it]  6%|â–Œ         | 389/6250 [10:48<2:38:05,  1.62s/it]  6%|â–Œ         | 390/6250 [10:49<2:37:33,  1.61s/it]  6%|â–Œ         | 390/6250 [10:49<2:37:32,  1.61s/it]  6%|â–‹         | 391/6250 [10:51<2:37:05,  1.61s/it]  6%|â–‹         | 391/6250 [10:51<2:37:07,  1.61s/it]  6%|â–‹         | 392/6250 [10:53<2:37:01,  1.61s/it]  6%|â–‹         | 392/6250 [10:53<2:37:04,  1.61s/it]  6%|â–‹         | 393/6250 [10:54<2:36:49,  1.61s/it]  6%|â–‹         | 393/6250 [10:54<2:36:52,  1.61s/it]  6%|â–‹         | 394/6250 [10:56<2:36:29,  1.60s/it]  6%|â–‹         | 394/6250 [10:56<2:36:28,  1.60s/it]  6%|â–‹         | 395/6250 [10:57<2:36:29,  1.60s/it]  6%|â–‹         | 395/6250 [10:57<2:36:34,  1.60s/it]  6%|â–‹         | 396/6250 [10:59<2:36:19,  1.60s/it]  6%|â–‹         | 396/6250 [10:59<2:36:20,  1.60s/it]  6%|â–‹         | 397/6250 [11:01<2:36:06,  1.60s/it]  6%|â–‹         | 397/6250 [11:01<2:36:16,  1.60s/it]  6%|â–‹         | 398/6250 [11:02<2:36:12,  1.60s/it]  6%|â–‹         | 398/6250 [11:02<2:36:09,  1.60s/it]  6%|â–‹         | 399/6250 [11:04<2:36:06,  1.60s/it]  6%|â–‹         | 399/6250 [11:04<2:36:02,  1.60s/it]  6%|â–‹         | 400/6250 [11:05<2:35:50,  1.60s/it]2025-08-19 11:16:10,281 - PixArt - INFO - Step/Epoch [400/1][400/6250]:total_eta: 11:20:42, epoch_eta:2:41:52, time_all:1.604, time_data:0.002, lr:3.880e-06, s:(32, 32), loss:2.1010, grad_norm:66.1052
  6%|â–‹         | 400/6250 [11:05<2:36:03,  1.60s/it]  6%|â–‹         | 401/6250 [11:07<2:35:59,  1.60s/it]  6%|â–‹         | 401/6250 [11:07<2:35:55,  1.60s/it]  6%|â–‹         | 402/6250 [11:08<2:35:46,  1.60s/it]  6%|â–‹         | 402/6250 [11:08<2:35:42,  1.60s/it]  6%|â–‹         | 403/6250 [11:10<2:35:36,  1.60s/it]  6%|â–‹         | 403/6250 [11:10<2:35:34,  1.60s/it]  6%|â–‹         | 404/6250 [11:12<2:36:17,  1.60s/it]  6%|â–‹         | 404/6250 [11:12<2:36:14,  1.60s/it]  6%|â–‹         | 405/6250 [11:13<2:36:05,  1.60s/it]  6%|â–‹         | 405/6250 [11:13<2:36:04,  1.60s/it]  6%|â–‹         | 406/6250 [11:15<2:36:00,  1.60s/it]  6%|â–‹         | 406/6250 [11:15<2:35:59,  1.60s/it]  7%|â–‹         | 407/6250 [11:17<2:35:44,  1.60s/it]  7%|â–‹         | 407/6250 [11:17<2:35:42,  1.60s/it]  7%|â–‹         | 408/6250 [11:18<2:35:56,  1.60s/it]  7%|â–‹         | 408/6250 [11:18<2:35:52,  1.60s/it]  7%|â–‹         | 409/6250 [11:20<2:35:42,  1.60s/it]  7%|â–‹         | 409/6250 [11:20<2:35:41,  1.60s/it]  7%|â–‹         | 410/6250 [11:21<2:35:31,  1.60s/it]  7%|â–‹         | 410/6250 [11:21<2:35:31,  1.60s/it]  7%|â–‹         | 411/6250 [11:23<2:35:41,  1.60s/it]  7%|â–‹         | 411/6250 [11:23<2:35:40,  1.60s/it]  7%|â–‹         | 412/6250 [11:24<2:35:29,  1.60s/it]  7%|â–‹         | 412/6250 [11:24<2:35:28,  1.60s/it]  7%|â–‹         | 413/6250 [11:26<2:35:38,  1.60s/it]  7%|â–‹         | 413/6250 [11:26<2:35:47,  1.60s/it]  7%|â–‹         | 414/6250 [11:28<2:35:52,  1.60s/it]  7%|â–‹         | 414/6250 [11:28<2:35:50,  1.60s/it]  7%|â–‹         | 415/6250 [11:29<2:35:39,  1.60s/it]  7%|â–‹         | 415/6250 [11:29<2:35:39,  1.60s/it]  7%|â–‹         | 416/6250 [11:31<2:36:23,  1.61s/it]  7%|â–‹         | 416/6250 [11:31<2:36:21,  1.61s/it]  7%|â–‹         | 417/6250 [11:33<2:36:16,  1.61s/it]  7%|â–‹         | 417/6250 [11:33<2:36:15,  1.61s/it]  7%|â–‹         | 418/6250 [11:34<2:35:43,  1.60s/it]  7%|â–‹         | 418/6250 [11:34<2:35:42,  1.60s/it]  7%|â–‹         | 419/6250 [11:36<2:35:49,  1.60s/it]  7%|â–‹         | 419/6250 [11:36<2:35:48,  1.60s/it]2025-08-19 11:16:42,322 - PixArt - INFO - Step/Epoch [420/1][420/6250]:total_eta: 11:19:01, epoch_eta:2:41:03, time_all:1.602, time_data:0.004, lr:4.080e-06, s:(32, 32), loss:2.0247, grad_norm:35.2797
  7%|â–‹         | 420/6250 [11:37<2:35:51,  1.60s/it]  7%|â–‹         | 420/6250 [11:37<2:35:53,  1.60s/it]  7%|â–‹         | 421/6250 [11:39<2:35:28,  1.60s/it]  7%|â–‹         | 421/6250 [11:39<2:35:28,  1.60s/it]  7%|â–‹         | 422/6250 [11:41<2:35:19,  1.60s/it]  7%|â–‹         | 422/6250 [11:41<2:35:18,  1.60s/it]  7%|â–‹         | 423/6250 [11:42<2:35:16,  1.60s/it]  7%|â–‹         | 423/6250 [11:42<2:35:16,  1.60s/it]  7%|â–‹         | 424/6250 [11:44<2:35:15,  1.60s/it]  7%|â–‹         | 424/6250 [11:44<2:35:16,  1.60s/it]  7%|â–‹         | 425/6250 [11:45<2:35:13,  1.60s/it]  7%|â–‹         | 425/6250 [11:45<2:35:12,  1.60s/it]  7%|â–‹         | 426/6250 [11:47<2:34:58,  1.60s/it]  7%|â–‹         | 426/6250 [11:47<2:34:57,  1.60s/it]  7%|â–‹         | 427/6250 [11:49<2:35:17,  1.60s/it]  7%|â–‹         | 427/6250 [11:49<2:35:17,  1.60s/it]  7%|â–‹         | 428/6250 [11:50<2:35:11,  1.60s/it]  7%|â–‹         | 428/6250 [11:50<2:35:10,  1.60s/it]  7%|â–‹         | 429/6250 [11:52<2:34:55,  1.60s/it]  7%|â–‹         | 429/6250 [11:52<2:34:57,  1.60s/it]  7%|â–‹         | 430/6250 [11:53<2:35:14,  1.60s/it]  7%|â–‹         | 430/6250 [11:53<2:35:13,  1.60s/it]  7%|â–‹         | 431/6250 [11:55<2:35:09,  1.60s/it]  7%|â–‹         | 431/6250 [11:55<2:35:09,  1.60s/it]  7%|â–‹         | 432/6250 [11:57<2:35:25,  1.60s/it]  7%|â–‹         | 432/6250 [11:57<2:35:24,  1.60s/it]  7%|â–‹         | 433/6250 [11:58<2:35:11,  1.60s/it]  7%|â–‹         | 433/6250 [11:58<2:35:10,  1.60s/it]  7%|â–‹         | 434/6250 [12:00<2:35:13,  1.60s/it]  7%|â–‹         | 434/6250 [12:00<2:35:11,  1.60s/it]  7%|â–‹         | 435/6250 [12:01<2:35:08,  1.60s/it]  7%|â–‹         | 435/6250 [12:01<2:35:09,  1.60s/it]  7%|â–‹         | 436/6250 [12:03<2:35:08,  1.60s/it]  7%|â–‹         | 436/6250 [12:03<2:35:08,  1.60s/it]  7%|â–‹         | 437/6250 [12:05<2:35:10,  1.60s/it]  7%|â–‹         | 437/6250 [12:05<2:35:09,  1.60s/it]  7%|â–‹         | 438/6250 [12:06<2:35:03,  1.60s/it]  7%|â–‹         | 438/6250 [12:06<2:35:04,  1.60s/it]  7%|â–‹         | 439/6250 [12:08<2:35:01,  1.60s/it]  7%|â–‹         | 439/6250 [12:08<2:35:07,  1.60s/it]  7%|â–‹         | 440/6250 [12:09<2:35:11,  1.60s/it]2025-08-19 11:17:14,320 - PixArt - INFO - Step/Epoch [440/1][440/6250]:total_eta: 11:17:24, epoch_eta:2:40:15, time_all:1.600, time_data:0.002, lr:4.280e-06, s:(32, 32), loss:1.8682, grad_norm:30.9202
  7%|â–‹         | 440/6250 [12:09<2:35:09,  1.60s/it]  7%|â–‹         | 441/6250 [12:11<2:34:46,  1.60s/it]  7%|â–‹         | 441/6250 [12:11<2:34:47,  1.60s/it]  7%|â–‹         | 442/6250 [12:13<2:34:36,  1.60s/it]  7%|â–‹         | 442/6250 [12:13<2:34:38,  1.60s/it]  7%|â–‹         | 443/6250 [12:14<2:34:42,  1.60s/it]  7%|â–‹         | 443/6250 [12:14<2:34:41,  1.60s/it]  7%|â–‹         | 444/6250 [12:16<2:34:42,  1.60s/it]  7%|â–‹         | 444/6250 [12:16<2:34:40,  1.60s/it]  7%|â–‹         | 445/6250 [12:17<2:34:37,  1.60s/it]  7%|â–‹         | 445/6250 [12:17<2:34:48,  1.60s/it]  7%|â–‹         | 446/6250 [12:19<2:34:43,  1.60s/it]  7%|â–‹         | 446/6250 [12:19<2:34:39,  1.60s/it]  7%|â–‹         | 447/6250 [12:21<2:34:34,  1.60s/it]  7%|â–‹         | 447/6250 [12:21<2:34:34,  1.60s/it]  7%|â–‹         | 448/6250 [12:22<2:34:29,  1.60s/it]  7%|â–‹         | 448/6250 [12:22<2:34:26,  1.60s/it]  7%|â–‹         | 449/6250 [12:24<2:34:22,  1.60s/it]  7%|â–‹         | 449/6250 [12:24<2:34:19,  1.60s/it]  7%|â–‹         | 450/6250 [12:25<2:34:30,  1.60s/it]  7%|â–‹         | 450/6250 [12:25<2:34:27,  1.60s/it]  7%|â–‹         | 451/6250 [12:27<2:34:27,  1.60s/it]  7%|â–‹         | 451/6250 [12:27<2:34:25,  1.60s/it]  7%|â–‹         | 452/6250 [12:28<2:34:12,  1.60s/it]  7%|â–‹         | 452/6250 [12:28<2:34:11,  1.60s/it]  7%|â–‹         | 453/6250 [12:30<2:34:11,  1.60s/it]  7%|â–‹         | 453/6250 [12:30<2:34:11,  1.60s/it]  7%|â–‹         | 454/6250 [12:32<2:34:08,  1.60s/it]  7%|â–‹         | 454/6250 [12:32<2:34:09,  1.60s/it]  7%|â–‹         | 455/6250 [12:33<2:34:08,  1.60s/it]  7%|â–‹         | 455/6250 [12:33<2:34:09,  1.60s/it]  7%|â–‹         | 456/6250 [12:35<2:34:03,  1.60s/it]  7%|â–‹         | 456/6250 [12:35<2:34:05,  1.60s/it]  7%|â–‹         | 457/6250 [12:36<2:34:09,  1.60s/it]  7%|â–‹         | 457/6250 [12:36<2:34:10,  1.60s/it]  7%|â–‹         | 458/6250 [12:38<2:34:07,  1.60s/it]  7%|â–‹         | 458/6250 [12:38<2:34:12,  1.60s/it]  7%|â–‹         | 459/6250 [12:40<2:34:15,  1.60s/it]  7%|â–‹         | 459/6250 [12:40<2:34:14,  1.60s/it]  7%|â–‹         | 460/6250 [12:41<2:34:03,  1.60s/it]2025-08-19 11:17:46,252 - PixArt - INFO - Step/Epoch [460/1][460/6250]:total_eta: 11:15:48, epoch_eta:2:39:27, time_all:1.597, time_data:0.002, lr:4.480e-06, s:(32, 32), loss:1.8821, grad_norm:36.2699
  7%|â–‹         | 460/6250 [12:41<2:34:02,  1.60s/it]  7%|â–‹         | 461/6250 [12:43<2:33:52,  1.59s/it]  7%|â–‹         | 461/6250 [12:43<2:33:58,  1.60s/it]  7%|â–‹         | 462/6250 [12:44<2:34:10,  1.60s/it]  7%|â–‹         | 462/6250 [12:44<2:34:10,  1.60s/it]  7%|â–‹         | 463/6250 [12:46<2:33:55,  1.60s/it]  7%|â–‹         | 463/6250 [12:46<2:33:55,  1.60s/it]  7%|â–‹         | 464/6250 [12:48<2:34:00,  1.60s/it]  7%|â–‹         | 464/6250 [12:48<2:33:58,  1.60s/it]  7%|â–‹         | 465/6250 [12:49<2:33:42,  1.59s/it]  7%|â–‹         | 465/6250 [12:49<2:33:40,  1.59s/it]  7%|â–‹         | 466/6250 [12:51<2:33:36,  1.59s/it]  7%|â–‹         | 466/6250 [12:51<2:33:46,  1.60s/it]  7%|â–‹         | 467/6250 [12:52<2:34:06,  1.60s/it]  7%|â–‹         | 467/6250 [12:52<2:34:01,  1.60s/it]  7%|â–‹         | 468/6250 [12:54<2:33:57,  1.60s/it]  7%|â–‹         | 468/6250 [12:54<2:33:53,  1.60s/it]  8%|â–Š         | 469/6250 [12:56<2:34:14,  1.60s/it]  8%|â–Š         | 469/6250 [12:56<2:34:12,  1.60s/it]  8%|â–Š         | 470/6250 [12:57<2:34:06,  1.60s/it]  8%|â–Š         | 470/6250 [12:57<2:34:06,  1.60s/it]  8%|â–Š         | 471/6250 [12:59<2:34:03,  1.60s/it]  8%|â–Š         | 471/6250 [12:59<2:34:05,  1.60s/it]  8%|â–Š         | 472/6250 [13:00<2:34:12,  1.60s/it]  8%|â–Š         | 472/6250 [13:00<2:34:11,  1.60s/it]  8%|â–Š         | 473/6250 [13:02<2:34:02,  1.60s/it]  8%|â–Š         | 473/6250 [13:02<2:34:00,  1.60s/it]  8%|â–Š         | 474/6250 [13:04<2:33:51,  1.60s/it]  8%|â–Š         | 474/6250 [13:04<2:33:51,  1.60s/it]  8%|â–Š         | 475/6250 [13:05<2:33:50,  1.60s/it]  8%|â–Š         | 475/6250 [13:05<2:33:51,  1.60s/it]  8%|â–Š         | 476/6250 [13:07<2:33:58,  1.60s/it]  8%|â–Š         | 476/6250 [13:07<2:33:57,  1.60s/it]  8%|â–Š         | 477/6250 [13:08<2:33:58,  1.60s/it]  8%|â–Š         | 477/6250 [13:08<2:34:07,  1.60s/it]  8%|â–Š         | 478/6250 [13:10<2:34:02,  1.60s/it]  8%|â–Š         | 478/6250 [13:10<2:33:58,  1.60s/it]  8%|â–Š         | 479/6250 [13:12<2:33:48,  1.60s/it]  8%|â–Š         | 479/6250 [13:12<2:33:48,  1.60s/it]2025-08-19 11:18:18,228 - PixArt - INFO - Step/Epoch [480/1][480/6250]:total_eta: 11:14:21, epoch_eta:2:38:41, time_all:1.599, time_data:0.002, lr:4.680e-06, s:(32, 32), loss:1.8953, grad_norm:29.9069
  8%|â–Š         | 480/6250 [13:13<2:33:50,  1.60s/it]  8%|â–Š         | 480/6250 [13:13<2:33:46,  1.60s/it]  8%|â–Š         | 481/6250 [13:15<2:33:33,  1.60s/it]  8%|â–Š         | 481/6250 [13:15<2:33:32,  1.60s/it]  8%|â–Š         | 482/6250 [13:16<2:33:20,  1.60s/it]  8%|â–Š         | 482/6250 [13:16<2:33:20,  1.60s/it]  8%|â–Š         | 483/6250 [13:18<2:33:17,  1.59s/it]  8%|â–Š         | 483/6250 [13:18<2:33:16,  1.59s/it]  8%|â–Š         | 484/6250 [13:20<2:33:31,  1.60s/it]  8%|â–Š         | 484/6250 [13:20<2:33:30,  1.60s/it]  8%|â–Š         | 485/6250 [13:21<2:33:23,  1.60s/it]  8%|â–Š         | 485/6250 [13:21<2:33:23,  1.60s/it]  8%|â–Š         | 486/6250 [13:23<2:33:23,  1.60s/it]  8%|â–Š         | 486/6250 [13:23<2:33:21,  1.60s/it]  8%|â–Š         | 487/6250 [13:24<2:33:28,  1.60s/it]  8%|â–Š         | 487/6250 [13:24<2:33:28,  1.60s/it]  8%|â–Š         | 488/6250 [13:26<2:33:17,  1.60s/it]  8%|â–Š         | 488/6250 [13:26<2:33:17,  1.60s/it]  8%|â–Š         | 489/6250 [13:28<2:33:05,  1.59s/it]  8%|â–Š         | 489/6250 [13:28<2:33:04,  1.59s/it]  8%|â–Š         | 490/6250 [13:29<2:33:12,  1.60s/it]  8%|â–Š         | 490/6250 [13:29<2:33:19,  1.60s/it]  8%|â–Š         | 491/6250 [13:31<2:33:17,  1.60s/it]  8%|â–Š         | 491/6250 [13:31<2:33:13,  1.60s/it]  8%|â–Š         | 492/6250 [13:32<2:33:18,  1.60s/it]  8%|â–Š         | 492/6250 [13:32<2:33:15,  1.60s/it]  8%|â–Š         | 493/6250 [13:34<2:33:12,  1.60s/it]  8%|â–Š         | 493/6250 [13:34<2:33:14,  1.60s/it]  8%|â–Š         | 494/6250 [13:36<2:33:14,  1.60s/it]  8%|â–Š         | 494/6250 [13:36<2:33:13,  1.60s/it]  8%|â–Š         | 495/6250 [13:37<2:33:08,  1.60s/it]  8%|â–Š         | 495/6250 [13:37<2:33:06,  1.60s/it]  8%|â–Š         | 496/6250 [13:39<2:32:55,  1.59s/it]  8%|â–Š         | 496/6250 [13:39<2:32:55,  1.59s/it]  8%|â–Š         | 497/6250 [13:40<2:32:59,  1.60s/it]  8%|â–Š         | 497/6250 [13:40<2:32:59,  1.60s/it]  8%|â–Š         | 498/6250 [13:42<2:33:05,  1.60s/it]  8%|â–Š         | 498/6250 [13:42<2:33:06,  1.60s/it]  8%|â–Š         | 499/6250 [13:44<2:33:27,  1.60s/it]2025-08-19 11:18:48,579 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.20it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.22it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.20it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.07it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.08it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.12it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.09it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.19it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.17it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.10it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 11.99it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:01<00:00, 11.77it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.87it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.21it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.16it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 11.94it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.94it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.00it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.05it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:01<00:00, 12.99it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.19it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.15it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.16it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.14it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.13it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.11it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.21it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.16it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.15it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.15it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.15it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.12it/s]
  8%|â–Š         | 499/6250 [13:52<6:22:46,  3.99s/it]2025-08-19 11:18:58,147 - PixArt - INFO - Step/Epoch [500/1][500/6250]:total_eta: 11:19:26, epoch_eta:2:39:27, time_all:1.996, time_data:0.402, lr:4.880e-06, s:(32, 32), loss:1.8346, grad_norm:33.3613
  8%|â–Š         | 500/6250 [13:53<6:22:49,  3.99s/it]  8%|â–Š         | 500/6250 [13:53<5:14:01,  3.28s/it]  8%|â–Š         | 501/6250 [13:55<5:13:38,  3.27s/it]  8%|â–Š         | 501/6250 [13:55<4:25:27,  2.77s/it]  8%|â–Š         | 502/6250 [13:56<4:25:17,  2.77s/it]  8%|â–Š         | 502/6250 [13:56<3:51:36,  2.42s/it]  8%|â–Š         | 503/6250 [13:58<3:51:19,  2.42s/it]  8%|â–Š         | 503/6250 [13:58<3:27:48,  2.17s/it]  8%|â–Š         | 504/6250 [14:00<3:27:51,  2.17s/it]  8%|â–Š         | 504/6250 [14:00<3:11:19,  2.00s/it]  8%|â–Š         | 505/6250 [14:01<3:11:02,  2.00s/it]  8%|â–Š         | 505/6250 [14:01<2:59:27,  1.87s/it]  8%|â–Š         | 506/6250 [14:03<2:59:24,  1.87s/it]  8%|â–Š         | 506/6250 [14:03<2:51:21,  1.79s/it]  8%|â–Š         | 507/6250 [14:04<2:51:29,  1.79s/it]  8%|â–Š         | 507/6250 [14:04<2:45:50,  1.73s/it]  8%|â–Š         | 508/6250 [14:06<2:45:52,  1.73s/it]  8%|â–Š         | 508/6250 [14:06<2:41:55,  1.69s/it]  8%|â–Š         | 509/6250 [14:08<2:42:03,  1.69s/it]  8%|â–Š         | 509/6250 [14:08<2:39:19,  1.67s/it]  8%|â–Š         | 510/6250 [14:09<2:39:03,  1.66s/it]  8%|â–Š         | 510/6250 [14:09<2:37:06,  1.64s/it]  8%|â–Š         | 511/6250 [14:11<2:37:05,  1.64s/it]  8%|â–Š         | 511/6250 [14:11<2:35:42,  1.63s/it]  8%|â–Š         | 512/6250 [14:12<2:35:35,  1.63s/it]  8%|â–Š         | 512/6250 [14:12<2:34:37,  1.62s/it]  8%|â–Š         | 513/6250 [14:14<2:34:38,  1.62s/it]  8%|â–Š         | 513/6250 [14:14<2:33:58,  1.61s/it]  8%|â–Š         | 514/6250 [14:15<2:33:44,  1.61s/it]  8%|â–Š         | 514/6250 [14:15<2:33:19,  1.60s/it]  8%|â–Š         | 515/6250 [14:17<2:33:18,  1.60s/it]  8%|â–Š         | 515/6250 [14:17<2:32:57,  1.60s/it]  8%|â–Š         | 516/6250 [14:19<2:33:00,  1.60s/it]  8%|â–Š         | 516/6250 [14:19<2:32:46,  1.60s/it]  8%|â–Š         | 517/6250 [14:20<2:32:36,  1.60s/it]  8%|â–Š         | 517/6250 [14:20<2:32:30,  1.60s/it]  8%|â–Š         | 518/6250 [14:22<2:32:33,  1.60s/it]  8%|â–Š         | 518/6250 [14:22<2:32:37,  1.60s/it]  8%|â–Š         | 519/6250 [14:23<2:32:43,  1.60s/it]  8%|â–Š         | 519/6250 [14:23<2:32:37,  1.60s/it]2025-08-19 11:19:30,022 - PixArt - INFO - Step/Epoch [520/1][520/6250]:total_eta: 11:17:46, epoch_eta:2:38:39, time_all:1.594, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.9973, grad_norm:54.3079
  8%|â–Š         | 520/6250 [14:25<2:32:32,  1.60s/it]  8%|â–Š         | 520/6250 [14:25<2:32:31,  1.60s/it]  8%|â–Š         | 521/6250 [14:27<2:32:30,  1.60s/it]  8%|â–Š         | 521/6250 [14:27<2:32:22,  1.60s/it]  8%|â–Š         | 522/6250 [14:28<2:32:26,  1.60s/it]  8%|â–Š         | 522/6250 [14:28<2:32:23,  1.60s/it]  8%|â–Š         | 523/6250 [14:30<2:32:39,  1.60s/it]  8%|â–Š         | 523/6250 [14:30<2:32:34,  1.60s/it]  8%|â–Š         | 524/6250 [14:31<2:32:33,  1.60s/it]  8%|â–Š         | 524/6250 [14:31<2:32:32,  1.60s/it]  8%|â–Š         | 525/6250 [14:33<2:32:35,  1.60s/it]  8%|â–Š         | 525/6250 [14:33<2:32:33,  1.60s/it]  8%|â–Š         | 526/6250 [14:35<2:32:28,  1.60s/it]  8%|â–Š         | 526/6250 [14:35<2:32:26,  1.60s/it]  8%|â–Š         | 527/6250 [14:36<2:32:22,  1.60s/it]  8%|â–Š         | 527/6250 [14:36<2:32:22,  1.60s/it]  8%|â–Š         | 528/6250 [14:38<2:32:16,  1.60s/it]  8%|â–Š         | 528/6250 [14:38<2:32:14,  1.60s/it]  8%|â–Š         | 529/6250 [14:39<2:32:14,  1.60s/it]  8%|â–Š         | 529/6250 [14:39<2:32:12,  1.60s/it]  8%|â–Š         | 530/6250 [14:41<2:31:57,  1.59s/it]  8%|â–Š         | 530/6250 [14:41<2:31:58,  1.59s/it]  8%|â–Š         | 531/6250 [14:43<2:32:02,  1.60s/it]  8%|â–Š         | 531/6250 [14:43<2:32:03,  1.60s/it]  9%|â–Š         | 532/6250 [14:44<2:32:04,  1.60s/it]  9%|â–Š         | 532/6250 [14:44<2:32:02,  1.60s/it]  9%|â–Š         | 533/6250 [14:46<2:31:54,  1.59s/it]  9%|â–Š         | 533/6250 [14:46<2:31:58,  1.59s/it]  9%|â–Š         | 534/6250 [14:47<2:31:46,  1.59s/it]  9%|â–Š         | 534/6250 [14:47<2:31:46,  1.59s/it]  9%|â–Š         | 535/6250 [14:49<2:31:51,  1.59s/it]  9%|â–Š         | 535/6250 [14:49<2:31:54,  1.59s/it]  9%|â–Š         | 536/6250 [14:51<2:31:56,  1.60s/it]  9%|â–Š         | 536/6250 [14:51<2:31:53,  1.59s/it]  9%|â–Š         | 537/6250 [14:52<2:32:17,  1.60s/it]  9%|â–Š         | 537/6250 [14:52<2:32:15,  1.60s/it]  9%|â–Š         | 538/6250 [14:54<2:32:03,  1.60s/it]  9%|â–Š         | 538/6250 [14:54<2:32:07,  1.60s/it]  9%|â–Š         | 539/6250 [14:55<2:32:06,  1.60s/it]  9%|â–Š         | 539/6250 [14:55<2:32:05,  1.60s/it]2025-08-19 11:20:01,956 - PixArt - INFO - Step/Epoch [540/1][540/6250]:total_eta: 11:16:15, epoch_eta:2:37:52, time_all:1.597, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:2.0036, grad_norm:37.9698
  9%|â–Š         | 540/6250 [14:57<2:32:00,  1.60s/it]  9%|â–Š         | 540/6250 [14:57<2:31:59,  1.60s/it]  9%|â–Š         | 541/6250 [14:59<2:31:51,  1.60s/it]  9%|â–Š         | 541/6250 [14:59<2:31:51,  1.60s/it]  9%|â–Š         | 542/6250 [15:00<2:31:35,  1.59s/it]  9%|â–Š         | 542/6250 [15:00<2:31:38,  1.59s/it]  9%|â–Š         | 543/6250 [15:02<2:31:35,  1.59s/it]  9%|â–Š         | 543/6250 [15:02<2:31:32,  1.59s/it]  9%|â–Š         | 544/6250 [15:03<2:31:35,  1.59s/it]  9%|â–Š         | 544/6250 [15:03<2:31:35,  1.59s/it]  9%|â–Š         | 545/6250 [15:05<2:31:44,  1.60s/it]  9%|â–Š         | 545/6250 [15:05<2:31:45,  1.60s/it]  9%|â–Š         | 546/6250 [15:07<2:31:43,  1.60s/it]  9%|â–Š         | 546/6250 [15:07<2:31:39,  1.60s/it]  9%|â–‰         | 547/6250 [15:08<2:31:26,  1.59s/it]  9%|â–‰         | 547/6250 [15:08<2:31:26,  1.59s/it]  9%|â–‰         | 548/6250 [15:10<2:31:24,  1.59s/it]  9%|â–‰         | 548/6250 [15:10<2:31:22,  1.59s/it]  9%|â–‰         | 549/6250 [15:11<2:31:13,  1.59s/it]  9%|â–‰         | 549/6250 [15:11<2:31:13,  1.59s/it]  9%|â–‰         | 550/6250 [15:13<2:31:26,  1.59s/it]  9%|â–‰         | 550/6250 [15:13<2:31:28,  1.59s/it]  9%|â–‰         | 551/6250 [15:15<2:31:30,  1.60s/it]  9%|â–‰         | 551/6250 [15:15<2:31:31,  1.60s/it]  9%|â–‰         | 552/6250 [15:16<2:31:26,  1.59s/it]  9%|â–‰         | 552/6250 [15:16<2:31:24,  1.59s/it]  9%|â–‰         | 553/6250 [15:18<2:31:14,  1.59s/it]  9%|â–‰         | 553/6250 [15:18<2:31:15,  1.59s/it]  9%|â–‰         | 554/6250 [15:19<2:31:08,  1.59s/it]  9%|â–‰         | 554/6250 [15:19<2:31:06,  1.59s/it]  9%|â–‰         | 555/6250 [15:21<2:31:03,  1.59s/it]  9%|â–‰         | 555/6250 [15:21<2:31:03,  1.59s/it]  9%|â–‰         | 556/6250 [15:22<2:31:01,  1.59s/it]  9%|â–‰         | 556/6250 [15:22<2:31:01,  1.59s/it]  9%|â–‰         | 557/6250 [15:24<2:31:16,  1.59s/it]  9%|â–‰         | 557/6250 [15:24<2:31:16,  1.59s/it]  9%|â–‰         | 558/6250 [15:26<2:31:18,  1.60s/it]  9%|â–‰         | 558/6250 [15:26<2:31:18,  1.59s/it]  9%|â–‰         | 559/6250 [15:27<2:31:19,  1.60s/it]  9%|â–‰         | 559/6250 [15:27<2:31:18,  1.60s/it]  9%|â–‰         | 560/6250 [15:29<2:31:19,  1.60s/it]2025-08-19 11:20:33,833 - PixArt - INFO - Step/Epoch [560/1][560/6250]:total_eta: 11:14:45, epoch_eta:2:37:06, time_all:1.594, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7397, grad_norm:22.6749
  9%|â–‰         | 560/6250 [15:29<2:31:23,  1.60s/it]  9%|â–‰         | 561/6250 [15:30<2:31:21,  1.60s/it]  9%|â–‰         | 561/6250 [15:30<2:31:22,  1.60s/it]  9%|â–‰         | 562/6250 [15:32<2:31:32,  1.60s/it]  9%|â–‰         | 562/6250 [15:32<2:31:31,  1.60s/it]  9%|â–‰         | 563/6250 [15:34<2:31:29,  1.60s/it]  9%|â–‰         | 563/6250 [15:34<2:31:31,  1.60s/it]  9%|â–‰         | 564/6250 [15:35<2:31:26,  1.60s/it]  9%|â–‰         | 564/6250 [15:35<2:31:24,  1.60s/it]  9%|â–‰         | 565/6250 [15:37<2:31:07,  1.60s/it]  9%|â–‰         | 565/6250 [15:37<2:31:06,  1.59s/it]  9%|â–‰         | 566/6250 [15:38<2:31:07,  1.60s/it]  9%|â–‰         | 566/6250 [15:38<2:31:07,  1.60s/it]  9%|â–‰         | 567/6250 [15:40<2:30:49,  1.59s/it]  9%|â–‰         | 567/6250 [15:40<2:30:49,  1.59s/it]  9%|â–‰         | 568/6250 [15:42<2:30:51,  1.59s/it]  9%|â–‰         | 568/6250 [15:42<2:30:55,  1.59s/it]  9%|â–‰         | 569/6250 [15:43<2:30:58,  1.59s/it]  9%|â–‰         | 569/6250 [15:43<2:30:56,  1.59s/it]  9%|â–‰         | 570/6250 [15:45<2:31:00,  1.60s/it]  9%|â–‰         | 570/6250 [15:45<2:30:59,  1.59s/it]  9%|â–‰         | 571/6250 [15:46<2:30:58,  1.60s/it]  9%|â–‰         | 571/6250 [15:46<2:30:56,  1.59s/it]  9%|â–‰         | 572/6250 [15:48<2:30:47,  1.59s/it]  9%|â–‰         | 572/6250 [15:48<2:30:46,  1.59s/it]  9%|â–‰         | 573/6250 [15:50<2:30:52,  1.59s/it]  9%|â–‰         | 573/6250 [15:50<2:30:54,  1.60s/it]  9%|â–‰         | 574/6250 [15:51<2:30:57,  1.60s/it]  9%|â–‰         | 574/6250 [15:51<2:30:56,  1.60s/it]  9%|â–‰         | 575/6250 [15:53<2:31:00,  1.60s/it]  9%|â–‰         | 575/6250 [15:53<2:31:00,  1.60s/it]  9%|â–‰         | 576/6250 [15:54<2:30:55,  1.60s/it]  9%|â–‰         | 576/6250 [15:54<2:30:56,  1.60s/it]  9%|â–‰         | 577/6250 [15:56<2:30:58,  1.60s/it]  9%|â–‰         | 577/6250 [15:56<2:30:57,  1.60s/it]  9%|â–‰         | 578/6250 [15:58<2:30:57,  1.60s/it]  9%|â–‰         | 578/6250 [15:58<2:30:57,  1.60s/it]  9%|â–‰         | 579/6250 [15:59<2:30:58,  1.60s/it]  9%|â–‰         | 579/6250 [15:59<2:30:59,  1.60s/it]  9%|â–‰         | 580/6250 [16:01<2:30:46,  1.60s/it]2025-08-19 11:21:05,746 - PixArt - INFO - Step/Epoch [580/1][580/6250]:total_eta: 11:13:21, epoch_eta:2:36:20, time_all:1.596, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7687, grad_norm:30.9032
  9%|â–‰         | 580/6250 [16:01<2:30:46,  1.60s/it]  9%|â–‰         | 581/6250 [16:02<2:30:45,  1.60s/it]  9%|â–‰         | 581/6250 [16:02<2:30:45,  1.60s/it]  9%|â–‰         | 582/6250 [16:04<2:30:34,  1.59s/it]  9%|â–‰         | 582/6250 [16:04<2:30:34,  1.59s/it]  9%|â–‰         | 583/6250 [16:06<2:30:41,  1.60s/it]  9%|â–‰         | 583/6250 [16:06<2:35:01,  1.64s/it]  9%|â–‰         | 584/6250 [16:07<2:35:22,  1.65s/it]  9%|â–‰         | 584/6250 [16:07<2:34:04,  1.63s/it]  9%|â–‰         | 585/6250 [16:09<2:34:07,  1.63s/it]  9%|â–‰         | 585/6250 [16:09<2:33:18,  1.62s/it]  9%|â–‰         | 586/6250 [16:11<2:32:53,  1.62s/it]  9%|â–‰         | 586/6250 [16:11<2:32:23,  1.61s/it]  9%|â–‰         | 587/6250 [16:12<2:32:22,  1.61s/it]  9%|â–‰         | 587/6250 [16:12<2:31:52,  1.61s/it]  9%|â–‰         | 588/6250 [16:14<2:31:50,  1.61s/it]  9%|â–‰         | 588/6250 [16:14<2:31:29,  1.61s/it]  9%|â–‰         | 589/6250 [16:15<2:31:28,  1.61s/it]  9%|â–‰         | 589/6250 [16:15<2:31:12,  1.60s/it]  9%|â–‰         | 590/6250 [16:17<2:31:05,  1.60s/it]  9%|â–‰         | 590/6250 [16:17<2:30:57,  1.60s/it]  9%|â–‰         | 591/6250 [16:18<2:30:52,  1.60s/it]  9%|â–‰         | 591/6250 [16:18<2:30:42,  1.60s/it]  9%|â–‰         | 592/6250 [16:20<2:30:33,  1.60s/it]  9%|â–‰         | 592/6250 [16:20<2:30:27,  1.60s/it]  9%|â–‰         | 593/6250 [16:22<2:30:27,  1.60s/it]  9%|â–‰         | 593/6250 [16:22<2:30:26,  1.60s/it] 10%|â–‰         | 594/6250 [16:23<2:30:29,  1.60s/it] 10%|â–‰         | 594/6250 [16:23<2:30:28,  1.60s/it] 10%|â–‰         | 595/6250 [16:25<2:30:31,  1.60s/it] 10%|â–‰         | 595/6250 [16:25<2:30:28,  1.60s/it] 10%|â–‰         | 596/6250 [16:26<2:30:19,  1.60s/it] 10%|â–‰         | 596/6250 [16:26<2:30:18,  1.60s/it] 10%|â–‰         | 597/6250 [16:28<2:30:14,  1.59s/it] 10%|â–‰         | 597/6250 [16:28<2:30:13,  1.59s/it] 10%|â–‰         | 598/6250 [16:30<2:30:08,  1.59s/it] 10%|â–‰         | 598/6250 [16:30<2:30:06,  1.59s/it] 10%|â–‰         | 599/6250 [16:31<2:30:10,  1.59s/it] 10%|â–‰         | 599/6250 [16:31<2:30:13,  1.59s/it] 10%|â–‰         | 600/6250 [16:33<2:30:19,  1.60s/it]2025-08-19 11:21:37,822 - PixArt - INFO - Step/Epoch [600/1][600/6250]:total_eta: 11:12:06, epoch_eta:2:35:38, time_all:1.604, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7486, grad_norm:51.1225
 10%|â–‰         | 600/6250 [16:33<2:30:23,  1.60s/it] 10%|â–‰         | 601/6250 [16:34<2:30:19,  1.60s/it] 10%|â–‰         | 601/6250 [16:34<2:30:15,  1.60s/it] 10%|â–‰         | 602/6250 [16:36<2:30:10,  1.60s/it] 10%|â–‰         | 602/6250 [16:36<2:30:07,  1.59s/it] 10%|â–‰         | 603/6250 [16:38<2:29:58,  1.59s/it] 10%|â–‰         | 603/6250 [16:38<2:29:56,  1.59s/it] 10%|â–‰         | 604/6250 [16:39<2:30:16,  1.60s/it] 10%|â–‰         | 604/6250 [16:39<2:30:16,  1.60s/it] 10%|â–‰         | 605/6250 [16:41<2:30:15,  1.60s/it] 10%|â–‰         | 605/6250 [16:41<2:30:15,  1.60s/it] 10%|â–‰         | 606/6250 [16:42<2:30:07,  1.60s/it] 10%|â–‰         | 606/6250 [16:42<2:30:05,  1.60s/it] 10%|â–‰         | 607/6250 [16:44<2:30:16,  1.60s/it] 10%|â–‰         | 607/6250 [16:44<2:30:17,  1.60s/it] 10%|â–‰         | 608/6250 [16:46<2:29:59,  1.60s/it] 10%|â–‰         | 608/6250 [16:46<2:29:59,  1.60s/it] 10%|â–‰         | 609/6250 [16:47<2:31:11,  1.61s/it] 10%|â–‰         | 609/6250 [16:47<2:31:12,  1.61s/it] 10%|â–‰         | 610/6250 [16:49<2:32:41,  1.62s/it] 10%|â–‰         | 610/6250 [16:49<2:32:42,  1.62s/it] 10%|â–‰         | 611/6250 [16:51<2:32:15,  1.62s/it] 10%|â–‰         | 611/6250 [16:51<2:32:14,  1.62s/it] 10%|â–‰         | 612/6250 [16:52<2:31:38,  1.61s/it] 10%|â–‰         | 612/6250 [16:52<2:31:36,  1.61s/it] 10%|â–‰         | 613/6250 [16:54<2:31:14,  1.61s/it] 10%|â–‰         | 613/6250 [16:54<2:31:24,  1.61s/it] 10%|â–‰         | 614/6250 [16:55<2:31:02,  1.61s/it] 10%|â–‰         | 614/6250 [16:55<2:30:58,  1.61s/it] 10%|â–‰         | 615/6250 [16:57<2:30:42,  1.60s/it] 10%|â–‰         | 615/6250 [16:57<2:30:40,  1.60s/it] 10%|â–‰         | 616/6250 [16:59<2:30:19,  1.60s/it] 10%|â–‰         | 616/6250 [16:59<2:30:20,  1.60s/it] 10%|â–‰         | 617/6250 [17:00<2:30:11,  1.60s/it] 10%|â–‰         | 617/6250 [17:00<2:30:09,  1.60s/it] 10%|â–‰         | 618/6250 [17:02<2:30:08,  1.60s/it] 10%|â–‰         | 618/6250 [17:02<2:30:06,  1.60s/it] 10%|â–‰         | 619/6250 [17:03<2:30:07,  1.60s/it] 10%|â–‰         | 619/6250 [17:03<2:30:05,  1.60s/it] 10%|â–‰         | 620/6250 [17:05<2:30:39,  1.61s/it]2025-08-19 11:22:09,905 - PixArt - INFO - Step/Epoch [620/1][620/6250]:total_eta: 11:10:55, epoch_eta:2:34:56, time_all:1.604, time_data:0.003, lr:5.000e-06, s:(32, 32), loss:1.9245, grad_norm:54.5721
 10%|â–‰         | 620/6250 [17:05<2:30:40,  1.61s/it] 10%|â–‰         | 621/6250 [17:07<2:30:26,  1.60s/it] 10%|â–‰         | 621/6250 [17:07<2:30:29,  1.60s/it] 10%|â–‰         | 622/6250 [17:08<2:31:57,  1.62s/it] 10%|â–‰         | 622/6250 [17:08<2:31:55,  1.62s/it] 10%|â–‰         | 623/6250 [17:10<2:31:10,  1.61s/it] 10%|â–‰         | 623/6250 [17:10<2:31:10,  1.61s/it] 10%|â–‰         | 624/6250 [17:11<2:30:54,  1.61s/it] 10%|â–‰         | 624/6250 [17:11<2:30:53,  1.61s/it] 10%|â–ˆ         | 625/6250 [17:13<2:30:26,  1.60s/it] 10%|â–ˆ         | 625/6250 [17:13<2:30:26,  1.60s/it] 10%|â–ˆ         | 626/6250 [17:15<2:30:11,  1.60s/it] 10%|â–ˆ         | 626/6250 [17:15<2:30:12,  1.60s/it] 10%|â–ˆ         | 627/6250 [17:16<2:29:59,  1.60s/it] 10%|â–ˆ         | 627/6250 [17:16<2:30:01,  1.60s/it] 10%|â–ˆ         | 628/6250 [17:18<2:29:42,  1.60s/it] 10%|â–ˆ         | 628/6250 [17:18<2:29:42,  1.60s/it] 10%|â–ˆ         | 629/6250 [17:19<2:29:44,  1.60s/it] 10%|â–ˆ         | 629/6250 [17:19<2:29:53,  1.60s/it] 10%|â–ˆ         | 630/6250 [17:21<2:29:40,  1.60s/it] 10%|â–ˆ         | 630/6250 [17:21<2:29:36,  1.60s/it] 10%|â–ˆ         | 631/6250 [17:23<2:29:31,  1.60s/it] 10%|â–ˆ         | 631/6250 [17:23<2:29:29,  1.60s/it] 10%|â–ˆ         | 632/6250 [17:24<2:29:18,  1.59s/it] 10%|â–ˆ         | 632/6250 [17:24<2:29:16,  1.59s/it] 10%|â–ˆ         | 633/6250 [17:26<2:29:14,  1.59s/it] 10%|â–ˆ         | 633/6250 [17:26<2:29:13,  1.59s/it] 10%|â–ˆ         | 634/6250 [17:27<2:29:21,  1.60s/it] 10%|â–ˆ         | 634/6250 [17:27<2:29:24,  1.60s/it] 10%|â–ˆ         | 635/6250 [17:29<2:29:24,  1.60s/it] 10%|â–ˆ         | 635/6250 [17:29<2:29:21,  1.60s/it] 10%|â–ˆ         | 636/6250 [17:31<2:29:24,  1.60s/it] 10%|â–ˆ         | 636/6250 [17:31<2:29:23,  1.60s/it] 10%|â–ˆ         | 637/6250 [17:32<2:29:22,  1.60s/it] 10%|â–ˆ         | 637/6250 [17:32<2:29:21,  1.60s/it] 10%|â–ˆ         | 638/6250 [17:34<2:29:18,  1.60s/it] 10%|â–ˆ         | 638/6250 [17:34<2:29:28,  1.60s/it] 10%|â–ˆ         | 639/6250 [17:35<2:29:29,  1.60s/it] 10%|â–ˆ         | 639/6250 [17:35<2:29:26,  1.60s/it] 10%|â–ˆ         | 640/6250 [17:37<2:29:23,  1.60s/it]2025-08-19 11:22:41,898 - PixArt - INFO - Step/Epoch [640/1][640/6250]:total_eta: 11:09:43, epoch_eta:2:34:14, time_all:1.600, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7270, grad_norm:33.2567
 10%|â–ˆ         | 640/6250 [17:37<2:29:21,  1.60s/it] 10%|â–ˆ         | 641/6250 [17:39<2:29:12,  1.60s/it] 10%|â–ˆ         | 641/6250 [17:39<2:29:10,  1.60s/it] 10%|â–ˆ         | 642/6250 [17:40<2:31:07,  1.62s/it] 10%|â–ˆ         | 642/6250 [17:40<2:31:08,  1.62s/it] 10%|â–ˆ         | 643/6250 [17:42<2:30:28,  1.61s/it] 10%|â–ˆ         | 643/6250 [17:42<2:30:29,  1.61s/it] 10%|â–ˆ         | 644/6250 [17:43<2:29:59,  1.61s/it] 10%|â–ˆ         | 644/6250 [17:43<2:29:56,  1.60s/it] 10%|â–ˆ         | 645/6250 [17:45<2:29:40,  1.60s/it] 10%|â–ˆ         | 645/6250 [17:45<2:29:44,  1.60s/it] 10%|â–ˆ         | 646/6250 [17:47<2:29:30,  1.60s/it] 10%|â–ˆ         | 646/6250 [17:47<2:29:28,  1.60s/it] 10%|â–ˆ         | 647/6250 [17:48<2:29:20,  1.60s/it] 10%|â–ˆ         | 647/6250 [17:48<2:29:18,  1.60s/it] 10%|â–ˆ         | 648/6250 [17:50<2:29:13,  1.60s/it] 10%|â–ˆ         | 648/6250 [17:50<2:29:13,  1.60s/it] 10%|â–ˆ         | 649/6250 [17:51<2:29:11,  1.60s/it] 10%|â–ˆ         | 649/6250 [17:51<2:29:11,  1.60s/it] 10%|â–ˆ         | 650/6250 [17:53<2:29:01,  1.60s/it] 10%|â–ˆ         | 650/6250 [17:53<2:28:58,  1.60s/it] 10%|â–ˆ         | 651/6250 [17:55<2:28:48,  1.59s/it] 10%|â–ˆ         | 651/6250 [17:55<2:28:52,  1.60s/it] 10%|â–ˆ         | 652/6250 [17:56<2:28:38,  1.59s/it] 10%|â–ˆ         | 652/6250 [17:56<2:28:38,  1.59s/it] 10%|â–ˆ         | 653/6250 [17:58<2:28:38,  1.59s/it] 10%|â–ˆ         | 653/6250 [17:58<2:28:37,  1.59s/it] 10%|â–ˆ         | 654/6250 [17:59<2:28:34,  1.59s/it] 10%|â–ˆ         | 654/6250 [17:59<2:28:40,  1.59s/it] 10%|â–ˆ         | 655/6250 [18:01<2:28:46,  1.60s/it] 10%|â–ˆ         | 655/6250 [18:01<2:28:42,  1.59s/it] 10%|â–ˆ         | 656/6250 [18:03<2:28:50,  1.60s/it] 10%|â–ˆ         | 656/6250 [18:03<2:28:50,  1.60s/it] 11%|â–ˆ         | 657/6250 [18:04<2:28:51,  1.60s/it] 11%|â–ˆ         | 657/6250 [18:04<2:28:48,  1.60s/it] 11%|â–ˆ         | 658/6250 [18:06<2:29:24,  1.60s/it] 11%|â–ˆ         | 658/6250 [18:06<2:29:24,  1.60s/it] 11%|â–ˆ         | 659/6250 [18:07<2:29:13,  1.60s/it] 11%|â–ˆ         | 659/6250 [18:07<2:29:13,  1.60s/it] 11%|â–ˆ         | 660/6250 [18:09<2:29:01,  1.60s/it]2025-08-19 11:23:13,893 - PixArt - INFO - Step/Epoch [660/1][660/6250]:total_eta: 11:08:33, epoch_eta:2:33:32, time_all:1.600, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.8448, grad_norm:52.9479
 11%|â–ˆ         | 660/6250 [18:09<2:28:59,  1.60s/it] 11%|â–ˆ         | 661/6250 [18:11<2:29:01,  1.60s/it] 11%|â–ˆ         | 661/6250 [18:11<2:29:07,  1.60s/it] 11%|â–ˆ         | 662/6250 [18:12<2:29:05,  1.60s/it] 11%|â–ˆ         | 662/6250 [18:12<2:29:02,  1.60s/it] 11%|â–ˆ         | 663/6250 [18:14<2:29:02,  1.60s/it] 11%|â–ˆ         | 663/6250 [18:14<2:28:59,  1.60s/it] 11%|â–ˆ         | 664/6250 [18:15<2:28:53,  1.60s/it] 11%|â–ˆ         | 664/6250 [18:15<2:28:52,  1.60s/it] 11%|â–ˆ         | 665/6250 [18:17<2:28:45,  1.60s/it] 11%|â–ˆ         | 665/6250 [18:17<2:28:45,  1.60s/it] 11%|â–ˆ         | 666/6250 [18:18<2:28:32,  1.60s/it] 11%|â–ˆ         | 666/6250 [18:19<2:28:31,  1.60s/it] 11%|â–ˆ         | 667/6250 [18:20<2:28:30,  1.60s/it] 11%|â–ˆ         | 667/6250 [18:20<2:28:30,  1.60s/it] 11%|â–ˆ         | 668/6250 [18:22<2:28:17,  1.59s/it] 11%|â–ˆ         | 668/6250 [18:22<2:28:18,  1.59s/it] 11%|â–ˆ         | 669/6250 [18:23<2:28:08,  1.59s/it] 11%|â–ˆ         | 669/6250 [18:23<2:28:07,  1.59s/it] 11%|â–ˆ         | 670/6250 [18:25<2:28:07,  1.59s/it] 11%|â–ˆ         | 670/6250 [18:25<2:28:07,  1.59s/it] 11%|â–ˆ         | 671/6250 [18:26<2:28:20,  1.60s/it] 11%|â–ˆ         | 671/6250 [18:26<2:28:19,  1.60s/it] 11%|â–ˆ         | 672/6250 [18:28<2:28:34,  1.60s/it] 11%|â–ˆ         | 672/6250 [18:28<2:28:35,  1.60s/it] 11%|â–ˆ         | 673/6250 [18:30<2:28:32,  1.60s/it] 11%|â–ˆ         | 673/6250 [18:30<2:28:33,  1.60s/it] 11%|â–ˆ         | 674/6250 [18:31<2:28:31,  1.60s/it] 11%|â–ˆ         | 674/6250 [18:31<2:28:29,  1.60s/it] 11%|â–ˆ         | 675/6250 [18:33<2:28:22,  1.60s/it] 11%|â–ˆ         | 675/6250 [18:33<2:28:22,  1.60s/it] 11%|â–ˆ         | 676/6250 [18:34<2:28:21,  1.60s/it] 11%|â–ˆ         | 676/6250 [18:34<2:28:22,  1.60s/it] 11%|â–ˆ         | 677/6250 [18:36<2:28:22,  1.60s/it] 11%|â–ˆ         | 677/6250 [18:36<2:28:21,  1.60s/it] 11%|â–ˆ         | 678/6250 [18:38<2:28:07,  1.59s/it] 11%|â–ˆ         | 678/6250 [18:38<2:28:07,  1.59s/it] 11%|â–ˆ         | 679/6250 [18:39<2:28:13,  1.60s/it] 11%|â–ˆ         | 679/6250 [18:39<2:28:17,  1.60s/it] 11%|â–ˆ         | 680/6250 [18:41<2:28:17,  1.60s/it]2025-08-19 11:23:45,827 - PixArt - INFO - Step/Epoch [680/1][680/6250]:total_eta: 11:07:23, epoch_eta:2:32:51, time_all:1.597, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6395, grad_norm:25.2648
 11%|â–ˆ         | 680/6250 [18:41<2:28:18,  1.60s/it] 11%|â–ˆ         | 681/6250 [18:42<2:28:22,  1.60s/it] 11%|â–ˆ         | 681/6250 [18:42<2:28:21,  1.60s/it] 11%|â–ˆ         | 682/6250 [18:44<2:28:19,  1.60s/it] 11%|â–ˆ         | 682/6250 [18:44<2:28:16,  1.60s/it] 11%|â–ˆ         | 683/6250 [18:46<2:28:08,  1.60s/it] 11%|â–ˆ         | 683/6250 [18:46<2:28:12,  1.60s/it] 11%|â–ˆ         | 684/6250 [18:47<2:27:59,  1.60s/it] 11%|â–ˆ         | 684/6250 [18:47<2:27:58,  1.60s/it] 11%|â–ˆ         | 685/6250 [18:49<2:29:32,  1.61s/it] 11%|â–ˆ         | 685/6250 [18:49<2:29:34,  1.61s/it] 11%|â–ˆ         | 686/6250 [18:50<2:28:58,  1.61s/it] 11%|â–ˆ         | 686/6250 [18:50<2:28:55,  1.61s/it] 11%|â–ˆ         | 687/6250 [18:52<2:28:37,  1.60s/it] 11%|â–ˆ         | 687/6250 [18:52<2:28:36,  1.60s/it] 11%|â–ˆ         | 688/6250 [18:54<2:28:10,  1.60s/it] 11%|â–ˆ         | 688/6250 [18:54<2:28:08,  1.60s/it] 11%|â–ˆ         | 689/6250 [18:55<2:28:13,  1.60s/it] 11%|â–ˆ         | 689/6250 [18:55<2:28:12,  1.60s/it] 11%|â–ˆ         | 690/6250 [18:57<2:28:10,  1.60s/it] 11%|â–ˆ         | 690/6250 [18:57<2:28:09,  1.60s/it] 11%|â–ˆ         | 691/6250 [18:58<2:27:55,  1.60s/it] 11%|â–ˆ         | 691/6250 [18:58<2:27:58,  1.60s/it] 11%|â–ˆ         | 692/6250 [19:00<2:27:54,  1.60s/it] 11%|â–ˆ         | 692/6250 [19:00<2:27:54,  1.60s/it] 11%|â–ˆ         | 693/6250 [19:02<2:27:39,  1.59s/it] 11%|â–ˆ         | 693/6250 [19:02<2:27:41,  1.59s/it] 11%|â–ˆ         | 694/6250 [19:03<2:27:42,  1.60s/it] 11%|â–ˆ         | 694/6250 [19:03<2:27:42,  1.60s/it] 11%|â–ˆ         | 695/6250 [19:05<2:27:42,  1.60s/it] 11%|â–ˆ         | 695/6250 [19:05<2:27:42,  1.60s/it] 11%|â–ˆ         | 696/6250 [19:06<2:27:46,  1.60s/it] 11%|â–ˆ         | 696/6250 [19:06<2:27:43,  1.60s/it] 11%|â–ˆ         | 697/6250 [19:08<2:27:36,  1.59s/it] 11%|â–ˆ         | 697/6250 [19:08<2:27:35,  1.59s/it] 11%|â–ˆ         | 698/6250 [19:10<2:27:32,  1.59s/it] 11%|â–ˆ         | 698/6250 [19:10<2:27:31,  1.59s/it] 11%|â–ˆ         | 699/6250 [19:11<2:27:32,  1.59s/it] 11%|â–ˆ         | 699/6250 [19:11<2:27:34,  1.60s/it]2025-08-19 11:24:17,781 - PixArt - INFO - Step/Epoch [700/1][700/6250]:total_eta: 11:06:17, epoch_eta:2:32:10, time_all:1.598, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7550, grad_norm:32.5983
 11%|â–ˆ         | 700/6250 [19:13<2:27:29,  1.59s/it] 11%|â–ˆ         | 700/6250 [19:13<2:27:27,  1.59s/it] 11%|â–ˆ         | 701/6250 [19:14<2:29:29,  1.62s/it] 11%|â–ˆ         | 701/6250 [19:14<2:29:30,  1.62s/it] 11%|â–ˆ         | 702/6250 [19:16<2:28:42,  1.61s/it] 11%|â–ˆ         | 702/6250 [19:16<2:28:43,  1.61s/it] 11%|â–ˆ         | 703/6250 [19:18<2:28:16,  1.60s/it] 11%|â–ˆ         | 703/6250 [19:18<2:28:15,  1.60s/it] 11%|â–ˆâ–        | 704/6250 [19:19<2:28:00,  1.60s/it] 11%|â–ˆâ–        | 704/6250 [19:19<2:28:00,  1.60s/it] 11%|â–ˆâ–        | 705/6250 [19:21<2:27:36,  1.60s/it] 11%|â–ˆâ–        | 705/6250 [19:21<2:27:34,  1.60s/it] 11%|â–ˆâ–        | 706/6250 [19:22<2:27:35,  1.60s/it] 11%|â–ˆâ–        | 706/6250 [19:22<2:27:35,  1.60s/it] 11%|â–ˆâ–        | 707/6250 [19:24<2:27:15,  1.59s/it] 11%|â–ˆâ–        | 707/6250 [19:24<2:27:13,  1.59s/it] 11%|â–ˆâ–        | 708/6250 [19:26<2:27:10,  1.59s/it] 11%|â–ˆâ–        | 708/6250 [19:26<2:27:13,  1.59s/it] 11%|â–ˆâ–        | 709/6250 [19:27<2:27:23,  1.60s/it] 11%|â–ˆâ–        | 709/6250 [19:27<2:27:22,  1.60s/it] 11%|â–ˆâ–        | 710/6250 [19:29<2:27:22,  1.60s/it] 11%|â–ˆâ–        | 710/6250 [19:29<2:27:22,  1.60s/it] 11%|â–ˆâ–        | 711/6250 [19:30<2:27:12,  1.59s/it] 11%|â–ˆâ–        | 711/6250 [19:30<2:27:10,  1.59s/it] 11%|â–ˆâ–        | 712/6250 [19:32<2:27:08,  1.59s/it] 11%|â–ˆâ–        | 712/6250 [19:32<2:27:08,  1.59s/it] 11%|â–ˆâ–        | 713/6250 [19:34<2:27:01,  1.59s/it] 11%|â–ˆâ–        | 713/6250 [19:34<2:27:01,  1.59s/it] 11%|â–ˆâ–        | 714/6250 [19:35<2:27:09,  1.60s/it] 11%|â–ˆâ–        | 714/6250 [19:35<2:27:09,  1.59s/it] 11%|â–ˆâ–        | 715/6250 [19:37<2:27:05,  1.59s/it] 11%|â–ˆâ–        | 715/6250 [19:37<2:27:06,  1.59s/it] 11%|â–ˆâ–        | 716/6250 [19:38<2:27:04,  1.59s/it] 11%|â–ˆâ–        | 716/6250 [19:38<2:27:04,  1.59s/it] 11%|â–ˆâ–        | 717/6250 [19:40<2:26:50,  1.59s/it] 11%|â–ˆâ–        | 717/6250 [19:40<2:26:51,  1.59s/it] 11%|â–ˆâ–        | 718/6250 [19:42<2:26:52,  1.59s/it] 11%|â–ˆâ–        | 718/6250 [19:42<2:26:49,  1.59s/it] 12%|â–ˆâ–        | 719/6250 [19:43<2:27:10,  1.60s/it] 12%|â–ˆâ–        | 719/6250 [19:43<2:27:11,  1.60s/it]2025-08-19 11:24:49,731 - PixArt - INFO - Step/Epoch [720/1][720/6250]:total_eta: 11:05:12, epoch_eta:2:31:30, time_all:1.597, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7465, grad_norm:46.8469
 12%|â–ˆâ–        | 720/6250 [19:45<2:27:03,  1.60s/it] 12%|â–ˆâ–        | 720/6250 [19:45<2:27:03,  1.60s/it] 12%|â–ˆâ–        | 721/6250 [19:46<2:27:12,  1.60s/it] 12%|â–ˆâ–        | 721/6250 [19:46<2:27:13,  1.60s/it] 12%|â–ˆâ–        | 722/6250 [19:48<2:26:54,  1.59s/it] 12%|â–ˆâ–        | 722/6250 [19:48<2:26:57,  1.60s/it] 12%|â–ˆâ–        | 723/6250 [19:50<2:26:46,  1.59s/it] 12%|â–ˆâ–        | 723/6250 [19:50<2:26:46,  1.59s/it] 12%|â–ˆâ–        | 724/6250 [19:51<2:26:37,  1.59s/it] 12%|â–ˆâ–        | 724/6250 [19:51<2:26:35,  1.59s/it] 12%|â–ˆâ–        | 725/6250 [19:53<2:26:32,  1.59s/it] 12%|â–ˆâ–        | 725/6250 [19:53<2:26:33,  1.59s/it] 12%|â–ˆâ–        | 726/6250 [19:54<2:26:41,  1.59s/it] 12%|â–ˆâ–        | 726/6250 [19:54<2:26:40,  1.59s/it] 12%|â–ˆâ–        | 727/6250 [19:56<2:27:01,  1.60s/it] 12%|â–ˆâ–        | 727/6250 [19:56<2:27:01,  1.60s/it] 12%|â–ˆâ–        | 728/6250 [19:58<2:26:48,  1.60s/it] 12%|â–ˆâ–        | 728/6250 [19:58<2:26:47,  1.60s/it] 12%|â–ˆâ–        | 729/6250 [19:59<2:26:44,  1.59s/it] 12%|â–ˆâ–        | 729/6250 [19:59<2:26:50,  1.60s/it] 12%|â–ˆâ–        | 730/6250 [20:01<2:26:47,  1.60s/it] 12%|â–ˆâ–        | 730/6250 [20:01<2:26:45,  1.60s/it] 12%|â–ˆâ–        | 731/6250 [20:02<2:26:41,  1.59s/it] 12%|â–ˆâ–        | 731/6250 [20:02<2:26:43,  1.60s/it] 12%|â–ˆâ–        | 732/6250 [20:04<2:26:46,  1.60s/it] 12%|â–ˆâ–        | 732/6250 [20:04<2:26:47,  1.60s/it] 12%|â–ˆâ–        | 733/6250 [20:05<2:27:00,  1.60s/it] 12%|â–ˆâ–        | 733/6250 [20:05<2:26:56,  1.60s/it] 12%|â–ˆâ–        | 734/6250 [20:07<2:26:47,  1.60s/it] 12%|â–ˆâ–        | 734/6250 [20:07<2:26:45,  1.60s/it] 12%|â–ˆâ–        | 735/6250 [20:09<2:26:48,  1.60s/it] 12%|â–ˆâ–        | 735/6250 [20:09<2:26:47,  1.60s/it] 12%|â–ˆâ–        | 736/6250 [20:10<2:26:36,  1.60s/it] 12%|â–ˆâ–        | 736/6250 [20:10<2:26:37,  1.60s/it] 12%|â–ˆâ–        | 737/6250 [20:12<2:26:59,  1.60s/it] 12%|â–ˆâ–        | 737/6250 [20:12<2:27:01,  1.60s/it] 12%|â–ˆâ–        | 738/6250 [20:13<2:26:40,  1.60s/it] 12%|â–ˆâ–        | 738/6250 [20:13<2:26:54,  1.60s/it] 12%|â–ˆâ–        | 739/6250 [20:15<2:26:56,  1.60s/it] 12%|â–ˆâ–        | 739/6250 [20:15<2:26:52,  1.60s/it] 12%|â–ˆâ–        | 740/6250 [20:17<2:26:38,  1.60s/it]2025-08-19 11:25:21,650 - PixArt - INFO - Step/Epoch [740/1][740/6250]:total_eta: 11:04:07, epoch_eta:2:30:50, time_all:1.596, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6381, grad_norm:36.8284
 12%|â–ˆâ–        | 740/6250 [20:17<2:26:33,  1.60s/it] 12%|â–ˆâ–        | 741/6250 [20:18<2:26:31,  1.60s/it] 12%|â–ˆâ–        | 741/6250 [20:18<2:26:35,  1.60s/it] 12%|â–ˆâ–        | 742/6250 [20:20<2:26:45,  1.60s/it] 12%|â–ˆâ–        | 742/6250 [20:20<2:26:43,  1.60s/it] 12%|â–ˆâ–        | 743/6250 [20:21<2:27:01,  1.60s/it] 12%|â–ˆâ–        | 743/6250 [20:21<2:27:02,  1.60s/it] 12%|â–ˆâ–        | 744/6250 [20:23<2:26:41,  1.60s/it] 12%|â–ˆâ–        | 744/6250 [20:23<2:26:43,  1.60s/it] 12%|â–ˆâ–        | 745/6250 [20:25<2:26:49,  1.60s/it] 12%|â–ˆâ–        | 745/6250 [20:25<2:26:46,  1.60s/it] 12%|â–ˆâ–        | 746/6250 [20:26<2:26:41,  1.60s/it] 12%|â–ˆâ–        | 746/6250 [20:26<2:26:37,  1.60s/it] 12%|â–ˆâ–        | 747/6250 [20:28<2:26:36,  1.60s/it] 12%|â–ˆâ–        | 747/6250 [20:28<2:26:35,  1.60s/it] 12%|â–ˆâ–        | 748/6250 [20:29<2:26:33,  1.60s/it] 12%|â–ˆâ–        | 748/6250 [20:29<2:26:31,  1.60s/it] 12%|â–ˆâ–        | 749/6250 [20:31<2:26:29,  1.60s/it] 12%|â–ˆâ–        | 749/6250 [20:31<2:26:35,  1.60s/it] 12%|â–ˆâ–        | 750/6250 [20:33<2:26:30,  1.60s/it] 12%|â–ˆâ–        | 750/6250 [20:33<2:26:28,  1.60s/it] 12%|â–ˆâ–        | 751/6250 [20:34<2:26:31,  1.60s/it] 12%|â–ˆâ–        | 751/6250 [20:34<2:26:28,  1.60s/it] 12%|â–ˆâ–        | 752/6250 [20:36<2:26:21,  1.60s/it] 12%|â–ˆâ–        | 752/6250 [20:36<2:26:21,  1.60s/it] 12%|â–ˆâ–        | 753/6250 [20:37<2:26:11,  1.60s/it] 12%|â–ˆâ–        | 753/6250 [20:37<2:26:10,  1.60s/it] 12%|â–ˆâ–        | 754/6250 [20:39<2:26:11,  1.60s/it] 12%|â–ˆâ–        | 754/6250 [20:39<2:26:13,  1.60s/it] 12%|â–ˆâ–        | 755/6250 [20:41<2:26:08,  1.60s/it] 12%|â–ˆâ–        | 755/6250 [20:41<2:26:07,  1.60s/it] 12%|â–ˆâ–        | 756/6250 [20:42<2:26:02,  1.59s/it] 12%|â–ˆâ–        | 756/6250 [20:42<2:26:00,  1.59s/it] 12%|â–ˆâ–        | 757/6250 [20:44<2:26:07,  1.60s/it] 12%|â–ˆâ–        | 757/6250 [20:44<2:26:06,  1.60s/it] 12%|â–ˆâ–        | 758/6250 [20:45<2:25:55,  1.59s/it] 12%|â–ˆâ–        | 758/6250 [20:45<2:25:52,  1.59s/it] 12%|â–ˆâ–        | 759/6250 [20:47<2:25:57,  1.59s/it] 12%|â–ˆâ–        | 759/6250 [20:47<2:25:56,  1.59s/it]2025-08-19 11:25:53,588 - PixArt - INFO - Step/Epoch [760/1][760/6250]:total_eta: 11:03:05, epoch_eta:2:30:11, time_all:1.597, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6114, grad_norm:27.6397
 12%|â–ˆâ–        | 760/6250 [20:49<2:25:51,  1.59s/it] 12%|â–ˆâ–        | 760/6250 [20:49<2:25:52,  1.59s/it] 12%|â–ˆâ–        | 761/6250 [20:50<2:25:54,  1.59s/it] 12%|â–ˆâ–        | 761/6250 [20:50<2:25:52,  1.59s/it] 12%|â–ˆâ–        | 762/6250 [20:52<2:25:54,  1.60s/it] 12%|â–ˆâ–        | 762/6250 [20:52<2:25:55,  1.60s/it] 12%|â–ˆâ–        | 763/6250 [20:53<2:26:01,  1.60s/it] 12%|â–ˆâ–        | 763/6250 [20:53<2:26:02,  1.60s/it] 12%|â–ˆâ–        | 764/6250 [20:55<2:26:57,  1.61s/it] 12%|â–ˆâ–        | 764/6250 [20:55<2:26:57,  1.61s/it] 12%|â–ˆâ–        | 765/6250 [20:57<2:26:29,  1.60s/it] 12%|â–ˆâ–        | 765/6250 [20:57<2:26:32,  1.60s/it] 12%|â–ˆâ–        | 766/6250 [20:58<2:26:15,  1.60s/it] 12%|â–ˆâ–        | 766/6250 [20:58<2:26:15,  1.60s/it] 12%|â–ˆâ–        | 767/6250 [21:00<2:26:03,  1.60s/it] 12%|â–ˆâ–        | 767/6250 [21:00<2:26:01,  1.60s/it] 12%|â–ˆâ–        | 768/6250 [21:01<2:25:56,  1.60s/it] 12%|â–ˆâ–        | 768/6250 [21:01<2:25:58,  1.60s/it] 12%|â–ˆâ–        | 769/6250 [21:03<2:25:54,  1.60s/it] 12%|â–ˆâ–        | 769/6250 [21:03<2:25:53,  1.60s/it] 12%|â–ˆâ–        | 770/6250 [21:05<2:25:44,  1.60s/it] 12%|â–ˆâ–        | 770/6250 [21:05<2:26:08,  1.60s/it] 12%|â–ˆâ–        | 771/6250 [21:06<2:26:12,  1.60s/it] 12%|â–ˆâ–        | 771/6250 [21:06<2:26:04,  1.60s/it] 12%|â–ˆâ–        | 772/6250 [21:08<2:25:54,  1.60s/it] 12%|â–ˆâ–        | 772/6250 [21:08<2:25:50,  1.60s/it] 12%|â–ˆâ–        | 773/6250 [21:09<2:25:53,  1.60s/it] 12%|â–ˆâ–        | 773/6250 [21:09<2:25:47,  1.60s/it] 12%|â–ˆâ–        | 774/6250 [21:11<2:25:43,  1.60s/it] 12%|â–ˆâ–        | 774/6250 [21:11<2:25:39,  1.60s/it] 12%|â–ˆâ–        | 775/6250 [21:13<2:25:37,  1.60s/it] 12%|â–ˆâ–        | 775/6250 [21:13<2:25:36,  1.60s/it] 12%|â–ˆâ–        | 776/6250 [21:14<2:25:26,  1.59s/it] 12%|â–ˆâ–        | 776/6250 [21:14<2:25:26,  1.59s/it] 12%|â–ˆâ–        | 777/6250 [21:16<2:25:36,  1.60s/it] 12%|â–ˆâ–        | 777/6250 [21:16<2:25:34,  1.60s/it] 12%|â–ˆâ–        | 778/6250 [21:17<2:25:29,  1.60s/it] 12%|â–ˆâ–        | 778/6250 [21:17<2:25:30,  1.60s/it] 12%|â–ˆâ–        | 779/6250 [21:19<2:25:31,  1.60s/it] 12%|â–ˆâ–        | 779/6250 [21:19<2:25:30,  1.60s/it]2025-08-19 11:26:25,546 - PixArt - INFO - Step/Epoch [780/1][780/6250]:total_eta: 11:02:06, epoch_eta:2:29:32, time_all:1.598, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6068, grad_norm:15.9606
 12%|â–ˆâ–        | 780/6250 [21:21<2:25:31,  1.60s/it] 12%|â–ˆâ–        | 780/6250 [21:21<2:25:30,  1.60s/it] 12%|â–ˆâ–        | 781/6250 [21:22<2:25:31,  1.60s/it] 12%|â–ˆâ–        | 781/6250 [21:22<2:25:29,  1.60s/it] 13%|â–ˆâ–Ž        | 782/6250 [21:24<2:25:48,  1.60s/it] 13%|â–ˆâ–Ž        | 782/6250 [21:24<2:25:49,  1.60s/it] 13%|â–ˆâ–Ž        | 783/6250 [21:25<2:25:44,  1.60s/it] 13%|â–ˆâ–Ž        | 783/6250 [21:25<2:25:45,  1.60s/it] 13%|â–ˆâ–Ž        | 784/6250 [21:27<2:25:27,  1.60s/it] 13%|â–ˆâ–Ž        | 784/6250 [21:27<2:25:26,  1.60s/it] 13%|â–ˆâ–Ž        | 785/6250 [21:29<2:25:25,  1.60s/it] 13%|â–ˆâ–Ž        | 785/6250 [21:29<2:25:25,  1.60s/it] 13%|â–ˆâ–Ž        | 786/6250 [21:30<2:25:11,  1.59s/it] 13%|â–ˆâ–Ž        | 786/6250 [21:30<2:25:13,  1.59s/it] 13%|â–ˆâ–Ž        | 787/6250 [21:32<2:25:20,  1.60s/it] 13%|â–ˆâ–Ž        | 787/6250 [21:32<2:25:20,  1.60s/it] 13%|â–ˆâ–Ž        | 788/6250 [21:33<2:25:10,  1.59s/it] 13%|â–ˆâ–Ž        | 788/6250 [21:33<2:25:10,  1.59s/it] 13%|â–ˆâ–Ž        | 789/6250 [21:35<2:25:22,  1.60s/it] 13%|â–ˆâ–Ž        | 789/6250 [21:35<2:25:19,  1.60s/it] 13%|â–ˆâ–Ž        | 790/6250 [21:37<2:25:08,  1.59s/it] 13%|â–ˆâ–Ž        | 790/6250 [21:37<2:25:06,  1.59s/it] 13%|â–ˆâ–Ž        | 791/6250 [21:38<2:24:52,  1.59s/it] 13%|â–ˆâ–Ž        | 791/6250 [21:38<2:24:53,  1.59s/it] 13%|â–ˆâ–Ž        | 792/6250 [21:40<2:24:50,  1.59s/it] 13%|â–ˆâ–Ž        | 792/6250 [21:40<2:24:54,  1.59s/it] 13%|â–ˆâ–Ž        | 793/6250 [21:41<2:25:12,  1.60s/it] 13%|â–ˆâ–Ž        | 793/6250 [21:41<2:25:11,  1.60s/it] 13%|â–ˆâ–Ž        | 794/6250 [21:43<2:25:09,  1.60s/it] 13%|â–ˆâ–Ž        | 794/6250 [21:43<2:25:08,  1.60s/it] 13%|â–ˆâ–Ž        | 795/6250 [21:45<2:25:05,  1.60s/it] 13%|â–ˆâ–Ž        | 795/6250 [21:45<2:25:04,  1.60s/it] 13%|â–ˆâ–Ž        | 796/6250 [21:46<2:25:08,  1.60s/it] 13%|â–ˆâ–Ž        | 796/6250 [21:46<2:25:08,  1.60s/it] 13%|â–ˆâ–Ž        | 797/6250 [21:48<2:25:12,  1.60s/it] 13%|â–ˆâ–Ž        | 797/6250 [21:48<2:25:12,  1.60s/it] 13%|â–ˆâ–Ž        | 798/6250 [21:49<2:25:02,  1.60s/it] 13%|â–ˆâ–Ž        | 798/6250 [21:49<2:25:01,  1.60s/it] 13%|â–ˆâ–Ž        | 799/6250 [21:51<2:24:52,  1.59s/it] 13%|â–ˆâ–Ž        | 799/6250 [21:51<2:24:52,  1.59s/it]2025-08-19 11:26:57,456 - PixArt - INFO - Step/Epoch [800/1][800/6250]:total_eta: 11:01:06, epoch_eta:2:28:53, time_all:1.595, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6317, grad_norm:18.1340
 13%|â–ˆâ–Ž        | 800/6250 [21:52<2:24:39,  1.59s/it] 13%|â–ˆâ–Ž        | 800/6250 [21:52<2:24:39,  1.59s/it] 13%|â–ˆâ–Ž        | 801/6250 [21:54<2:24:37,  1.59s/it] 13%|â–ˆâ–Ž        | 801/6250 [21:54<2:24:37,  1.59s/it] 13%|â–ˆâ–Ž        | 802/6250 [21:56<2:24:38,  1.59s/it] 13%|â–ˆâ–Ž        | 802/6250 [21:56<2:24:39,  1.59s/it] 13%|â–ˆâ–Ž        | 803/6250 [21:57<2:24:56,  1.60s/it] 13%|â–ˆâ–Ž        | 803/6250 [21:57<2:24:56,  1.60s/it] 13%|â–ˆâ–Ž        | 804/6250 [21:59<2:24:46,  1.60s/it] 13%|â–ˆâ–Ž        | 804/6250 [21:59<2:24:46,  1.59s/it] 13%|â–ˆâ–Ž        | 805/6250 [22:00<2:24:39,  1.59s/it] 13%|â–ˆâ–Ž        | 805/6250 [22:00<2:24:38,  1.59s/it] 13%|â–ˆâ–Ž        | 806/6250 [22:02<2:24:58,  1.60s/it] 13%|â–ˆâ–Ž        | 806/6250 [22:02<2:24:59,  1.60s/it] 13%|â–ˆâ–Ž        | 807/6250 [22:04<2:24:55,  1.60s/it] 13%|â–ˆâ–Ž        | 807/6250 [22:04<2:24:56,  1.60s/it] 13%|â–ˆâ–Ž        | 808/6250 [22:05<2:25:02,  1.60s/it] 13%|â–ˆâ–Ž        | 808/6250 [22:05<2:24:59,  1.60s/it] 13%|â–ˆâ–Ž        | 809/6250 [22:07<2:24:54,  1.60s/it] 13%|â–ˆâ–Ž        | 809/6250 [22:07<2:24:55,  1.60s/it] 13%|â–ˆâ–Ž        | 810/6250 [22:08<2:24:52,  1.60s/it] 13%|â–ˆâ–Ž        | 810/6250 [22:08<2:24:52,  1.60s/it] 13%|â–ˆâ–Ž        | 811/6250 [22:10<2:24:45,  1.60s/it] 13%|â–ˆâ–Ž        | 811/6250 [22:10<2:24:45,  1.60s/it] 13%|â–ˆâ–Ž        | 812/6250 [22:12<2:25:07,  1.60s/it] 13%|â–ˆâ–Ž        | 812/6250 [22:12<2:25:09,  1.60s/it] 13%|â–ˆâ–Ž        | 813/6250 [22:13<2:24:55,  1.60s/it] 13%|â–ˆâ–Ž        | 813/6250 [22:13<2:24:58,  1.60s/it] 13%|â–ˆâ–Ž        | 814/6250 [22:15<2:24:43,  1.60s/it] 13%|â–ˆâ–Ž        | 814/6250 [22:15<2:24:39,  1.60s/it] 13%|â–ˆâ–Ž        | 815/6250 [22:16<2:24:37,  1.60s/it] 13%|â–ˆâ–Ž        | 815/6250 [22:16<2:24:36,  1.60s/it] 13%|â–ˆâ–Ž        | 816/6250 [22:18<2:24:34,  1.60s/it] 13%|â–ˆâ–Ž        | 816/6250 [22:18<2:24:35,  1.60s/it] 13%|â–ˆâ–Ž        | 817/6250 [22:20<2:24:27,  1.60s/it] 13%|â–ˆâ–Ž        | 817/6250 [22:20<2:24:28,  1.60s/it] 13%|â–ˆâ–Ž        | 818/6250 [22:21<2:24:38,  1.60s/it] 13%|â–ˆâ–Ž        | 818/6250 [22:21<2:24:35,  1.60s/it] 13%|â–ˆâ–Ž        | 819/6250 [22:23<2:24:31,  1.60s/it] 13%|â–ˆâ–Ž        | 819/6250 [22:23<2:24:30,  1.60s/it]2025-08-19 11:27:29,395 - PixArt - INFO - Step/Epoch [820/1][820/6250]:total_eta: 11:00:08, epoch_eta:2:28:15, time_all:1.597, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.5552, grad_norm:21.7171
 13%|â–ˆâ–Ž        | 820/6250 [22:24<2:24:20,  1.59s/it] 13%|â–ˆâ–Ž        | 820/6250 [22:24<2:24:21,  1.60s/it] 13%|â–ˆâ–Ž        | 821/6250 [22:26<2:24:17,  1.59s/it] 13%|â–ˆâ–Ž        | 821/6250 [22:26<2:24:17,  1.59s/it] 13%|â–ˆâ–Ž        | 822/6250 [22:28<2:24:16,  1.59s/it] 13%|â–ˆâ–Ž        | 822/6250 [22:28<2:24:15,  1.59s/it] 13%|â–ˆâ–Ž        | 823/6250 [22:29<2:24:36,  1.60s/it] 13%|â–ˆâ–Ž        | 823/6250 [22:29<2:24:36,  1.60s/it] 13%|â–ˆâ–Ž        | 824/6250 [22:31<2:24:34,  1.60s/it] 13%|â–ˆâ–Ž        | 824/6250 [22:31<2:24:33,  1.60s/it] 13%|â–ˆâ–Ž        | 825/6250 [22:32<2:24:27,  1.60s/it] 13%|â–ˆâ–Ž        | 825/6250 [22:32<2:24:29,  1.60s/it] 13%|â–ˆâ–Ž        | 826/6250 [22:34<2:24:14,  1.60s/it] 13%|â–ˆâ–Ž        | 826/6250 [22:34<2:24:16,  1.60s/it] 13%|â–ˆâ–Ž        | 827/6250 [22:36<2:24:11,  1.60s/it] 13%|â–ˆâ–Ž        | 827/6250 [22:36<2:24:14,  1.60s/it] 13%|â–ˆâ–Ž        | 828/6250 [22:37<2:24:21,  1.60s/it] 13%|â–ˆâ–Ž        | 828/6250 [22:37<2:24:19,  1.60s/it] 13%|â–ˆâ–Ž        | 829/6250 [22:39<2:24:17,  1.60s/it] 13%|â–ˆâ–Ž        | 829/6250 [22:39<2:24:17,  1.60s/it] 13%|â–ˆâ–Ž        | 830/6250 [22:40<2:24:06,  1.60s/it] 13%|â–ˆâ–Ž        | 830/6250 [22:40<2:24:04,  1.59s/it] 13%|â–ˆâ–Ž        | 831/6250 [22:42<2:24:05,  1.60s/it] 13%|â–ˆâ–Ž        | 831/6250 [22:42<2:24:13,  1.60s/it] 13%|â–ˆâ–Ž        | 832/6250 [22:44<2:24:12,  1.60s/it] 13%|â–ˆâ–Ž        | 832/6250 [22:44<2:24:11,  1.60s/it] 13%|â–ˆâ–Ž        | 833/6250 [22:45<2:24:12,  1.60s/it] 13%|â–ˆâ–Ž        | 833/6250 [22:45<2:24:10,  1.60s/it] 13%|â–ˆâ–Ž        | 834/6250 [22:47<2:24:14,  1.60s/it] 13%|â–ˆâ–Ž        | 834/6250 [22:47<2:24:12,  1.60s/it] 13%|â–ˆâ–Ž        | 835/6250 [22:48<2:24:15,  1.60s/it] 13%|â–ˆâ–Ž        | 835/6250 [22:48<2:24:15,  1.60s/it] 13%|â–ˆâ–Ž        | 836/6250 [22:50<2:24:08,  1.60s/it] 13%|â–ˆâ–Ž        | 836/6250 [22:50<2:24:07,  1.60s/it] 13%|â–ˆâ–Ž        | 837/6250 [22:52<2:29:58,  1.66s/it] 13%|â–ˆâ–Ž        | 837/6250 [22:52<2:29:57,  1.66s/it] 13%|â–ˆâ–Ž        | 838/6250 [22:53<2:28:11,  1.64s/it] 13%|â–ˆâ–Ž        | 838/6250 [22:53<2:28:10,  1.64s/it] 13%|â–ˆâ–Ž        | 839/6250 [22:55<2:26:45,  1.63s/it] 13%|â–ˆâ–Ž        | 839/6250 [22:55<2:26:48,  1.63s/it]2025-08-19 11:28:01,551 - PixArt - INFO - Step/Epoch [840/1][840/6250]:total_eta: 10:59:18, epoch_eta:2:27:38, time_all:1.608, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7775, grad_norm:36.8216
 13%|â–ˆâ–Ž        | 840/6250 [22:57<2:25:58,  1.62s/it] 13%|â–ˆâ–Ž        | 840/6250 [22:57<2:25:58,  1.62s/it] 13%|â–ˆâ–Ž        | 841/6250 [22:58<2:25:15,  1.61s/it] 13%|â–ˆâ–Ž        | 841/6250 [22:58<2:25:14,  1.61s/it] 13%|â–ˆâ–Ž        | 842/6250 [23:00<2:24:41,  1.61s/it] 13%|â–ˆâ–Ž        | 842/6250 [23:00<2:24:39,  1.60s/it] 13%|â–ˆâ–Ž        | 843/6250 [23:01<2:24:19,  1.60s/it] 13%|â–ˆâ–Ž        | 843/6250 [23:01<2:24:21,  1.60s/it] 14%|â–ˆâ–Ž        | 844/6250 [23:03<2:24:10,  1.60s/it] 14%|â–ˆâ–Ž        | 844/6250 [23:03<2:24:08,  1.60s/it] 14%|â–ˆâ–Ž        | 845/6250 [23:05<2:24:13,  1.60s/it] 14%|â–ˆâ–Ž        | 845/6250 [23:05<2:24:14,  1.60s/it] 14%|â–ˆâ–Ž        | 846/6250 [23:06<2:23:58,  1.60s/it] 14%|â–ˆâ–Ž        | 846/6250 [23:06<2:23:59,  1.60s/it] 14%|â–ˆâ–Ž        | 847/6250 [23:08<2:23:53,  1.60s/it] 14%|â–ˆâ–Ž        | 847/6250 [23:08<2:23:52,  1.60s/it] 14%|â–ˆâ–Ž        | 848/6250 [23:09<2:23:42,  1.60s/it] 14%|â–ˆâ–Ž        | 848/6250 [23:09<2:23:40,  1.60s/it] 14%|â–ˆâ–Ž        | 849/6250 [23:11<2:23:34,  1.59s/it] 14%|â–ˆâ–Ž        | 849/6250 [23:11<2:23:35,  1.60s/it] 14%|â–ˆâ–Ž        | 850/6250 [23:13<2:23:28,  1.59s/it] 14%|â–ˆâ–Ž        | 850/6250 [23:13<2:23:26,  1.59s/it] 14%|â–ˆâ–Ž        | 851/6250 [23:14<2:23:23,  1.59s/it] 14%|â–ˆâ–Ž        | 851/6250 [23:14<2:23:24,  1.59s/it] 14%|â–ˆâ–Ž        | 852/6250 [23:16<2:23:25,  1.59s/it] 14%|â–ˆâ–Ž        | 852/6250 [23:16<2:23:23,  1.59s/it] 14%|â–ˆâ–Ž        | 853/6250 [23:17<2:23:13,  1.59s/it] 14%|â–ˆâ–Ž        | 853/6250 [23:17<2:23:15,  1.59s/it] 14%|â–ˆâ–Ž        | 854/6250 [23:19<2:23:13,  1.59s/it] 14%|â–ˆâ–Ž        | 854/6250 [23:19<2:23:12,  1.59s/it] 14%|â–ˆâ–Ž        | 855/6250 [23:20<2:23:13,  1.59s/it] 14%|â–ˆâ–Ž        | 855/6250 [23:20<2:23:16,  1.59s/it] 14%|â–ˆâ–Ž        | 856/6250 [23:22<2:23:13,  1.59s/it] 14%|â–ˆâ–Ž        | 856/6250 [23:22<2:23:14,  1.59s/it] 14%|â–ˆâ–Ž        | 857/6250 [23:24<2:23:23,  1.60s/it] 14%|â–ˆâ–Ž        | 857/6250 [23:24<2:23:22,  1.60s/it] 14%|â–ˆâ–Ž        | 858/6250 [23:25<2:23:18,  1.59s/it] 14%|â–ˆâ–Ž        | 858/6250 [23:25<2:23:17,  1.59s/it] 14%|â–ˆâ–Ž        | 859/6250 [23:27<2:23:14,  1.59s/it] 14%|â–ˆâ–Ž        | 859/6250 [23:27<2:23:13,  1.59s/it]2025-08-19 11:28:33,434 - PixArt - INFO - Step/Epoch [860/1][860/6250]:total_eta: 10:58:21, epoch_eta:2:27:00, time_all:1.594, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6048, grad_norm:27.4638
 14%|â–ˆâ–        | 860/6250 [23:28<2:23:17,  1.60s/it] 14%|â–ˆâ–        | 860/6250 [23:28<2:23:17,  1.60s/it] 14%|â–ˆâ–        | 861/6250 [23:30<2:23:15,  1.59s/it] 14%|â–ˆâ–        | 861/6250 [23:30<2:23:15,  1.60s/it] 14%|â–ˆâ–        | 862/6250 [23:32<2:23:14,  1.60s/it] 14%|â–ˆâ–        | 862/6250 [23:32<2:23:14,  1.60s/it] 14%|â–ˆâ–        | 863/6250 [23:33<2:23:05,  1.59s/it] 14%|â–ˆâ–        | 863/6250 [23:33<2:23:05,  1.59s/it] 14%|â–ˆâ–        | 864/6250 [23:35<2:23:07,  1.59s/it] 14%|â–ˆâ–        | 864/6250 [23:35<2:23:08,  1.59s/it] 14%|â–ˆâ–        | 865/6250 [23:36<2:23:08,  1.59s/it] 14%|â–ˆâ–        | 865/6250 [23:36<2:23:06,  1.59s/it] 14%|â–ˆâ–        | 866/6250 [23:38<2:23:12,  1.60s/it] 14%|â–ˆâ–        | 866/6250 [23:38<2:23:12,  1.60s/it] 14%|â–ˆâ–        | 867/6250 [23:40<2:23:22,  1.60s/it] 14%|â–ˆâ–        | 867/6250 [23:40<2:23:23,  1.60s/it] 14%|â–ˆâ–        | 868/6250 [23:41<2:23:26,  1.60s/it] 14%|â–ˆâ–        | 868/6250 [23:41<2:23:26,  1.60s/it] 14%|â–ˆâ–        | 869/6250 [23:43<2:23:20,  1.60s/it] 14%|â–ˆâ–        | 869/6250 [23:43<2:23:21,  1.60s/it] 14%|â–ˆâ–        | 870/6250 [23:44<2:23:26,  1.60s/it] 14%|â–ˆâ–        | 870/6250 [23:44<2:23:25,  1.60s/it] 14%|â–ˆâ–        | 871/6250 [23:46<2:23:15,  1.60s/it] 14%|â–ˆâ–        | 871/6250 [23:46<2:23:21,  1.60s/it] 14%|â–ˆâ–        | 872/6250 [23:48<2:23:16,  1.60s/it] 14%|â–ˆâ–        | 872/6250 [23:48<2:23:13,  1.60s/it] 14%|â–ˆâ–        | 873/6250 [23:49<2:22:58,  1.60s/it] 14%|â–ˆâ–        | 873/6250 [23:49<2:22:57,  1.60s/it] 14%|â–ˆâ–        | 874/6250 [23:51<2:23:00,  1.60s/it] 14%|â–ˆâ–        | 874/6250 [23:51<2:23:00,  1.60s/it] 14%|â–ˆâ–        | 875/6250 [23:52<2:22:55,  1.60s/it] 14%|â–ˆâ–        | 875/6250 [23:52<2:22:52,  1.59s/it] 14%|â–ˆâ–        | 876/6250 [23:54<2:22:49,  1.59s/it] 14%|â–ˆâ–        | 876/6250 [23:54<2:22:48,  1.59s/it] 14%|â–ˆâ–        | 877/6250 [23:56<2:22:49,  1.59s/it] 14%|â–ˆâ–        | 877/6250 [23:56<2:22:49,  1.59s/it] 14%|â–ˆâ–        | 878/6250 [23:57<2:22:50,  1.60s/it] 14%|â–ˆâ–        | 878/6250 [23:57<2:22:49,  1.60s/it] 14%|â–ˆâ–        | 879/6250 [23:59<2:22:42,  1.59s/it] 14%|â–ˆâ–        | 879/6250 [23:59<2:22:50,  1.60s/it]2025-08-19 11:29:05,359 - PixArt - INFO - Step/Epoch [880/1][880/6250]:total_eta: 10:57:26, epoch_eta:2:26:22, time_all:1.596, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.5901, grad_norm:20.1241
 14%|â–ˆâ–        | 880/6250 [24:00<2:22:50,  1.60s/it] 14%|â–ˆâ–        | 880/6250 [24:00<2:22:47,  1.60s/it] 14%|â–ˆâ–        | 881/6250 [24:02<2:22:49,  1.60s/it] 14%|â–ˆâ–        | 881/6250 [24:02<2:22:46,  1.60s/it] 14%|â–ˆâ–        | 882/6250 [24:04<2:22:36,  1.59s/it] 14%|â–ˆâ–        | 882/6250 [24:04<2:22:36,  1.59s/it] 14%|â–ˆâ–        | 883/6250 [24:05<2:22:49,  1.60s/it] 14%|â–ˆâ–        | 883/6250 [24:05<2:22:48,  1.60s/it] 14%|â–ˆâ–        | 884/6250 [24:07<2:22:48,  1.60s/it] 14%|â–ˆâ–        | 884/6250 [24:07<2:22:48,  1.60s/it] 14%|â–ˆâ–        | 885/6250 [24:08<2:22:47,  1.60s/it] 14%|â–ˆâ–        | 885/6250 [24:08<2:22:45,  1.60s/it] 14%|â–ˆâ–        | 886/6250 [24:10<2:22:50,  1.60s/it] 14%|â–ˆâ–        | 886/6250 [24:10<2:22:50,  1.60s/it] 14%|â–ˆâ–        | 887/6250 [24:12<2:22:32,  1.59s/it] 14%|â–ˆâ–        | 887/6250 [24:12<2:22:33,  1.59s/it] 14%|â–ˆâ–        | 888/6250 [24:13<2:22:24,  1.59s/it] 14%|â–ˆâ–        | 888/6250 [24:13<2:22:22,  1.59s/it] 14%|â–ˆâ–        | 889/6250 [24:15<2:22:10,  1.59s/it] 14%|â–ˆâ–        | 889/6250 [24:15<2:22:09,  1.59s/it] 14%|â–ˆâ–        | 890/6250 [24:16<2:22:12,  1.59s/it] 14%|â–ˆâ–        | 890/6250 [24:16<2:22:20,  1.59s/it] 14%|â–ˆâ–        | 891/6250 [24:18<2:22:18,  1.59s/it] 14%|â–ˆâ–        | 891/6250 [24:18<2:22:16,  1.59s/it] 14%|â–ˆâ–        | 892/6250 [24:20<2:22:28,  1.60s/it] 14%|â–ˆâ–        | 892/6250 [24:20<2:22:28,  1.60s/it] 14%|â–ˆâ–        | 893/6250 [24:21<2:22:31,  1.60s/it] 14%|â–ˆâ–        | 893/6250 [24:21<2:22:31,  1.60s/it] 14%|â–ˆâ–        | 894/6250 [24:23<2:22:33,  1.60s/it] 14%|â–ˆâ–        | 894/6250 [24:23<2:22:33,  1.60s/it] 14%|â–ˆâ–        | 895/6250 [24:24<2:22:31,  1.60s/it] 14%|â–ˆâ–        | 895/6250 [24:24<2:22:34,  1.60s/it] 14%|â–ˆâ–        | 896/6250 [24:26<2:22:39,  1.60s/it] 14%|â–ˆâ–        | 896/6250 [24:26<2:22:39,  1.60s/it] 14%|â–ˆâ–        | 897/6250 [24:28<2:22:39,  1.60s/it] 14%|â–ˆâ–        | 897/6250 [24:28<2:22:39,  1.60s/it] 14%|â–ˆâ–        | 898/6250 [24:29<2:22:45,  1.60s/it] 14%|â–ˆâ–        | 898/6250 [24:29<2:22:41,  1.60s/it] 14%|â–ˆâ–        | 899/6250 [24:31<2:22:39,  1.60s/it] 14%|â–ˆâ–        | 899/6250 [24:31<2:22:39,  1.60s/it]2025-08-19 11:29:37,289 - PixArt - INFO - Step/Epoch [900/1][900/6250]:total_eta: 10:56:33, epoch_eta:2:25:45, time_all:1.596, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.8056, grad_norm:48.2166
 14%|â–ˆâ–        | 900/6250 [24:32<2:22:30,  1.60s/it] 14%|â–ˆâ–        | 900/6250 [24:32<2:22:30,  1.60s/it] 14%|â–ˆâ–        | 901/6250 [24:34<2:22:25,  1.60s/it] 14%|â–ˆâ–        | 901/6250 [24:34<2:22:25,  1.60s/it] 14%|â–ˆâ–        | 902/6250 [24:35<2:22:17,  1.60s/it] 14%|â–ˆâ–        | 902/6250 [24:35<2:22:16,  1.60s/it] 14%|â–ˆâ–        | 903/6250 [24:37<2:22:07,  1.59s/it] 14%|â–ˆâ–        | 903/6250 [24:37<2:22:10,  1.60s/it] 14%|â–ˆâ–        | 904/6250 [24:39<2:22:11,  1.60s/it] 14%|â–ˆâ–        | 904/6250 [24:39<2:22:12,  1.60s/it] 14%|â–ˆâ–        | 905/6250 [24:40<2:22:04,  1.59s/it] 14%|â–ˆâ–        | 905/6250 [24:40<2:22:04,  1.59s/it] 14%|â–ˆâ–        | 906/6250 [24:42<2:22:00,  1.59s/it] 14%|â–ˆâ–        | 906/6250 [24:42<2:22:02,  1.59s/it] 15%|â–ˆâ–        | 907/6250 [24:43<2:22:00,  1.59s/it] 15%|â–ˆâ–        | 907/6250 [24:43<2:21:59,  1.59s/it] 15%|â–ˆâ–        | 908/6250 [24:45<2:21:52,  1.59s/it] 15%|â–ˆâ–        | 908/6250 [24:45<2:21:51,  1.59s/it] 15%|â–ˆâ–        | 909/6250 [24:47<2:21:48,  1.59s/it] 15%|â–ˆâ–        | 909/6250 [24:47<2:21:46,  1.59s/it] 15%|â–ˆâ–        | 910/6250 [24:48<2:21:37,  1.59s/it] 15%|â–ˆâ–        | 910/6250 [24:48<2:21:40,  1.59s/it] 15%|â–ˆâ–        | 911/6250 [24:50<2:21:55,  1.59s/it] 15%|â–ˆâ–        | 911/6250 [24:50<2:21:53,  1.59s/it] 15%|â–ˆâ–        | 912/6250 [24:51<2:21:49,  1.59s/it] 15%|â–ˆâ–        | 912/6250 [24:51<2:21:48,  1.59s/it] 15%|â–ˆâ–        | 913/6250 [24:53<2:21:38,  1.59s/it] 15%|â–ˆâ–        | 913/6250 [24:53<2:21:37,  1.59s/it] 15%|â–ˆâ–        | 914/6250 [24:55<2:21:35,  1.59s/it] 15%|â–ˆâ–        | 914/6250 [24:55<2:21:35,  1.59s/it] 15%|â–ˆâ–        | 915/6250 [24:56<2:21:35,  1.59s/it] 15%|â–ˆâ–        | 915/6250 [24:56<2:21:35,  1.59s/it] 15%|â–ˆâ–        | 916/6250 [24:58<2:21:48,  1.60s/it] 15%|â–ˆâ–        | 916/6250 [24:58<2:21:49,  1.60s/it] 15%|â–ˆâ–        | 917/6250 [24:59<2:21:50,  1.60s/it] 15%|â–ˆâ–        | 917/6250 [24:59<2:21:50,  1.60s/it] 15%|â–ˆâ–        | 918/6250 [25:01<2:21:40,  1.59s/it] 15%|â–ˆâ–        | 918/6250 [25:01<2:21:40,  1.59s/it] 15%|â–ˆâ–        | 919/6250 [25:03<2:21:50,  1.60s/it] 15%|â–ˆâ–        | 919/6250 [25:03<2:21:49,  1.60s/it]2025-08-19 11:30:09,173 - PixArt - INFO - Step/Epoch [920/1][920/6250]:total_eta: 10:55:39, epoch_eta:2:25:07, time_all:1.594, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6972, grad_norm:40.3355
 15%|â–ˆâ–        | 920/6250 [25:04<2:21:43,  1.60s/it] 15%|â–ˆâ–        | 920/6250 [25:04<2:21:44,  1.60s/it] 15%|â–ˆâ–        | 921/6250 [25:06<2:21:40,  1.60s/it] 15%|â–ˆâ–        | 921/6250 [25:06<2:21:41,  1.60s/it] 15%|â–ˆâ–        | 922/6250 [25:07<2:21:47,  1.60s/it] 15%|â–ˆâ–        | 922/6250 [25:07<2:21:46,  1.60s/it] 15%|â–ˆâ–        | 923/6250 [25:09<2:21:46,  1.60s/it] 15%|â–ˆâ–        | 923/6250 [25:09<2:21:48,  1.60s/it] 15%|â–ˆâ–        | 924/6250 [25:11<2:21:42,  1.60s/it] 15%|â–ˆâ–        | 924/6250 [25:11<2:21:43,  1.60s/it] 15%|â–ˆâ–        | 925/6250 [25:12<2:21:31,  1.59s/it] 15%|â–ˆâ–        | 925/6250 [25:12<2:21:30,  1.59s/it] 15%|â–ˆâ–        | 926/6250 [25:14<2:21:36,  1.60s/it] 15%|â–ˆâ–        | 926/6250 [25:14<2:21:38,  1.60s/it] 15%|â–ˆâ–        | 927/6250 [25:15<2:21:41,  1.60s/it] 15%|â–ˆâ–        | 927/6250 [25:15<2:21:42,  1.60s/it] 15%|â–ˆâ–        | 928/6250 [25:17<2:21:56,  1.60s/it] 15%|â–ˆâ–        | 928/6250 [25:17<2:21:56,  1.60s/it] 15%|â–ˆâ–        | 929/6250 [25:19<2:21:44,  1.60s/it] 15%|â–ˆâ–        | 929/6250 [25:19<2:21:42,  1.60s/it] 15%|â–ˆâ–        | 930/6250 [25:20<2:21:43,  1.60s/it] 15%|â–ˆâ–        | 930/6250 [25:20<2:21:43,  1.60s/it] 15%|â–ˆâ–        | 931/6250 [25:22<2:21:48,  1.60s/it] 15%|â–ˆâ–        | 931/6250 [25:22<2:21:48,  1.60s/it] 15%|â–ˆâ–        | 932/6250 [25:23<2:21:49,  1.60s/it] 15%|â–ˆâ–        | 932/6250 [25:23<2:21:46,  1.60s/it] 15%|â–ˆâ–        | 933/6250 [25:25<2:21:37,  1.60s/it] 15%|â–ˆâ–        | 933/6250 [25:25<2:21:39,  1.60s/it] 15%|â–ˆâ–        | 934/6250 [25:27<2:21:48,  1.60s/it] 15%|â–ˆâ–        | 934/6250 [25:27<2:21:51,  1.60s/it] 15%|â–ˆâ–        | 935/6250 [25:28<2:21:43,  1.60s/it] 15%|â–ˆâ–        | 935/6250 [25:28<2:21:44,  1.60s/it] 15%|â–ˆâ–        | 936/6250 [25:30<2:21:34,  1.60s/it] 15%|â–ˆâ–        | 936/6250 [25:30<2:21:40,  1.60s/it] 15%|â–ˆâ–        | 937/6250 [25:31<2:21:38,  1.60s/it] 15%|â–ˆâ–        | 937/6250 [25:31<2:21:38,  1.60s/it] 15%|â–ˆâ–Œ        | 938/6250 [25:33<2:21:21,  1.60s/it] 15%|â–ˆâ–Œ        | 938/6250 [25:33<2:21:21,  1.60s/it] 15%|â–ˆâ–Œ        | 939/6250 [25:35<2:21:13,  1.60s/it] 15%|â–ˆâ–Œ        | 939/6250 [25:35<2:21:12,  1.60s/it]2025-08-19 11:30:41,133 - PixArt - INFO - Step/Epoch [940/1][940/6250]:total_eta: 10:54:48, epoch_eta:2:24:31, time_all:1.598, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.7678, grad_norm:39.1265
 15%|â–ˆâ–Œ        | 940/6250 [25:36<2:21:21,  1.60s/it] 15%|â–ˆâ–Œ        | 940/6250 [25:36<2:21:19,  1.60s/it] 15%|â–ˆâ–Œ        | 941/6250 [25:38<2:21:25,  1.60s/it] 15%|â–ˆâ–Œ        | 941/6250 [25:38<2:21:25,  1.60s/it] 15%|â–ˆâ–Œ        | 942/6250 [25:39<2:21:17,  1.60s/it] 15%|â–ˆâ–Œ        | 942/6250 [25:39<2:21:16,  1.60s/it] 15%|â–ˆâ–Œ        | 943/6250 [25:41<2:21:14,  1.60s/it] 15%|â–ˆâ–Œ        | 943/6250 [25:41<2:21:12,  1.60s/it] 15%|â–ˆâ–Œ        | 944/6250 [25:43<2:21:10,  1.60s/it] 15%|â–ˆâ–Œ        | 944/6250 [25:43<2:21:09,  1.60s/it] 15%|â–ˆâ–Œ        | 945/6250 [25:44<2:21:14,  1.60s/it] 15%|â–ˆâ–Œ        | 945/6250 [25:44<2:21:15,  1.60s/it] 15%|â–ˆâ–Œ        | 946/6250 [25:46<2:21:17,  1.60s/it] 15%|â–ˆâ–Œ        | 946/6250 [25:46<2:21:16,  1.60s/it] 15%|â–ˆâ–Œ        | 947/6250 [25:47<2:21:07,  1.60s/it] 15%|â–ˆâ–Œ        | 947/6250 [25:47<2:21:08,  1.60s/it] 15%|â–ˆâ–Œ        | 948/6250 [25:49<2:21:08,  1.60s/it] 15%|â–ˆâ–Œ        | 948/6250 [25:49<2:21:07,  1.60s/it] 15%|â–ˆâ–Œ        | 949/6250 [25:51<2:21:02,  1.60s/it] 15%|â–ˆâ–Œ        | 949/6250 [25:51<2:21:02,  1.60s/it] 15%|â–ˆâ–Œ        | 950/6250 [25:52<2:20:59,  1.60s/it] 15%|â–ˆâ–Œ        | 950/6250 [25:52<2:20:59,  1.60s/it] 15%|â–ˆâ–Œ        | 951/6250 [25:54<2:21:01,  1.60s/it] 15%|â–ˆâ–Œ        | 951/6250 [25:54<2:21:01,  1.60s/it] 15%|â–ˆâ–Œ        | 952/6250 [25:55<2:20:51,  1.60s/it] 15%|â–ˆâ–Œ        | 952/6250 [25:55<2:20:49,  1.59s/it] 15%|â–ˆâ–Œ        | 953/6250 [25:57<2:20:38,  1.59s/it] 15%|â–ˆâ–Œ        | 953/6250 [25:57<2:20:38,  1.59s/it] 15%|â–ˆâ–Œ        | 954/6250 [25:59<2:20:54,  1.60s/it] 15%|â–ˆâ–Œ        | 954/6250 [25:59<2:20:54,  1.60s/it] 15%|â–ˆâ–Œ        | 955/6250 [26:00<2:20:59,  1.60s/it] 15%|â–ˆâ–Œ        | 955/6250 [26:00<2:21:00,  1.60s/it] 15%|â–ˆâ–Œ        | 956/6250 [26:02<2:20:54,  1.60s/it] 15%|â–ˆâ–Œ        | 956/6250 [26:02<2:20:52,  1.60s/it] 15%|â–ˆâ–Œ        | 957/6250 [26:03<2:20:42,  1.59s/it] 15%|â–ˆâ–Œ        | 957/6250 [26:03<2:20:42,  1.60s/it] 15%|â–ˆâ–Œ        | 958/6250 [26:05<2:20:41,  1.60s/it] 15%|â–ˆâ–Œ        | 958/6250 [26:05<2:20:43,  1.60s/it] 15%|â–ˆâ–Œ        | 959/6250 [26:06<2:20:43,  1.60s/it] 15%|â–ˆâ–Œ        | 959/6250 [26:06<2:20:43,  1.60s/it]2025-08-19 11:31:13,058 - PixArt - INFO - Step/Epoch [960/1][960/6250]:total_eta: 10:53:57, epoch_eta:2:23:54, time_all:1.596, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.5078, grad_norm:29.0453
 15%|â–ˆâ–Œ        | 960/6250 [26:08<2:20:41,  1.60s/it] 15%|â–ˆâ–Œ        | 960/6250 [26:08<2:20:41,  1.60s/it] 15%|â–ˆâ–Œ        | 961/6250 [26:10<2:20:47,  1.60s/it] 15%|â–ˆâ–Œ        | 961/6250 [26:10<2:20:46,  1.60s/it] 15%|â–ˆâ–Œ        | 962/6250 [26:11<2:20:37,  1.60s/it] 15%|â–ˆâ–Œ        | 962/6250 [26:11<2:20:36,  1.60s/it] 15%|â–ˆâ–Œ        | 963/6250 [26:13<2:20:42,  1.60s/it] 15%|â–ˆâ–Œ        | 963/6250 [26:13<2:20:43,  1.60s/it] 15%|â–ˆâ–Œ        | 964/6250 [26:14<2:20:33,  1.60s/it] 15%|â–ˆâ–Œ        | 964/6250 [26:14<2:20:31,  1.60s/it] 15%|â–ˆâ–Œ        | 965/6250 [26:16<2:20:28,  1.59s/it] 15%|â–ˆâ–Œ        | 965/6250 [26:16<2:20:27,  1.59s/it] 15%|â–ˆâ–Œ        | 966/6250 [26:18<2:20:37,  1.60s/it] 15%|â–ˆâ–Œ        | 966/6250 [26:18<2:20:38,  1.60s/it] 15%|â–ˆâ–Œ        | 967/6250 [26:19<2:20:26,  1.60s/it] 15%|â–ˆâ–Œ        | 967/6250 [26:19<2:20:29,  1.60s/it] 15%|â–ˆâ–Œ        | 968/6250 [26:21<2:20:23,  1.59s/it] 15%|â–ˆâ–Œ        | 968/6250 [26:21<2:20:21,  1.59s/it] 16%|â–ˆâ–Œ        | 969/6250 [26:22<2:20:16,  1.59s/it] 16%|â–ˆâ–Œ        | 969/6250 [26:22<2:20:14,  1.59s/it] 16%|â–ˆâ–Œ        | 970/6250 [26:24<2:20:14,  1.59s/it] 16%|â–ˆâ–Œ        | 970/6250 [26:24<2:20:15,  1.59s/it] 16%|â–ˆâ–Œ        | 971/6250 [26:26<2:20:17,  1.59s/it] 16%|â–ˆâ–Œ        | 971/6250 [26:26<2:20:17,  1.59s/it] 16%|â–ˆâ–Œ        | 972/6250 [26:27<2:20:24,  1.60s/it] 16%|â–ˆâ–Œ        | 972/6250 [26:27<2:20:23,  1.60s/it] 16%|â–ˆâ–Œ        | 973/6250 [26:29<2:20:22,  1.60s/it] 16%|â–ˆâ–Œ        | 973/6250 [26:29<2:20:21,  1.60s/it] 16%|â–ˆâ–Œ        | 974/6250 [26:30<2:20:15,  1.60s/it] 16%|â–ˆâ–Œ        | 974/6250 [26:30<2:20:16,  1.60s/it] 16%|â–ˆâ–Œ        | 975/6250 [26:32<2:20:17,  1.60s/it] 16%|â–ˆâ–Œ        | 975/6250 [26:32<2:20:17,  1.60s/it] 16%|â–ˆâ–Œ        | 976/6250 [26:34<2:20:23,  1.60s/it] 16%|â–ˆâ–Œ        | 976/6250 [26:34<2:20:23,  1.60s/it] 16%|â–ˆâ–Œ        | 977/6250 [26:35<2:20:16,  1.60s/it] 16%|â–ˆâ–Œ        | 977/6250 [26:35<2:20:15,  1.60s/it] 16%|â–ˆâ–Œ        | 978/6250 [26:37<2:20:07,  1.59s/it] 16%|â–ˆâ–Œ        | 978/6250 [26:37<2:20:06,  1.59s/it] 16%|â–ˆâ–Œ        | 979/6250 [26:38<2:20:04,  1.59s/it] 16%|â–ˆâ–Œ        | 979/6250 [26:38<2:20:05,  1.59s/it]2025-08-19 11:31:44,955 - PixArt - INFO - Step/Epoch [980/1][980/6250]:total_eta: 10:53:06, epoch_eta:2:23:17, time_all:1.595, time_data:0.002, lr:5.000e-06, s:(32, 32), loss:1.6292, grad_norm:26.8201
 16%|â–ˆâ–Œ        | 980/6250 [26:40<2:19:48,  1.59s/it] 16%|â–ˆâ–Œ        | 980/6250 [26:40<2:19:48,  1.59s/it] 16%|â–ˆâ–Œ        | 981/6250 [26:42<2:19:48,  1.59s/it] 16%|â–ˆâ–Œ        | 981/6250 [26:42<2:19:52,  1.59s/it] 16%|â–ˆâ–Œ        | 982/6250 [26:43<2:19:59,  1.59s/it] 16%|â–ˆâ–Œ        | 982/6250 [26:43<2:19:57,  1.59s/it] 16%|â–ˆâ–Œ        | 983/6250 [26:45<2:19:57,  1.59s/it] 16%|â–ˆâ–Œ        | 983/6250 [26:45<2:20:01,  1.60s/it] 16%|â–ˆâ–Œ        | 984/6250 [26:46<2:19:50,  1.59s/it] 16%|â–ˆâ–Œ        | 984/6250 [26:46<2:19:48,  1.59s/it] 16%|â–ˆâ–Œ        | 985/6250 [26:48<2:19:51,  1.59s/it] 16%|â–ˆâ–Œ        | 985/6250 [26:48<2:19:50,  1.59s/it] 16%|â–ˆâ–Œ        | 986/6250 [26:50<2:19:53,  1.59s/it] 16%|â–ˆâ–Œ        | 986/6250 [26:50<2:19:51,  1.59s/it] 16%|â–ˆâ–Œ        | 987/6250 [26:51<2:19:53,  1.59s/it] 16%|â–ˆâ–Œ        | 987/6250 [26:51<2:19:54,  1.60s/it] 16%|â–ˆâ–Œ        | 988/6250 [26:53<2:19:54,  1.60s/it] 16%|â–ˆâ–Œ        | 988/6250 [26:53<2:19:53,  1.60s/it] 16%|â–ˆâ–Œ        | 989/6250 [26:54<2:19:47,  1.59s/it] 16%|â–ˆâ–Œ        | 989/6250 [26:54<2:19:45,  1.59s/it] 16%|â–ˆâ–Œ        | 990/6250 [26:56<2:19:44,  1.59s/it] 16%|â–ˆâ–Œ        | 990/6250 [26:56<2:19:50,  1.60s/it] 16%|â–ˆâ–Œ        | 991/6250 [26:58<2:19:36,  1.59s/it] 16%|â–ˆâ–Œ        | 991/6250 [26:58<2:19:35,  1.59s/it] 16%|â–ˆâ–Œ        | 992/6250 [26:59<2:19:40,  1.59s/it] 16%|â–ˆâ–Œ        | 992/6250 [26:59<2:19:36,  1.59s/it] 16%|â–ˆâ–Œ        | 993/6250 [27:01<2:19:27,  1.59s/it] 16%|â–ˆâ–Œ        | 993/6250 [27:01<2:19:27,  1.59s/it] 16%|â–ˆâ–Œ        | 994/6250 [27:02<2:19:13,  1.59s/it] 16%|â–ˆâ–Œ        | 994/6250 [27:02<2:19:13,  1.59s/it] 16%|â–ˆâ–Œ        | 995/6250 [27:04<2:19:07,  1.59s/it] 16%|â–ˆâ–Œ        | 995/6250 [27:04<2:19:08,  1.59s/it] 16%|â–ˆâ–Œ        | 996/6250 [27:05<2:19:06,  1.59s/it] 16%|â–ˆâ–Œ        | 996/6250 [27:05<2:19:05,  1.59s/it] 16%|â–ˆâ–Œ        | 997/6250 [27:07<2:19:16,  1.59s/it] 16%|â–ˆâ–Œ        | 997/6250 [27:07<2:19:16,  1.59s/it] 16%|â–ˆâ–Œ        | 998/6250 [27:09<2:19:26,  1.59s/it] 16%|â–ˆâ–Œ        | 998/6250 [27:09<2:19:26,  1.59s/it] 16%|â–ˆâ–Œ        | 999/6250 [27:10<2:19:27,  1.59s/it]2025-08-19 11:32:15,234 - PixArt - INFO - Running validation... 

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.03it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 11.98it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.06it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.07it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.12it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.12it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.05it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.17it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.17it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.14it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.13it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.14it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.11it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.13it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.14it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.08it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.11it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.10it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.07it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.15it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.13it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.15it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 11.95it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.02it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.07it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.02it/s]

  0%|          | 0/13 [00:00<?, ?it/s][A
 15%|â–ˆâ–Œ        | 2/13 [00:00<00:00, 12.18it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:00, 12.16it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:00<00:00, 12.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:00<00:00, 12.04it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:00<00:00, 12.07it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 12.02it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 13.02it/s]
 16%|â–ˆâ–Œ        | 999/6250 [27:18<5:48:40,  3.98s/it]
  0%|          | 0/32 [00:00<?, ?it/s][A
  0%|          | 0/32 [00:00<?, ?it/s][A
  3%|â–Ž         | 1/32 [00:01<00:40,  1.30s/it][A  0%|          | 0/32 [00:01<?, ?it/s]
 16%|â–ˆâ–Œ        | 999/6250 [27:22<2:23:50,  1.64s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 834, in <module>
    train()
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 330, in train
    s_loss = compute_stable_loss()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 103, in compute_stable_loss
    stable_loss_term = train_diffusion.training_losses(
  File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 97, in training_losses
    return super().training_losses(self._wrap_model(model), *args, **kwargs)
  File "/export/home/sheid/PixArt-sigma/diffusion/model/gaussian_diffusion.py", line 830, in training_losses
    model_output, feat_list = model(x_t, t, **model_kwargs, train=True)
  File "/export/home/sheid/PixArt-sigma/diffusion/model/respace.py", line 134, in __call__
    return self.model(x, timestep=new_ts, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/accelerate/utils/operations.py", line 814, in forward
    return model_forward(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/accelerate/utils/operations.py", line 802, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 279, in forward
    x = auto_grad_checkpoint(
  File "/export/home/sheid/PixArt-sigma/diffusion/model/utils.py", line 44, in auto_grad_checkpoint
    return checkpoint(module, *args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 482, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 261, in forward
    outputs = run_function(*args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/home/sheid/PixArt-sigma/diffusion/model/nets/PixArtMS.py", line 117, in forward
    gate_mlp * self.mlp(t2i_modulate(self.norm2(x), shift_mlp, scale_mlp))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/timm/models/layers/mlp.py", line 28, in forward
    x = self.act(x)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 682, in forward
    return F.gelu(input, approximate=self.approximate)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 33.19 MiB is free. Including non-PyTorch memory, this process has 12.02 GiB memory in use. Process 2394383 has 10.10 GiB memory in use. Of the allocated memory 11.19 GiB is allocated by PyTorch, and 510.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

  6%|â–‹         | 2/32 [00:01<00:25,  1.19it/s][A
  9%|â–‰         | 3/32 [00:02<00:20,  1.44it/s][A
 12%|â–ˆâ–Ž        | 4/32 [00:02<00:17,  1.61it/s][A
 16%|â–ˆâ–Œ        | 5/32 [00:03<00:15,  1.71it/s][A
 19%|â–ˆâ–‰        | 6/32 [00:03<00:14,  1.78it/s][A
 22%|â–ˆâ–ˆâ–       | 7/32 [00:04<00:13,  1.83it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:04<00:12,  1.86it/s][A[2025-08-19 11:32:29,754] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2389497 closing signal SIGTERM
[2025-08-19 11:32:31,173] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2389495) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:32:29
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2389495)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:32:59,043 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:32:59,181 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:32:59,181 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:32:59,181 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:32:59,181 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:32:59,182 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:32:59,182 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.97s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:33:13,872] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2406660 closing signal SIGTERM
[2025-08-19 11:33:14,388] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2406659) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:33:13
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2406659)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:33:40,807 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:33:40,944 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_learning/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_finetuning/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:33:40,944 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:33:40,944 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:33:40,944 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:33:40,945 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:33:40,945 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.70s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:33:57,461] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2407471 closing signal SIGTERM
[2025-08-19 11:33:58,127] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2407470) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:33:57
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2407470)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:34:21,653 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:34:21,782 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12, 23]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:34:21,782 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:34:21,782 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:34:21,782 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:34:21,782 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:34:21,782 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.34s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:34:35,556] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2407753 closing signal SIGTERM
[2025-08-19 11:34:36,071] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2407752) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:34:35
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2407752)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:35:01,125 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:35:01,261 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_finetuning/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 12, 13, 17, 18, 20, 21, 23, 24, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12, 23]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:35:01,262 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:35:01,262 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:35:01,262 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:35:01,263 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:35:01,264 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.72s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:35:18,794] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2407993 closing signal SIGTERM
[2025-08-19 11:35:19,560] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2407992) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:35:18
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2407992)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 472, in <module>
    config = read_config(args.config)
  File "/export/home/sheid/PixArt-sigma/diffusion/utils/misc.py", line 25, in read_config
    config = Config.fromfile(file)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/config.py", line 340, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/config.py", line 183, in _file2dict
    check_file_exist(filename)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/path.py", line 23, in check_file_exist
    raise FileNotFoundError(msg_tmpl.format(filename))
FileNotFoundError: file "/export/home/sheid/PixArt-sigma/configs/pixart_sigma_config/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_finetuning.pyy" does not exist
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 472, in <module>
    config = read_config(args.config)
  File "/export/home/sheid/PixArt-sigma/diffusion/utils/misc.py", line 25, in read_config
    config = Config.fromfile(file)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/config.py", line 340, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/config.py", line 183, in _file2dict
    check_file_exist(filename)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/utils/path.py", line 23, in check_file_exist
    raise FileNotFoundError(msg_tmpl.format(filename))
FileNotFoundError: file "/export/home/sheid/PixArt-sigma/configs/pixart_sigma_config/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_finetuning.pyy" does not exist
[2025-08-19 11:35:47,024] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2408269) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-08-19_11:35:47
  host      : hcigpu07
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2408270)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:35:47
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2408269)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:36:12,153 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:36:12,275 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_finetuning/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 12, 13, 17, 18, 21, 23, 24, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12, 23, 21]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:36:12,275 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:36:12,275 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:36:12,275 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:36:12,276 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:36:12,276 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.35s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:36:24,896] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2408451 closing signal SIGTERM
[2025-08-19 11:36:25,411] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2408450) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:36:24
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2408450)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:36:49,345 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:36:49,479 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/',
    load_img_vae_feat=False)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 37990
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_finetuning_trained_on_pixart_generated_images/checkpoints/epoch_2_step_12500.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_18_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 12, 13, 18, 21, 22, 23, 24, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12, 23, 21, 18]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:36:49,480 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:36:49,480 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:36:49,480 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:36:49,480 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:36:49,480 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.51s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:37:03,060] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2408709 closing signal SIGTERM
[2025-08-19 11:37:03,525] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2408708) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:37:03
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2408708)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:37:28,139 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:37:28,267 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    image_list_json=['data_info.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = False
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = False
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 3
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 2
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=2e-05,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 12500
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_18_finetuning/checkpoints/epoch_1_step_37990.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/shortcut_training/PixArt_sigma_xl2_img512_laion_17_15_8_20_11_16_12_23_21_18_finetuning_trained_on_pixart_generated_images'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [8, 9, 12, 13, 18, 21, 23, 24, 25, 26, 27]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 11, 16, 12, 23, 21, 18]
trainable_blocks = []
reserve_memory = False
stable_loss = True
image_list_json = ['data_info.json']
data_stable_loss = dict(
    type='InternalDataMSSigma',
    root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/feature_pixart',
    img_root=
    '/export/data/vislearn/rother_subgroup/sheid/pixart/pixart_generated_images/images',
    image_list_json=['data_info_stable_loss.json'],
    transform='default_train',
    load_vae_feat=True,
    load_t5_feat=True,
    load_img_vae_feat=False)
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
log_interval_stable_loss = 1000
org_loss_flag = False

2025-08-19 11:37:28,267 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:37:28,267 - PixArt - INFO - Initializing: DDP for training
2025-08-19 11:37:28,268 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:37:28,269 - PixArt - INFO - Preparing Visualization prompt embeddings...
2025-08-19 11:37:28,269 - PixArt - INFO - Loading text encoder and tokenizer from /export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers ...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.56s/it]
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 732, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 346, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 314, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 11.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 36.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:37:46,133] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2408984 closing signal SIGTERM
[2025-08-19 11:37:46,749] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2408983) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:37:46
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2408983)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:38:11,520 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:38:11,631 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root='/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M',
    image_list_json=['data_info_fixed.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = True
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = True
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 12
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=0.0002,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/third_pruning_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/third_pruning_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 10, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12]
trainable_blocks = []
reserve_memory = False
stable_loss = False
image_list_json = ['data_info_fixed.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False

2025-08-19 11:38:11,631 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:38:11,631 - PixArt - INFO - Initializing: DDP for training
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.09s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.67s/it]
2025-08-19 11:38:24,343 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:38:24,344 - PixArt - INFO - Preparing Visualization prompt embeddings...
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 682, in forward
    self_attention_outputs = self.layer[0](
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 600, in forward
    attention_output = self.SelfAttention(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 559, in forward
    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 13.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 35.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:38:29,425] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2409264 closing signal SIGTERM
[2025-08-19 11:38:29,991] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2409263) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:38:29
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2409263)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2025-08-19 11:38:54,419 - PixArt - INFO - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

2025-08-19 11:38:54,520 - PixArt - INFO - Config: 
data_root = 'pixart-sigma-toy-dataset'
data = dict(
    type='InternalDataMSSigma',
    root='/export/data/vislearn/rother_subgroup/sheid/pixart/laion2M',
    image_list_json=['data_info_fixed.json'],
    transform='default_train',
    load_vae_feat=False,
    load_t5_feat=False,
    img_root=
    '/export/data/vislearn/rother_subgroup/rother_datasets/LaionAE/laion2B-en-art_512/'
)
image_size = 512
train_batch_size = 8
eval_batch_size = 16
use_fsdp = False
valid_num = 0
fp32_attention = True
model = 'PixArtMS_XL_2'
aspect_ratio_type = 'ASPECT_RATIO_512'
multi_scale = True
pe_interpolation = 1.0
qk_norm = False
kv_compress = False
kv_compress_config = dict(sampling=None, scale_factor=1, kv_compress_layer=[])
num_workers = 12
train_sampling_steps = 1000
visualize = True
deterministic_validation = False
eval_sampling_steps = 500
model_max_length = 300
lora_rank = 4
num_epochs = 1
gradient_accumulation_steps = 1
grad_checkpointing = True
gradient_clip = 0.01
gc_step = 1
auto_lr = dict(rule='sqrt')
validation_prompts = [
    'dog',
    'portrait photo of a girl, photograph, highly detailed face, depth of field',
    'Self-portrait oil painting, a beautiful cyborg with golden hair, 8k',
    'Astronaut in a jungle, cold color palette, muted colors, detailed, 8k',
    'A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece'
]
optimizer = dict(
    type='CAMEWrapper',
    lr=0.0002,
    weight_decay=0.03,
    eps=(1e-30, 1e-16),
    betas=(0.9, 0.999, 0.9999))
lr_schedule = 'constant'
lr_schedule_args = dict(num_warmup_steps=1000)
save_image_epochs = 1
save_model_epochs = 1
save_model_steps = 38000
sample_posterior = True
mixed_precision = 'fp16'
scale_factor = 0.13025
ema_rate = 0.9999
tensorboard_mox_interval = 50
log_interval = 20
cfg_scale = 4
mask_type = 'null'
num_group_tokens = 0
mask_loss_coef = 0.0
load_mask_index = False
vae_pretrained = '/export/scratch/sheid/pixart/pixart_sigma_sdxlvae_T5_diffusers/vae'
load_from = '/export/data/sheid/pixart/third_pruning_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12/checkpoints/epoch_1_step_38000.pth'
resume_from = None
snr_loss = False
real_prompt_ratio = 1.0
class_dropout_prob = 0.1
work_dir = '/export/data/sheid/pixart/third_pruning_attempt/PixArt_sigma_xl2_img512_laion_17_15_8_20_12_finetuning'
s3_work_dir = None
micro_condition = False
seed = 43
skip_step = 0
loss_type = 'huber'
huber_c = 0.001
num_ddim_timesteps = 50
w_max = 15.0
w_min = 3.0
ema_decay = 0.95
intermediate_loss_flag = True
intermediate_loss_blocks = [
    8, 9, 10, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27
]
final_output_loss_flag = True
transformer_blocks = [8, 15, 17, 20, 12]
trainable_blocks = []
reserve_memory = False
stable_loss = False
image_list_json = ['data_info_fixed.json']
ref_load_from = '/export/scratch/sheid/pixart/PixArt-Sigma-XL-2-512-MS.pth'
org_loss_flag = False

2025-08-19 11:38:54,520 - PixArt - INFO - World_size: 2, seed: 43
2025-08-19 11:38:54,520 - PixArt - INFO - Initializing: DDP for training
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.65s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.68s/it]
2025-08-19 11:39:08,116 - PixArt - INFO - vae scale factor: 0.13025
2025-08-19 11:39:08,118 - PixArt - INFO - Preparing Visualization prompt embeddings...
Traceback (most recent call last):
  File "/export/home/sheid/PixArt-sigma/train_scripts/train.py", line 607, in <module>
    caption_emb = text_encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 2100, in forward
    encoder_outputs = self.encoder(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 1131, in forward
    layer_outputs = layer_module(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 682, in forward
    self_attention_outputs = self.layer[0](
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 600, in forward
    attention_output = self.SelfAttention(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py", line 559, in forward
    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 13.19 MiB is free. Process 2394383 has 10.10 GiB memory in use. Including non-PyTorch memory, this process has 12.04 GiB memory in use. Of the allocated memory 11.81 GiB is allocated by PyTorch, and 35.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-08-19 11:39:12,552] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2409533 closing signal SIGTERM
[2025-08-19 11:39:13,068] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2409532) of binary: /export/scratch/sheid/miniconda3/envs/pixart/bin/python
Traceback (most recent call last):
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/export/scratch/sheid/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/export/home/sheid/PixArt-sigma/train_scripts/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_11:39:12
  host      : hcigpu07
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2409532)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
